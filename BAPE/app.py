"""
BAPE å•†å“çˆ¬èŸ² + Shopify Bulk Operations ä¸Šæ¶å·¥å…· v2.2
ä¾†æºï¼šjp.bape.com
åŠŸèƒ½ï¼š
1. æŒ‰åˆ†é¡çˆ¬å– jp.bape.com å•†å“ï¼ˆãƒ¡ãƒ³ã‚ºã€ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹ã€ã‚­ãƒƒã‚ºï¼‰
2. ç¿»è­¯ä¸¦ç”¢ç”Ÿ JSONL æª”æ¡ˆ
3. ä½¿ç”¨ Shopify Bulk Operations API æ‰¹é‡ä¸Šå‚³
4. æ™ºæ…§åŒæ­¥ï¼šæ–°å•†å“ä¸Šæ¶ã€å·²å­˜åœ¨åªæ›´æ–°åƒ¹æ ¼
5. v2.2: ä¸‹æ¶/ç¼ºè²¨å•†å“ç›´æ¥åˆªé™¤ï¼ˆä¸è¨­è‰ç¨¿ï¼‰
"""

from flask import Flask, jsonify, request
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
import threading

app = Flask(__name__)

SHOPIFY_SHOP = ""
SHOPIFY_ACCESS_TOKEN = ""
SOURCE_URL = "https://jp.bape.com"

CATEGORIES = {
    'mens': {'name': 'ãƒ¡ãƒ³ã‚º', 'collection': "BAPE Men's", 'base_url': '/collections/all',
        'filter': 'filter.p.m.bape_data.type=%E3%83%A1%E3%83%B3%E3%82%BA&filter.v.availability=1',
        'tags': ['BAPE', 'A BATHING APE', 'æ—¥æœ¬', 'æ½®æµ', 'ç”·è£'], 'product_type': "BAPE ç”·è£"},
    'womens': {'name': 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹', 'collection': "BAPE Women's", 'base_url': '/collections/all',
        'filter': 'filter.p.m.bape_data.type=%E3%83%AC%E3%83%87%E3%82%A3%E3%83%BC%E3%82%B9&filter.v.availability=1',
        'tags': ['BAPE', 'A BATHING APE', 'æ—¥æœ¬', 'æ½®æµ', 'å¥³è£'], 'product_type': "BAPE å¥³è£"},
    'kids': {'name': 'ã‚­ãƒƒã‚º', 'collection': "BAPE Kids", 'base_url': '/collections/all',
        'filter': 'filter.p.m.bape_data.type=%E3%82%AD%E3%83%83%E3%82%BA&filter.v.availability=1',
        'tags': ['BAPE', 'A BATHING APE', 'æ—¥æœ¬', 'æ½®æµ', 'ç«¥è£', 'å…’ç«¥'], 'product_type': "BAPE ç«¥è£"},
}

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
MIN_PRICE = 1000
DEFAULT_WEIGHT = 0.5
JSONL_DIR = "/tmp/bape_jsonl"

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/json, text/html', 'Accept-Language': 'ja,en;q=0.9'}

os.makedirs(JSONL_DIR, exist_ok=True)

scrape_status = {
    "running": False, "phase": "", "progress": 0, "total": 0,
    "current_product": "", "products": [], "errors": [],
    "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "",
    "deleted": 0,
}


def load_shopify_token():
    global SHOPIFY_SHOP, SHOPIFY_ACCESS_TOKEN
    if not SHOPIFY_SHOP: SHOPIFY_SHOP = os.environ.get("SHOPIFY_SHOP", "")
    if not SHOPIFY_ACCESS_TOKEN: SHOPIFY_ACCESS_TOKEN = os.environ.get("SHOPIFY_ACCESS_TOKEN", "")


def graphql_request(query, variables=None):
    load_shopify_token()
    url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-10/graphql.json"
    headers = {'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN, 'Content-Type': 'application/json'}
    payload = {'query': query}
    if variables: payload['variables'] = variables
    return requests.post(url, headers=headers, json=payload, timeout=60).json()


_collection_id_cache = {}


def get_or_create_collection(collection_name):
    global _collection_id_cache
    if collection_name in _collection_id_cache: return _collection_id_cache[collection_name]
    query = """query findCollection($title: String!) { collections(first: 1, query: $title) { edges { node { id title } } } }"""
    result = graphql_request(query, {"title": f"title:{collection_name}"})
    for edge in result.get('data', {}).get('collections', {}).get('edges', []):
        if edge['node']['title'] == collection_name:
            _collection_id_cache[collection_name] = edge['node']['id']
            return edge['node']['id']
    mutation = """mutation createCollection($input: CollectionInput!) { collectionCreate(input: $input) { collection { id title } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"title": collection_name, "descriptionHtml": f"<p>{collection_name} - æ—¥æœ¬ A BATHING APE å®˜æ–¹æ­£å“ä»£è³¼</p>"}})
    collection = result.get('data', {}).get('collectionCreate', {}).get('collection')
    if collection:
        _collection_id_cache[collection_name] = collection['id']
        publish_collection_to_all_channels(collection['id'])
        return collection['id']
    return None


def publish_collection_to_all_channels(collection_id):
    pub_ids = get_all_publication_ids()
    if not pub_ids: return
    mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { publishable { availablePublicationsCount { count } } userErrors { field message } } }"""
    graphql_request(mutation, {"id": collection_id, "input": [{"publicationId": pid} for pid in pub_ids]})


def get_all_publication_ids():
    result = graphql_request("""{ publications(first: 20) { edges { node { id name } } } }""")
    return [edge['node']['id'] for edge in result.get('data', {}).get('publications', {}).get('edges', [])]


def get_all_publications():
    result = graphql_request("""{ publications(first: 20) { edges { node { id name } } } }""")
    return [{'id': edge['node']['id'], 'name': edge['node']['name']} for edge in result.get('data', {}).get('publications', {}).get('edges', [])]


def calculate_selling_price(cost, weight):
    return int((cost + weight * 1250) / 0.7)


def contains_japanese(text):
    if not text: return False
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF]', text))


def remove_japanese(text):
    if not text: return text
    return re.sub(r'\s+', ' ', re.sub(r'[\u3040-\u309F\u30A0-\u30FF]+', '', text)).strip()


# ========== ç¿»è­¯ ==========

def translate_with_chatgpt(title, description, size_spec=''):
    if not OPENAI_API_KEY:
        return {'success': False, 'title': f"BAPE {title}", 'description': description}
    size_spec_section = f"\nå°ºå¯¸è¦æ ¼è¡¨ï¼š\n{size_spec}" if size_spec else ""
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬æ½®æµå“ç‰Œå•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡/è‹±æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼š{description[:1500] if description else ''}{size_spec_section}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{"title": "ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆç¹é«”ä¸­æ–‡ï¼Œå‰é¢åŠ ä¸Š BAPEï¼‰","description": "ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆHTMLæ ¼å¼ï¼Œç”¨<br>æ›è¡Œï¼‰","size_spec_translated": "ç¿»è­¯å¾Œçš„å°ºå¯¸è¦æ ¼ï¼ˆæ ¼å¼ï¼šåˆ—1|åˆ—2|åˆ—3ï¼Œæ¯è¡Œæ›è¡Œåˆ†éš”ï¼‰"}}

è¦å‰‡ï¼š1. çµ•å°ç¦æ­¢æ—¥æ–‡ 2. é–‹é ­ã€ŒBAPEã€3. å°ºå¯¸ï¼šã‚µã‚¤ã‚ºâ†’å°ºå¯¸ã€ç€ä¸ˆâ†’è¡£é•·ã€èº«å¹…â†’èº«å¯¬ã€è‚©å¹…â†’è‚©å¯¬ã€è¢–ä¸ˆâ†’è¢–é•· 4. å¿½ç•¥æ³¨æ„äº‹é …å’Œåƒ¹æ ¼ 5. åªå›å‚³JSON"""
    try:
        response = requests.post("https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
            json={"model": "gpt-4o-mini", "messages": [
                {"role": "system", "content": "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è¼¸å‡ºç¦æ­¢ä»»ä½•æ—¥æ–‡ã€‚"},
                {"role": "user", "content": prompt}], "temperature": 0, "max_tokens": 1500}, timeout=60)
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content'].strip()
            if content.startswith('```'): content = content.split('\n', 1)[1]
            if content.endswith('```'): content = content.rsplit('```', 1)[0]
            translated = json.loads(content.strip())
            trans_title = translated.get('title', title)
            trans_desc = translated.get('description', description)
            trans_size = translated.get('size_spec_translated', '')
            if contains_japanese(trans_title): trans_title = remove_japanese(trans_title)
            if contains_japanese(trans_desc): trans_desc = remove_japanese(trans_desc)
            if not trans_title.startswith('BAPE'): trans_title = f"BAPE {trans_title}"
            size_html = build_size_table_html(trans_size) if trans_size else ''
            if size_html: trans_desc += '<br><br>' + size_html
            return {'success': True, 'title': trans_title, 'description': trans_desc}
        return {'success': False, 'title': f"BAPE {title}", 'description': description}
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {'success': False, 'title': f"BAPE {title}", 'description': description}


def build_size_table_html(size_spec_text):
    if not size_spec_text: return ''
    lines = [line.strip() for line in size_spec_text.strip().split('\n') if line.strip()]
    if not lines: return ''
    html = '<div class="size-spec"><h3>ğŸ“ å°ºå¯¸è¦æ ¼</h3><table style="border-collapse:collapse;width:100%;margin:10px 0;">'
    for i, line in enumerate(lines):
        cells = [c.strip() for c in line.split('|')]
        if i == 0:
            html += '<tr style="background:#f5f5f5;">' + ''.join(f'<th style="border:1px solid #ddd;padding:8px;text-align:center;">{c}</th>' for c in cells) + '</tr>'
        else:
            html += '<tr>' + ''.join(f'<td style="border:1px solid #ddd;padding:8px;{"font-weight:bold;background:#fafafa;" if j==0 else "text-align:center;"}">{c}</td>' for j, c in enumerate(cells)) + '</tr>'
    html += '</table><p style="font-size:12px;color:#666;">â€» å°ºå¯¸å¯èƒ½æœ‰äº›è¨±èª¤å·®</p></div>'
    return html


def clean_description(description):
    description = re.sub(r'<a[^>]*>.*?</a>', '', description)
    for pat in [r'[^<>]*\d+[,ï¼Œ]?\d*\s*æ—¥åœ“[^<>]*', r'[^<>]*\d+[,ï¼Œ]?\d*\s*å††[^<>]*',
                r'[^<>]*\d+%\s*OFF[^<>]*', r'[^<>]*é™åƒ¹[^<>]*', r'[^<>]*å¤§å¹…[^<>]*',
                r'[^<>]*æ³¨æ„äº‹é …[^<>]*', r'[^<>]*è«‹æ³¨æ„[^<>]*', r'[^<>]*æ•¬è«‹è«’è§£[^<>]*',
                r'[^<>]*æ•¬è«‹è¦‹è«’[^<>]*', r'[^<>]*â€»[^<>]*']:
        description = re.sub(pat, '', description, flags=re.IGNORECASE)
    description = re.sub(r'<p>\s*</p>', '', description)
    description = re.sub(r'<br\s*/?>\s*<br\s*/?>', '<br>', description)
    description = re.sub(r'^\s*(<br\s*/?>)+', '', description)
    description = re.sub(r'(<br\s*/?>)+\s*$', '', description).strip()
    return description + "\n<br><br>\n<p><strong>ã€è«‹æ³¨æ„ä»¥ä¸‹äº‹é …ã€‘</strong></p>\n<p>â€»ä¸æ¥å—é€€æ›è²¨</p>\n<p>â€»é–‹ç®±è«‹å…¨ç¨‹éŒ„å½±</p>\n<p>â€»å› åº«å­˜æœ‰é™ï¼Œè¨‚è³¼æ™‚é–“ä¸åŒå¯èƒ½æœƒå‡ºç¾ç¼ºè²¨æƒ…æ³ã€‚</p>\n"


# ========== çˆ¬å–å‡½æ•¸ ==========

def fetch_products_json(page=1):
    url = f"{SOURCE_URL}/collections/all/products.json?page={page}&limit=50"
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code != 200: return []
        return response.json().get('products', [])
    except Exception as e:
        print(f"[éŒ¯èª¤] {e}"); return []


def get_product_category(product):
    tags = product.get('tags', [])
    title = product.get('title', '').upper()
    tags_str = ','.join(tags).lower() if isinstance(tags, list) else str(tags).lower()
    if 'ã‚­ãƒƒã‚º' in tags_str or 'kids' in tags_str or 'KIDS' in title or 'ã‚­ãƒƒã‚º' in title: return 'kids'
    if 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹' in tags_str or 'ladies' in tags_str or 'women' in tags_str or 'LADIES' in title or 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹' in title: return 'womens'
    return 'mens'


def fetch_all_products_by_category():
    all_products = {'mens': [], 'womens': [], 'kids': []}
    page = 1; seen_handles = set()
    while True:
        products = fetch_products_json(page)
        if not products: break
        for p in products:
            handle = p.get('handle', '')
            if handle in seen_handles: continue
            seen_handles.add(handle)
            if not any(v.get('available', False) for v in p.get('variants', [])): continue
            all_products[get_product_category(p)].append(p)
        if len(products) < 50: break
        page += 1; time.sleep(0.5)
    print(f"[çˆ¬å–] åˆ†é¡çµæœ: ç”·è£ {len(all_products['mens'])}, å¥³è£ {len(all_products['womens'])}, ç«¥è£ {len(all_products['kids'])}")
    return all_products


def fetch_size_table(handle):
    try:
        response = requests.get(f"{SOURCE_URL}/products/{handle}", headers={'User-Agent': HEADERS['User-Agent'], 'Accept': 'text/html'}, timeout=30)
        if response.status_code != 200: return None
        soup = BeautifulSoup(response.text, 'html.parser')
        def_list = soup.find('dl', class_='s-product-detail__def-list-description')
        if not def_list: return None
        size_dt = def_list.find('dt', string=re.compile(r'ã‚µã‚¤ã‚º'))
        if not size_dt: return None
        size_dd = size_dt.find_next_sibling('dd')
        if not size_dd: return None
        table = size_dd.find('table')
        if not table: return None
        return '\n'.join([' | '.join([cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]) for row in table.find_all('tr')])
    except: return None


# ========== JSONL ç”Ÿæˆ ==========

def product_to_jsonl_entry(product, category_key, collection_id, existing_product_id=None):
    cat_info = CATEGORIES[category_key]
    title = product.get('title', ''); body_html = product.get('body_html', ''); handle = product.get('handle', '')
    source_url = f"{SOURCE_URL}/products/{handle}"
    size_spec = fetch_size_table(handle)
    translated = translate_with_chatgpt(title, body_html, size_spec or '')
    trans_title = translated['title']; trans_desc = clean_description(translated['description'])
    options = product.get('options', []); source_variants = product.get('variants', []); images = product.get('images', [])
    has_options = len(options) > 0 and not (len(options) == 1 and options[0].get('name') == 'Title')
    product_options = []
    if has_options:
        for opt in options:
            opt_name = opt.get('name', ''); opt_values = opt.get('values', [])
            if opt_values and opt_name != 'Title':
                product_options.append({"name": opt_name, "values": [{"name": v} for v in opt_values]})
    image_list = [img['src'] for img in images[:10]] if images else []
    first_image = image_list[0] if image_list else None
    image_id_to_url = {img.get('id'): img.get('src', '') for img in images if img.get('id')}
    files = [{"originalSource": u, "contentType": "IMAGE"} for u in image_list]
    variants = []
    for sv in source_variants:
        if not sv.get('available', False): continue
        cost = float(sv.get('price', 0))
        if cost < MIN_PRICE: continue
        weight = float(sv.get('grams', 0)) / 1000 if sv.get('grams') else DEFAULT_WEIGHT
        selling_price = calculate_selling_price(cost, weight)
        sku_parts = [f"bape-{handle}"]; option_values = []
        if sv.get('option1') and len(options) > 0 and options[0].get('name') != 'Title':
            sku_parts.append(sv['option1']); option_values.append({"optionName": options[0]['name'], "name": sv['option1']})
        if sv.get('option2') and len(options) > 1:
            sku_parts.append(sv['option2']); option_values.append({"optionName": options[1]['name'], "name": sv['option2']})
        if sv.get('option3') and len(options) > 2:
            sku_parts.append(sv['option3']); option_values.append({"optionName": options[2]['name'], "name": sv['option3']})
        variant = {"price": selling_price, "sku": '-'.join(sku_parts), "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}}
        if option_values: variant["optionValues"] = option_values
        variant_image_id = sv.get('image_id') or sv.get('featured_image', {}).get('id')
        if variant_image_id and variant_image_id in image_id_to_url:
            variant["file"] = {"originalSource": image_id_to_url[variant_image_id], "contentType": "IMAGE"}
        elif first_image: variant["file"] = {"originalSource": first_image, "contentType": "IMAGE"}
        variants.append(variant)
    if not variants: return None
    product_input = {
        "title": trans_title, "descriptionHtml": trans_desc, "vendor": "BAPE",
        "productType": cat_info['product_type'], "status": "ACTIVE", "handle": f"bape-{handle}",
        "tags": cat_info['tags'],
        "seo": {"title": f"{trans_title} | BAPE æ—¥æœ¬ä»£è³¼", "description": f"æ—¥æœ¬ A BATHING APE å®˜æ–¹æ­£å“ä»£è³¼ã€‚{trans_title}ï¼Œå°ç£ç¾è²¨æˆ–æ—¥æœ¬ç›´é€ã€‚GOYOUTATI å¾¡ç”¨é”æ—¥æœ¬ä¼´æ‰‹ç¦®å°ˆé–€åº—ã€‚"},
        "metafields": [{"namespace": "custom", "key": "link", "value": source_url, "type": "url"}]}
    if existing_product_id: product_input["id"] = existing_product_id
    if collection_id: product_input["collections"] = [collection_id]
    if product_options: product_input["productOptions"] = product_options
    if variants: product_input["variants"] = variants
    if files: product_input["files"] = files
    return {"productSet": product_input, "synchronous": True}


# ========== Bulk Operations ==========

def create_staged_upload():
    query = """mutation stagedUploadsCreate($input: [StagedUploadInput!]!) { stagedUploadsCreate(input: $input) { stagedTargets { url resourceUrl parameters { name value } } userErrors { field message } } }"""
    result = graphql_request(query, {"input": [{"resource": "BULK_MUTATION_VARIABLES", "filename": "products.jsonl", "mimeType": "text/jsonl", "httpMethod": "POST"}]})
    targets = result.get('data', {}).get('stagedUploadsCreate', {}).get('stagedTargets', [])
    return targets[0] if targets else None


def upload_jsonl_to_staged(staged_target, jsonl_path):
    params = {p['name']: p['value'] for p in staged_target['parameters']}
    with open(jsonl_path, 'rb') as f:
        response = requests.post(staged_target['url'], data=params, files={'file': ('products.jsonl', f, 'text/jsonl')}, timeout=300)
    return response.status_code in [200, 201, 204]


def run_bulk_mutation(staged_upload_path):
    query = """mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) { bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) { bulkOperation { id status } userErrors { field message } } }"""
    mutation = """mutation call($productSet: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $productSet) { product { id title } userErrors { field message } } }"""
    return graphql_request(query, {"mutation": mutation, "stagedUploadPath": staged_upload_path})


def check_bulk_operation_status(operation_id=None):
    if operation_id:
        query = """query($id: ID!) { node(id: $id) { ... on BulkOperation { id status errorCode objectCount url } } }"""
        return graphql_request(query, {"id": operation_id}).get('data', {}).get('node', {})
    else:
        return graphql_request("""{ currentBulkOperation(type: MUTATION) { id status errorCode objectCount url } }""").get('data', {}).get('currentBulkOperation', {})


# ========== å•†å“ç®¡ç† ==========

def fetch_bape_product_ids():
    all_products = []; cursor = None
    while True:
        if cursor:
            query = """query($cursor: String) { products(first: 250, after: $cursor, query: "vendor:BAPE") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }"""
            result = graphql_request(query, {"cursor": cursor})
        else:
            result = graphql_request("""{ products(first: 250, query: "vendor:BAPE") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }""")
        products = result.get('data', {}).get('products', {})
        for edge in products.get('edges', []):
            node = edge['node']
            all_products.append({'id': node['id'], 'title': node['title'], 'handle': node['handle'], 'status': node.get('status', '')})
            cursor = edge['cursor']
        if not products.get('pageInfo', {}).get('hasNextPage', False): break
        time.sleep(0.5)
    return all_products


def set_product_active(product_id):
    mutation = """mutation productUpdate($input: ProductInput!) { productUpdate(input: $input) { product { id status } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"id": product_id, "status": "ACTIVE"}})
    return not result.get('data', {}).get('productUpdate', {}).get('userErrors', [])


def update_existing_product_price(product_id, source_variants):
    query = f"""{{ product(id: "{product_id}") {{ variants(first: 100) {{ edges {{ node {{ id sku }} }} }} }} }}"""
    result = graphql_request(query)
    shopify_variants = result.get('data', {}).get('product', {}).get('variants', {}).get('edges', [])
    if not shopify_variants: return 0
    costs = [float(sv.get('price', 0)) for sv in source_variants if sv.get('available', False) and float(sv.get('price', 0)) >= MIN_PRICE]
    if not costs: return 0
    min_cost = min(costs)
    weight = float(source_variants[0].get('grams', 0)) / 1000 if source_variants[0].get('grams') else DEFAULT_WEIGHT
    selling_price = calculate_selling_price(min_cost, weight)
    updated = 0
    for v_edge in shopify_variants:
        mutation = """mutation productVariantUpdate($input: ProductVariantInput!) { productVariantUpdate(input: $input) { productVariant { id } userErrors { field message } } }"""
        graphql_request(mutation, {"input": {"id": v_edge['node']['id'], "price": str(selling_price)}})
        updated += 1; time.sleep(0.1)
    return updated


def delete_product(product_id):
    mutation = """mutation productDelete($input: ProductDeleteInput!) { productDelete(input: $input) { deletedProductId userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"id": product_id}})
    return not result.get('data', {}).get('productDelete', {}).get('userErrors', [])


def delete_all_bape_products():
    global scrape_status
    products = fetch_bape_product_ids()
    total = len(products)
    if total == 0: return {'success': True, 'deleted': 0, 'message': 'æ²’æœ‰ BAPE å•†å“'}
    deleted = 0; failed = 0
    for i, product in enumerate(products):
        scrape_status['current_product'] = f"åˆªé™¤ä¸­ [{i+1}/{total}] {product.get('title', '')[:30]}"
        scrape_status['progress'] = i + 1
        if delete_product(product['id']): deleted += 1
        else: failed += 1
        time.sleep(0.2)
    return {'success': True, 'deleted': deleted, 'failed': failed, 'total': total}


def batch_publish_bape_products():
    products = fetch_bape_product_ids()
    if not products: return {'success': 0, 'failed': 0}
    publications = get_all_publications()
    if not publications: return {'success': 0, 'failed': 0}
    publication_inputs = [{"publicationId": pub['id']} for pub in publications]
    results = {'total': len(products), 'success': 0, 'failed': 0}
    pub_mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
    cat_mutation = """mutation productUpdate($input: ProductInput!) { productUpdate(input: $input) { product { id } userErrors { field message } } }"""
    for product in products:
        result = graphql_request(pub_mutation, {"id": product['id'], "input": publication_inputs})
        if result.get('data', {}).get('publishablePublish', {}).get('userErrors', []): results['failed'] += 1
        else: results['success'] += 1
        graphql_request(cat_mutation, {"input": {"id": product['id'], "productCategory": {"productTaxonomyNodeId": "gid://shopify/ProductTaxonomyNode/1"}}})
        time.sleep(0.1)
    return results


# ========== ä¸»æµç¨‹ ==========

def run_test_single(category='mens'):
    global scrape_status
    scrape_status = {"running": True, "phase": "testing", "progress": 0, "total": 1, "current_product": "æ¸¬è©¦å–®å“...",
        "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "deleted": 0}
    try:
        cat_info = CATEGORIES[category]
        collection_id = get_or_create_collection(cat_info['collection'])
        if not collection_id:
            scrape_status['errors'].append({'error': 'ç„¡æ³•å»ºç«‹ Collection'}); return
        all_by_category = fetch_all_products_by_category()
        products = all_by_category.get(category, [])
        if not products:
            scrape_status['errors'].append({'error': f'æ²’æœ‰æ‰¾åˆ° {cat_info["name"]} çš„å•†å“'}); return
        test_product = None
        for p in products:
            if min((float(v.get('price', 0)) for v in p.get('variants', [])), default=0) >= MIN_PRICE:
                test_product = p; break
        if not test_product:
            scrape_status['errors'].append({'error': 'æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å•†å“'}); return
        scrape_status['current_product'] = f"ç¿»è­¯: {test_product['title'][:30]}..."
        entry = product_to_jsonl_entry(test_product, category, collection_id)
        if not entry:
            scrape_status['errors'].append({'error': 'å•†å“è½‰æ›å¤±æ•—'}); return
        product_input = entry['productSet']
        scrape_status['products'].append({'title': product_input['title'], 'handle': product_input['handle'], 'variants': len(product_input.get('variants', []))})
        scrape_status['current_product'] = "ä¸Šå‚³åˆ° Shopify..."
        mutation = """mutation productSet($input: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $input) { product { id title handle } userErrors { field code message } } }"""
        result = graphql_request(mutation, {"input": product_input, "synchronous": True})
        product_set = result.get('data', {}).get('productSet', {})
        user_errors = product_set.get('userErrors', [])
        if user_errors:
            scrape_status['errors'].append({'error': '; '.join([e.get('message', str(e)) for e in user_errors])})
        else:
            product = product_set.get('product', {})
            publications = get_all_publications()
            if publications:
                pub_mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
                graphql_request(pub_mutation, {"id": product['id'], "input": [{"publicationId": pub['id']} for pub in publications]})
            scrape_status['current_product'] = f"âœ… æˆåŠŸï¼{product.get('title', '')}"
        scrape_status['progress'] = 1
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False


def run_full_sync(category='all'):
    """v2.2 æ™ºæ…§åŒæ­¥ï¼šæ–°å•†å“â†’Bulk Upload / å·²å­˜åœ¨â†’æ›´æ–°åƒ¹æ ¼ / ä¸‹æ¶/ç¼ºè²¨â†’åˆªé™¤"""
    global scrape_status
    print(f"[SYNC] ========== é–‹å§‹æ™ºæ…§åŒæ­¥ v2.2 ==========")
    scrape_status = {"running": True, "phase": "cron_sync", "progress": 0, "total": 0,
        "current_product": "é–‹å§‹æ™ºæ…§åŒæ­¥...", "products": [], "errors": [],
        "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "deleted": 0}
    try:
        categories_to_scrape = ['mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not categories_to_scrape: raise Exception(f'æœªçŸ¥åˆ†é¡: {category}')

        # 1. å–å¾— Shopify ç¾æœ‰å•†å“
        scrape_status['current_product'] = 'å–å¾— Shopify ç¾æœ‰å•†å“...'
        existing_products = fetch_bape_product_ids()
        existing_handles = {p['handle']: p for p in existing_products}
        print(f"[SYNC] Shopify ç¾æœ‰ {len(existing_handles)} å€‹ BAPE å•†å“")

        # 2. çˆ¬å– BAPE æ‰€æœ‰å•†å“
        scrape_status['current_product'] = 'çˆ¬å– BAPE å•†å“...'
        all_by_category = fetch_all_products_by_category()

        # 3. æ¯”å° + è™•ç†
        new_entries = []; scraped_handles = set()
        updated_count = 0; price_updated_count = 0

        for cat_key in categories_to_scrape:
            cat_info = CATEGORIES[cat_key]
            collection_id = get_or_create_collection(cat_info['collection'])
            if not collection_id: continue
            products = all_by_category.get(cat_key, [])
            print(f"[SYNC] {cat_info['collection']} å…± {len(products)} å€‹æœ‰åº«å­˜å•†å“")
            scrape_status['total'] += len(products)

            for product in products:
                scrape_status['progress'] += 1
                handle = product.get('handle', ''); title = product.get('title', '')[:30]
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {title}"
                my_handle = f"bape-{handle}"; scraped_handles.add(my_handle)
                existing_info = existing_handles.get(my_handle)

                if existing_info:
                    try:
                        cnt = update_existing_product_price(existing_info['id'], product.get('variants', []))
                        if cnt > 0: price_updated_count += 1
                        if existing_info.get('status') == 'DRAFT':
                            set_product_active(existing_info['id'])
                            pub_ids = get_all_publication_ids()
                            if pub_ids:
                                graphql_request("""mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }""",
                                    {"id": existing_info['id'], "input": [{"publicationId": pid} for pid in pub_ids]})
                        updated_count += 1
                    except Exception as e:
                        scrape_status['errors'].append({'error': f'æ›´æ–°å¤±æ•— {title}: {str(e)}'})
                    time.sleep(0.2)
                else:
                    try:
                        entry = product_to_jsonl_entry(product, cat_key, collection_id)
                        if entry:
                            new_entries.append(entry)
                            scrape_status['products'].append({'title': entry['productSet']['title'], 'handle': entry['productSet']['handle'], 'variants': len(entry['productSet'].get('variants', []))})
                    except Exception as e:
                        scrape_status['errors'].append({'error': f'è½‰æ›å¤±æ•— {title}: {str(e)}'})
                    time.sleep(0.3)

        # 4. æ–°å•†å“æ‰¹é‡ä¸Šå‚³
        if new_entries:
            jsonl_path = os.path.join(JSONL_DIR, f"bape_{category}_{int(time.time())}.jsonl")
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for entry in new_entries: f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            scrape_status['jsonl_file'] = jsonl_path
            scrape_status['phase'] = 'uploading'
            scrape_status['current_product'] = f'æ‰¹é‡ä¸Šå‚³ {len(new_entries)} å€‹æ–°å•†å“...'
            staged = create_staged_upload()
            if not staged: raise Exception('å»ºç«‹ Staged Upload å¤±æ•—')
            if not upload_jsonl_to_staged(staged, jsonl_path): raise Exception('ä¸Šå‚³ JSONL å¤±æ•—')
            staged_path = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), '')
            result = run_bulk_mutation(staged_path)
            user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
            if user_errors: raise Exception(f'Bulk Mutation éŒ¯èª¤: {user_errors}')
            scrape_status['bulk_operation_id'] = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {}).get('id', '')
            scrape_status['current_product'] = 'ç­‰å¾…ä¸Šå‚³å®Œæˆ...'
            for _ in range(120):
                status = check_bulk_operation_status()
                scrape_status['bulk_status'] = status.get('status', '')
                if status.get('status') == 'COMPLETED': break
                elif status.get('status') in ['FAILED', 'CANCELED']: raise Exception(f'Bulk å¤±æ•—: {status.get("status")}')
                time.sleep(5)
            scrape_status['phase'] = 'publishing'
            scrape_status['current_product'] = 'ç™¼å¸ƒæ–°å•†å“...'
            batch_publish_bape_products()

        # === v2.2: ä¸‹æ¶/ç¼ºè²¨å•†å“ç›´æ¥åˆªé™¤ï¼ˆä¸è¨­è‰ç¨¿ï¼‰===
        scrape_status['phase'] = 'deleting'
        scrape_status['current_product'] = 'æ¸…ç†ä¸‹æ¶/ç¼ºè²¨å•†å“...'
        delete_count = 0
        for handle, product_info in existing_handles.items():
            if handle not in scraped_handles:
                print(f"[SYNC] ğŸ—‘ åˆªé™¤: {handle} - {product_info.get('title', '')[:30]}")
                scrape_status['current_product'] = f"åˆªé™¤: {product_info.get('title', '')[:30]}"
                if delete_product(product_info['id']):
                    delete_count += 1
                time.sleep(0.2)

        scrape_status['deleted'] = delete_count
        scrape_status['current_product'] = f"âœ… å®Œæˆï¼æ–°å•†å“ {len(new_entries)} å€‹ï¼Œæ›´æ–° {updated_count} å€‹ï¼Œåˆªé™¤ {delete_count} å€‹"
        scrape_status['phase'] = 'completed'
        print(f"[SYNC] âœ… æ–°å•†å“: {len(new_entries)}, æ›´æ–°åƒ¹æ ¼: {price_updated_count}, åˆªé™¤: {delete_count}")
        return {'success': True, 'new_products': len(new_entries), 'updated': updated_count, 'deleted': delete_count}

    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        scrape_status['current_product'] = f"âŒ éŒ¯èª¤: {str(e)}"
        scrape_status['phase'] = 'error'
        import traceback; traceback.print_exc()
        return {'success': False, 'error': str(e)}
    finally:
        scrape_status['running'] = False


# ========== Flask Routes + Frontend ==========

@app.route('/')
def index():
    return '''<!DOCTYPE html>
<html lang="zh-TW">
<head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>BAPE çˆ¬èŸ²å·¥å…·</title>
<style>*{margin:0;padding:0;box-sizing:border-box}body{font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,sans-serif;background:#f5f5f5;padding:20px}.container{max-width:1000px;margin:0 auto}h1{color:#333;margin-bottom:20px}.card{background:white;border-radius:12px;padding:20px;margin-bottom:20px;box-shadow:0 2px 8px rgba(0,0,0,0.1)}.card h2{color:#444;margin-bottom:15px;font-size:18px}.btn{display:inline-block;padding:12px 24px;border:none;border-radius:8px;cursor:pointer;font-size:14px;font-weight:600;margin:5px}.btn-primary{background:#0066ff;color:white}.btn-success{background:#00c853;color:white}.btn-warning{background:#ff9800;color:white}.btn-danger{background:#f44336;color:white}.btn:disabled{opacity:0.5;cursor:not-allowed}.status{background:#f8f9fa;border-radius:8px;padding:15px;margin-top:15px}.progress-bar{height:20px;background:#e0e0e0;border-radius:10px;overflow:hidden;margin:10px 0}.progress-fill{height:100%;background:linear-gradient(90deg,#0066ff,#00c853);transition:width 0.3s}.log{background:#1e1e1e;color:#d4d4d4;padding:15px;border-radius:8px;max-height:300px;overflow-y:auto;font-family:monospace;font-size:12px}select{padding:10px;border-radius:6px;border:1px solid #ddd;font-size:14px;margin-right:10px}.stats{display:grid;grid-template-columns:repeat(3,1fr);gap:15px;margin-top:15px}.stat-box{background:#f0f4f8;padding:15px;border-radius:8px;text-align:center}.stat-value{font-size:24px;font-weight:bold}.stat-label{font-size:12px;color:#666}.danger-zone{border:2px solid #f44336;background:#fff5f5}</style></head>
<body><div class="container">
<h1>ğŸ¦ BAPE çˆ¬èŸ²å·¥å…· <small style="font-size:14px;color:#999">v2.2</small></h1>
<div class="card"><h2>âš¡ æ¸¬è©¦å–®å“</h2>
<select id="testCat"><option value="mens">ç”·è£</option><option value="womens">å¥³è£</option><option value="kids">ç«¥è£</option></select>
<button class="btn btn-warning" onclick="startTest()">ğŸ§ª æ¸¬è©¦å–®å“</button></div>
<div class="card"><h2>ğŸ”„ æ™ºæ…§åŒæ­¥</h2>
<p style="color:#666;margin-bottom:10px;">æ–°å•†å“â†’ç¿»è­¯ä¸Šæ¶ / å·²å­˜åœ¨â†’æ›´æ–°åƒ¹æ ¼ / <b style="color:#e67e22">ä¸‹æ¶/ç¼ºè²¨â†’è‡ªå‹•åˆªé™¤</b></p>
<select id="syncCat"><option value="all">å…¨éƒ¨</option><option value="mens">ç”·è£</option><option value="womens">å¥³è£</option><option value="kids">ç«¥è£</option></select>
<button class="btn btn-success" onclick="startSync()">ğŸ”„ é–‹å§‹åŒæ­¥</button></div>
<div class="card"><h2>ğŸ“Š åŸ·è¡Œç‹€æ…‹</h2>
<div class="progress-bar"><div class="progress-fill" id="progressFill" style="width:0%"></div></div>
<div class="status"><div>éšæ®µï¼š<span id="phase">-</span></div><div>é€²åº¦ï¼š<span id="progress">0/0</span></div><div>ç›®å‰ï¼š<span id="current">-</span></div></div>
<div class="stats">
<div class="stat-box"><div class="stat-value" id="productCount">0</div><div class="stat-label">æ–°å•†å“</div></div>
<div class="stat-box"><div class="stat-value" id="deletedCount" style="color:#e67e22">0</div><div class="stat-label">å·²åˆªé™¤</div></div>
<div class="stat-box"><div class="stat-value" id="errorCount" style="color:#e74c3c">0</div><div class="stat-label">éŒ¯èª¤</div></div>
</div></div>
<div class="card"><h2>ğŸ“ æ—¥èªŒ</h2><div class="log" id="log"></div></div>
<div class="card"><h2>ğŸ”§ å·¥å…·</h2>
<button class="btn btn-primary" onclick="testShopify()">æ¸¬è©¦ Shopify</button>
<button class="btn btn-primary" onclick="testBape()">æ¸¬è©¦ BAPE</button>
<button class="btn btn-primary" onclick="countProducts()">å•†å“æ•¸é‡</button>
<button class="btn btn-success" onclick="publishAll()">ç™¼å¸ƒæ‰€æœ‰</button></div>
<div class="card danger-zone"><h2>âš ï¸ å±éšªå€åŸŸ</h2><p style="color:#666;margin-bottom:10px;">åˆªé™¤æ“ä½œç„¡æ³•å¾©åŸ</p>
<button class="btn btn-danger" onclick="deleteAll()">ğŸ—‘ï¸ åˆªé™¤æ‰€æœ‰ BAPE å•†å“</button></div>
</div>
<script>
let pollInterval;
function log(msg,type='info'){const d=document.getElementById('log');const t=new Date().toLocaleTimeString();const c=type==='success'?'#4ec9b0':type==='error'?'#f14c4c':'#d4d4d4';d.innerHTML+=`<div style="color:${c}">[${t}] ${msg}</div>`;d.scrollTop=d.scrollHeight}
function updateStatus(d){document.getElementById('phase').textContent=d.phase||'-';document.getElementById('progress').textContent=`${d.progress||0}/${d.total||0}`;document.getElementById('current').textContent=d.current_product||'-';document.getElementById('productCount').textContent=d.products?.length||0;document.getElementById('deletedCount').textContent=d.deleted||0;document.getElementById('errorCount').textContent=d.errors?.length||0;document.getElementById('progressFill').style.width=d.total>0?(d.progress/d.total*100)+'%':'0%'}
async function pollStatus(){try{const r=await fetch('/api/status');const d=await r.json();updateStatus(d);if(!d.running){clearInterval(pollInterval);if(d.phase==='completed')log('âœ… å®Œæˆï¼','success');if(d.errors?.length>0)d.errors.forEach(e=>log('âŒ '+(e.error||JSON.stringify(e)),'error'))}}catch(e){}}
async function startTest(){log('ğŸ§ª é–‹å§‹æ¸¬è©¦å–®å“...');const r=await fetch('/api/test_single?category='+document.getElementById('testCat').value);const d=await r.json();if(d.success){log('æ¸¬è©¦å·²å•Ÿå‹•','success');pollInterval=setInterval(pollStatus,1000)}else log('âŒ '+(d.error||'å•Ÿå‹•å¤±æ•—'),'error')}
async function startSync(){log('ğŸ”„ é–‹å§‹æ™ºæ…§åŒæ­¥...');const r=await fetch('/api/auto_sync?category='+document.getElementById('syncCat').value);const d=await r.json();if(d.success){log('åŒæ­¥å·²å•Ÿå‹•','success');pollInterval=setInterval(pollStatus,1000)}else log('âŒ '+(d.error||'å•Ÿå‹•å¤±æ•—'),'error')}
async function deleteAll(){if(!confirm('ç¢ºå®šè¦åˆªé™¤æ‰€æœ‰ BAPE å•†å“å—ï¼Ÿ'))return;if(!confirm('çœŸçš„ç¢ºå®šå—ï¼Ÿ'))return;log('ğŸ—‘ï¸ é–‹å§‹åˆªé™¤...','error');const r=await fetch('/api/delete_all');const d=await r.json();if(d.success){log('åˆªé™¤å·²å•Ÿå‹•','success');pollInterval=setInterval(pollStatus,1000)}else log('âŒ '+(d.error||'å•Ÿå‹•å¤±æ•—'),'error')}
async function testShopify(){log('æ¸¬è©¦ Shopify...');const r=await fetch('/api/test');const d=await r.json();if(d.data?.shop)log('âœ… '+d.data.shop.name,'success');else log('âŒ é€£ç·šå¤±æ•—','error')}
async function testBape(){log('æ¸¬è©¦ BAPE...');const r=await fetch('/api/test_bape');const d=await r.json();log('ç¸½å•†å“: '+(d.total_products||0),d.total_products?'success':'error')}
async function countProducts(){const r=await fetch('/api/count');const d=await r.json();log('Shopify å•†å“æ•¸é‡: '+d.count,'success')}
async function publishAll(){log('ğŸ“¢ ç™¼å¸ƒæ‰€æœ‰å•†å“...');const r=await fetch('/api/publish_all');const d=await r.json();if(d.success){log('ç™¼å¸ƒå·²å•Ÿå‹•','success');pollInterval=setInterval(pollStatus,1000)}else log('âŒ '+(d.error||'å•Ÿå‹•å¤±æ•—'),'error')}
</script></body></html>'''


@app.route('/api/status')
def api_status():
    return jsonify(scrape_status)

@app.route('/api/test')
def api_test():
    load_shopify_token()
    return jsonify(graphql_request("{ shop { name } }"))

@app.route('/api/test_single')
def api_test_single():
    category = request.args.get('category', 'mens')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    if category not in CATEGORIES: return jsonify({'success': False, 'error': 'ç„¡æ•ˆåˆ†é¡'})
    threading.Thread(target=run_test_single, args=(category,)).start()
    return jsonify({'success': True})

@app.route('/api/auto_sync')
def api_auto_sync():
    category = request.args.get('category', 'all')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_full_sync, args=(category,)).start()
    return jsonify({'success': True})

@app.route('/api/cron')
def api_cron():
    category = request.args.get('category', 'all')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_full_sync, args=(category,)).start()
    return jsonify({'success': True, 'message': 'æ™ºæ…§åŒæ­¥å·²å•Ÿå‹•'})

@app.route('/api/bulk_status')
def api_bulk_status():
    return jsonify(check_bulk_operation_status(scrape_status.get('bulk_operation_id') or None))

@app.route('/api/publish_all')
def api_publish_all():
    if scrape_status.get('running'): return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    def run_publish():
        global scrape_status
        scrape_status['running'] = True; scrape_status['phase'] = 'publishing'
        try:
            results = batch_publish_bape_products()
            scrape_status['current_product'] = f"å®Œæˆï¼æˆåŠŸ: {results.get('success', 0)}"
        except Exception as e: scrape_status['errors'].append({'error': str(e)})
        finally: scrape_status['running'] = False; scrape_status['phase'] = 'idle'
    threading.Thread(target=run_publish).start()
    return jsonify({'success': True})

@app.route('/api/delete_all')
def api_delete_all():
    if scrape_status.get('running'): return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    def run_delete():
        global scrape_status
        scrape_status['running'] = True; scrape_status['phase'] = 'deleting'
        scrape_status['progress'] = 0; scrape_status['total'] = 0; scrape_status['errors'] = []
        try:
            products = fetch_bape_product_ids(); scrape_status['total'] = len(products)
            results = delete_all_bape_products()
            scrape_status['current_product'] = f"âœ… å·²åˆªé™¤ {results.get('deleted', 0)} å€‹å•†å“"
        except Exception as e: scrape_status['errors'].append({'error': str(e)})
        finally: scrape_status['running'] = False; scrape_status['phase'] = 'completed'
    threading.Thread(target=run_delete).start()
    return jsonify({'success': True})

@app.route('/api/count')
def api_count():
    load_shopify_token()
    result = graphql_request("{ productsCount(query: \"vendor:BAPE\") { count } }")
    return jsonify({'count': result.get('data', {}).get('productsCount', {}).get('count', 0)})

@app.route('/api/test_bape')
def api_test_bape():
    results = {}
    try:
        all_by_category = fetch_all_products_by_category()
        results['total_products'] = sum(len(v) for v in all_by_category.values())
        results['categories'] = {k: len(v) for k, v in all_by_category.items()}
    except Exception as e: results['error'] = str(e)
    return jsonify(results)


if __name__ == '__main__':
    print("BAPE çˆ¬èŸ²å·¥å…· v2.2")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=False)
