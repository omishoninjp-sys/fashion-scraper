"""
BAPE å•†å“çˆ¬èŸ² + Shopify Bulk Operations ä¸Šæ¶å·¥å…·
ä¾†æºï¼šjp.bape.com
åŠŸèƒ½ï¼š
1. æŒ‰åˆ†é¡çˆ¬å– jp.bape.com å•†å“ï¼ˆãƒ¡ãƒ³ã‚ºã€ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹ã€ã‚­ãƒƒã‚ºï¼‰
2. ç¿»è­¯ä¸¦ç”¢ç”Ÿ JSONL æª”æ¡ˆ
3. ä½¿ç”¨ Shopify Bulk Operations API æ‰¹é‡ä¸Šå‚³
4. è‡ªå‹•åŒæ­¥ï¼šç›¸åŒå•†å“è¦†è“‹æ›´æ–°ï¼Œä¸‹æ¶å•†å“è¨­ç‚ºè‰ç¨¿
"""

from flask import Flask, jsonify, request
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
import threading

app = Flask(__name__)

# ========== è¨­å®š ==========
SHOPIFY_SHOP = ""
SHOPIFY_ACCESS_TOKEN = ""

SOURCE_URL = "https://jp.bape.com"

# åˆ†é¡è¨­å®š
CATEGORIES = {
    'mens': {
        'name': 'ãƒ¡ãƒ³ã‚º',
        'collection': "BAPE Men's",
        'base_url': '/collections/all',
        'filter': 'filter.p.m.bape_data.type=%E3%83%A1%E3%83%B3%E3%82%BA&filter.v.availability=1',
        'tags': ['BAPE', 'A BATHING APE', 'æ—¥æœ¬', 'æ½®æµ', 'ç”·è£'],
        'product_type': "BAPE ç”·è£"
    },
    'womens': {
        'name': 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹',
        'collection': "BAPE Women's",
        'base_url': '/collections/all',
        'filter': 'filter.p.m.bape_data.type=%E3%83%AC%E3%83%87%E3%82%A3%E3%83%BC%E3%82%B9&filter.v.availability=1',
        'tags': ['BAPE', 'A BATHING APE', 'æ—¥æœ¬', 'æ½®æµ', 'å¥³è£'],
        'product_type': "BAPE å¥³è£"
    },
    'kids': {
        'name': 'ã‚­ãƒƒã‚º',
        'collection': "BAPE Kids",
        'base_url': '/collections/all',
        'filter': 'filter.p.m.bape_data.type=%E3%82%AD%E3%83%83%E3%82%BA&filter.v.availability=1',
        'tags': ['BAPE', 'A BATHING APE', 'æ—¥æœ¬', 'æ½®æµ', 'ç«¥è£', 'å…’ç«¥'],
        'product_type': "BAPE ç«¥è£"
    }
}

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
MIN_PRICE = 1000
DEFAULT_WEIGHT = 0.5
JSONL_DIR = "/tmp/bape_jsonl"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'application/json, text/html',
    'Accept-Language': 'ja,en;q=0.9',
}

os.makedirs(JSONL_DIR, exist_ok=True)

scrape_status = {
    "running": False,
    "phase": "",
    "progress": 0,
    "total": 0,
    "current_product": "",
    "products": [],
    "errors": [],
    "jsonl_file": "",
    "bulk_operation_id": "",
    "bulk_status": "",
    "set_to_draft": 0,
}

# ========== å·¥å…·å‡½æ•¸ ==========

def load_shopify_token():
    global SHOPIFY_SHOP, SHOPIFY_ACCESS_TOKEN
    if not SHOPIFY_SHOP:
        SHOPIFY_SHOP = os.environ.get("SHOPIFY_SHOP", "")
    if not SHOPIFY_ACCESS_TOKEN:
        SHOPIFY_ACCESS_TOKEN = os.environ.get("SHOPIFY_ACCESS_TOKEN", "")


def graphql_request(query, variables=None):
    load_shopify_token()
    url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/graphql.json"
    headers = {
        'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN,
        'Content-Type': 'application/json',
    }
    payload = {'query': query}
    if variables:
        payload['variables'] = variables
    
    response = requests.post(url, headers=headers, json=payload, timeout=60)
    return response.json()


_collection_id_cache = {}


def get_or_create_collection(collection_name):
    global _collection_id_cache
    
    if collection_name in _collection_id_cache:
        return _collection_id_cache[collection_name]
    
    query = """
    query findCollection($title: String!) {
      collections(first: 1, query: $title) {
        edges { node { id title } }
      }
    }
    """
    result = graphql_request(query, {"title": f"title:{collection_name}"})
    edges = result.get('data', {}).get('collections', {}).get('edges', [])
    
    for edge in edges:
        if edge['node']['title'] == collection_name:
            collection_id = edge['node']['id']
            _collection_id_cache[collection_name] = collection_id
            print(f"[Collection] æ‰¾åˆ°: {collection_name}")
            return collection_id
    
    mutation = """
    mutation createCollection($input: CollectionInput!) {
      collectionCreate(input: $input) {
        collection { id title }
        userErrors { field message }
      }
    }
    """
    result = graphql_request(mutation, {
        "input": {
            "title": collection_name,
            "descriptionHtml": f"<p>{collection_name} - æ—¥æœ¬ A BATHING APE å®˜æ–¹æ­£å“ä»£è³¼</p>"
        }
    })
    
    collection = result.get('data', {}).get('collectionCreate', {}).get('collection')
    if collection:
        collection_id = collection['id']
        _collection_id_cache[collection_name] = collection_id
        print(f"[Collection] å»ºç«‹: {collection_name}")
        publish_collection_to_all_channels(collection_id)
        return collection_id
    
    return None


def publish_collection_to_all_channels(collection_id):
    publication_ids = get_all_publication_ids()
    if not publication_ids:
        return
    
    publication_inputs = [{"publicationId": pub_id} for pub_id in publication_ids]
    
    mutation = """
    mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
      publishablePublish(id: $id, input: $input) {
        publishable { availablePublicationsCount { count } }
        userErrors { field message }
      }
    }
    """
    graphql_request(mutation, {"id": collection_id, "input": publication_inputs})


def get_all_publication_ids():
    query = """{ publications(first: 20) { edges { node { id name } } } }"""
    result = graphql_request(query)
    return [edge['node']['id'] for edge in result.get('data', {}).get('publications', {}).get('edges', [])]


def calculate_selling_price(cost, weight):
    """(æˆæœ¬ + é‡é‡*1250) / 0.7ï¼Œç„¡æ¢ä»¶æ¨å»"""
    shipping_cost = weight * 1250
    base_price = cost + shipping_cost
    return int(base_price / 0.7)


def contains_japanese(text):
    if not text:
        return False
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF]', text))


def remove_japanese(text):
    if not text:
        return text
    cleaned = re.sub(r'[\u3040-\u309F\u30A0-\u30FF]+', '', text)
    return re.sub(r'\s+', ' ', cleaned).strip()


# ========== ç¿»è­¯ ==========

def translate_with_chatgpt(title, description, size_spec=''):
    size_spec_section = f"\nå°ºå¯¸è¦æ ¼è¡¨ï¼š\n{size_spec}" if size_spec else ""
    
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬æ½®æµå“ç‰Œå•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡/è‹±æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼š{description[:1500] if description else ''}{size_spec_section}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{
    "title": "ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆç¹é«”ä¸­æ–‡ï¼Œå‰é¢åŠ ä¸Š BAPEï¼‰",
    "description": "ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆHTMLæ ¼å¼ï¼Œç”¨<br>æ›è¡Œï¼‰",
    "size_spec_translated": "ç¿»è­¯å¾Œçš„å°ºå¯¸è¦æ ¼ï¼ˆæ ¼å¼ï¼šåˆ—1|åˆ—2|åˆ—3ï¼Œæ¯è¡Œæ›è¡Œåˆ†éš”ï¼‰"
}}

è¦å‰‡ï¼š
1. çµ•å°ç¦æ­¢æ—¥æ–‡ï¼ˆå¹³å‡åã€ç‰‡å‡åï¼‰
2. å•†å“åç¨±é–‹é ­å¿…é ˆæ˜¯ã€ŒBAPEã€
3. å°ºå¯¸æ¬„ä½ç¿»è­¯ï¼šã‚µã‚¤ã‚ºâ†’å°ºå¯¸ã€ç€ä¸ˆâ†’è¡£é•·ã€èº«å¹…â†’èº«å¯¬ã€è‚©å¹…â†’è‚©å¯¬ã€è¢–ä¸ˆâ†’è¢–é•·
4. å®Œå…¨å¿½ç•¥æ³¨æ„äº‹é …ï¼ˆã”æ³¨æ„ã€æ³¨æ„äº‹é …ã€ã”äº†æ‰¿ã€â€»è¨˜è™Ÿé–‹é ­çš„è­¦å‘Šæ–‡å­—ç­‰ï¼‰
5. å®Œå…¨å¿½ç•¥åƒ¹æ ¼ç›¸é—œå…§å®¹ï¼ˆå††ã€æ—¥åœ“ã€OFFã€å‰²å¼•ã€å€¤ä¸‹ã’ç­‰ï¼‰
6. åªå›å‚³ JSON"""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
            json={
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è¼¸å‡ºç¦æ­¢ä»»ä½•æ—¥æ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0,
                "max_tokens": 1500
            },
            timeout=60
        )
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content'].strip()
            if content.startswith('```'):
                content = content.split('\n', 1)[1]
            if content.endswith('```'):
                content = content.rsplit('```', 1)[0]
            
            translated = json.loads(content.strip())
            
            trans_title = translated.get('title', title)
            trans_desc = translated.get('description', description)
            trans_size = translated.get('size_spec_translated', '')
            
            if contains_japanese(trans_title):
                trans_title = remove_japanese(trans_title)
            if contains_japanese(trans_desc):
                trans_desc = remove_japanese(trans_desc)
            
            if not trans_title.startswith('BAPE'):
                trans_title = f"BAPE {trans_title}"
            
            size_html = build_size_table_html(trans_size) if trans_size else ''
            if size_html:
                trans_desc += '<br><br>' + size_html
            
            return {'success': True, 'title': trans_title, 'description': trans_desc}
        
        return {'success': False, 'title': f"BAPE {title}", 'description': description}
            
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {'success': False, 'title': f"BAPE {title}", 'description': description}


def build_size_table_html(size_spec_text):
    if not size_spec_text:
        return ''
    
    lines = [line.strip() for line in size_spec_text.strip().split('\n') if line.strip()]
    if not lines:
        return ''
    
    html = '<div class="size-spec"><h3>ğŸ“ å°ºå¯¸è¦æ ¼</h3>'
    html += '<table style="border-collapse:collapse;width:100%;margin:10px 0;">'
    
    for i, line in enumerate(lines):
        cells = [c.strip() for c in line.split('|')]
        if i == 0:
            html += '<tr style="background:#f5f5f5;">'
            for cell in cells:
                html += f'<th style="border:1px solid #ddd;padding:8px;text-align:center;">{cell}</th>'
            html += '</tr>'
        else:
            html += '<tr>'
            for j, cell in enumerate(cells):
                style = 'border:1px solid #ddd;padding:8px;'
                style += 'font-weight:bold;background:#fafafa;' if j == 0 else 'text-align:center;'
                html += f'<td style="{style}">{cell}</td>'
            html += '</tr>'
    
    html += '</table><p style="font-size:12px;color:#666;">â€» å°ºå¯¸å¯èƒ½æœ‰äº›è¨±èª¤å·®</p></div>'
    return html


def clean_description(description):
    description = re.sub(r'<a[^>]*>.*?</a>', '', description)
    description = re.sub(r'[^<>]*\d+[,ï¼Œ]?\d*\s*æ—¥åœ“[^<>]*', '', description)
    description = re.sub(r'[^<>]*\d+[,ï¼Œ]?\d*\s*å††[^<>]*', '', description)
    description = re.sub(r'[^<>]*\d+%\s*OFF[^<>]*', '', description, flags=re.IGNORECASE)
    description = re.sub(r'[^<>]*é™åƒ¹[^<>]*', '', description)
    description = re.sub(r'[^<>]*å¤§å¹…[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ³¨æ„äº‹é …[^<>]*', '', description)
    description = re.sub(r'[^<>]*è«‹æ³¨æ„[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ•¬è«‹è«’è§£[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ•¬è«‹è¦‹è«’[^<>]*', '', description)
    description = re.sub(r'[^<>]*â€»[^<>]*', '', description)
    description = re.sub(r'<p>\s*</p>', '', description)
    description = re.sub(r'<br\s*/?>\s*<br\s*/?>', '<br>', description)
    description = re.sub(r'^\s*(<br\s*/?>)+', '', description)
    description = re.sub(r'(<br\s*/?>)+\s*$', '', description)
    description = description.strip()
    
    notice = """
<br><br>
<p><strong>ã€è«‹æ³¨æ„ä»¥ä¸‹äº‹é …ã€‘</strong></p>
<p>â€»ä¸æ¥å—é€€æ›è²¨</p>
<p>â€»é–‹ç®±è«‹å…¨ç¨‹éŒ„å½±</p>
<p>â€»å› åº«å­˜æœ‰é™ï¼Œè¨‚è³¼æ™‚é–“ä¸åŒå¯èƒ½æœƒå‡ºç¾ç¼ºè²¨æƒ…æ³ã€‚</p>
"""
    return description + notice


# ========== çˆ¬å–å‡½æ•¸ ==========

def fetch_products_json(page=1):
    """å¾ BAPE ç¶²ç«™å–å¾—æ‰€æœ‰å•†å“ï¼ˆJSON APIï¼‰"""
    url = f"{SOURCE_URL}/collections/all/products.json?page={page}&limit=50"
    
    print(f"[çˆ¬å–] ç¬¬ {page} é : {url}")
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        print(f"[çˆ¬å–] HTTP ç‹€æ…‹: {response.status_code}")
        
        if response.status_code != 200:
            return []
        
        data = response.json()
        products = data.get('products', [])
        print(f"[çˆ¬å–] ç¬¬ {page} é å–å¾— {len(products)} å€‹å•†å“")
        return products
        
    except Exception as e:
        print(f"[éŒ¯èª¤] {e}")
        import traceback
        traceback.print_exc()
        return []


def get_product_category(product):
    """æ ¹æ“šå•†å“è³‡è¨Šåˆ¤æ–·åˆ†é¡"""
    tags = product.get('tags', [])
    title = product.get('title', '').upper()
    
    # å„ªå…ˆæª¢æŸ¥ tags
    tags_str = ','.join(tags).lower() if isinstance(tags, list) else str(tags).lower()
    
    # ç«¥è£åˆ¤æ–·ï¼ˆå„ªå…ˆï¼Œå› ç‚ºç«¥è£å•†å“åç¨±å¯èƒ½ä¸å« KIDSï¼‰
    if 'ã‚­ãƒƒã‚º' in tags_str or 'kids' in tags_str or 'KIDS' in title or 'ã‚­ãƒƒã‚º' in title:
        return 'kids'
    
    # å¥³è£åˆ¤æ–·
    if 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹' in tags_str or 'ladies' in tags_str or 'women' in tags_str or 'LADIES' in title or 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹' in title:
        return 'womens'
    
    # ç”·è£åˆ¤æ–·ï¼ˆé è¨­ï¼‰
    if 'ãƒ¡ãƒ³ã‚º' in tags_str or 'mens' in tags_str or 'men' in tags_str or 'ãƒ¡ãƒ³ã‚º' in title:
        return 'mens'
    
    # ç„¡æ³•åˆ¤æ–·æ™‚é è¨­ç‚ºç”·è£
    return 'mens'


def fetch_all_products_by_category():
    """å–å¾—æ‰€æœ‰å•†å“ä¸¦æŒ‰åˆ†é¡æ•´ç†"""
    all_products = {'mens': [], 'womens': [], 'kids': []}
    page = 1
    seen_handles = set()
    
    while True:
        products = fetch_products_json(page)
        
        if not products:
            break
        
        new_count = 0
        for p in products:
            handle = p.get('handle', '')
            if handle in seen_handles:
                continue
            seen_handles.add(handle)
            
            # æª¢æŸ¥åº«å­˜
            has_stock = any(v.get('available', False) for v in p.get('variants', []))
            if not has_stock:
                continue
            
            # åˆ¤æ–·åˆ†é¡
            category = get_product_category(p)
            all_products[category].append(p)
            new_count += 1
        
        print(f"[çˆ¬å–] ç¬¬ {page} é æ–°å¢ {new_count} å€‹æœ‰åº«å­˜å•†å“")
        
        if len(products) < 50:
            break
        
        page += 1
        time.sleep(0.5)
    
    print(f"[çˆ¬å–] åˆ†é¡çµæœ: ç”·è£ {len(all_products['mens'])}, å¥³è£ {len(all_products['womens'])}, ç«¥è£ {len(all_products['kids'])}")
    return all_products


def fetch_category_products_html(category_key, page=1):
    """å¾ BAPE ç¶²ç«™ HTML é é¢å–å¾—å•†å“åˆ—è¡¨ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰"""
    cat_info = CATEGORIES[category_key]
    
    if page == 1:
        url = f"{SOURCE_URL}{cat_info['base_url']}?{cat_info['filter']}"
    else:
        url = f"{SOURCE_URL}{cat_info['base_url']}?{cat_info['filter']}&page={page}"
    
    print(f"[çˆ¬å– HTML] {cat_info['name']} ç¬¬ {page} é : {url}")
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        print(f"[çˆ¬å– HTML] HTTP ç‹€æ…‹: {response.status_code}")
        
        if response.status_code != 200:
            return [], False
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        product_handles = []
        all_links = soup.find_all('a', href=True)
        
        for link in all_links:
            href = link.get('href', '')
            if '/products/' in href:
                match = re.search(r'/products/([^/?#]+)', href)
                if match:
                    handle = match.group(1)
                    if handle not in product_handles and handle != 'products':
                        product_handles.append(handle)
        
        print(f"[çˆ¬å– HTML] æ‰¾åˆ° {len(product_handles)} å€‹å•†å“")
        
        has_next_page = False
        for link in all_links:
            if f'page={page + 1}' in link.get('href', ''):
                has_next_page = True
                break
        
        return product_handles, has_next_page
        
    except Exception as e:
        print(f"[éŒ¯èª¤] {e}")
        return [], False


def fetch_product_json(handle):
    """å–å¾—å–®ä¸€å•†å“çš„ JSON è³‡æ–™"""
    url = f"{SOURCE_URL}/products/{handle}.json"
    
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            data = response.json()
            return data.get('product')
    except Exception as e:
        print(f"[éŒ¯èª¤] å–å¾—å•†å“ {handle} å¤±æ•—: {e}")
    
    return None


def fetch_all_category_products(category_key):
    """å–å¾—åˆ†é¡å…§æ‰€æœ‰å•†å“ï¼ˆä½¿ç”¨ JSON API + ç¨‹å¼éæ¿¾ï¼‰"""
    all_by_category = fetch_all_products_by_category()
    products = all_by_category.get(category_key, [])
    print(f"[çˆ¬å–] {CATEGORIES[category_key]['name']} å…± {len(products)} å€‹æœ‰åº«å­˜å•†å“")
    return products


def fetch_size_table(handle):
    try:
        url = f"{SOURCE_URL}/products/{handle}"
        response = requests.get(url, headers={'User-Agent': HEADERS['User-Agent'], 'Accept': 'text/html'}, timeout=30)
        
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        def_list = soup.find('dl', class_='s-product-detail__def-list-description')
        if not def_list:
            return None
        
        size_dt = def_list.find('dt', string=re.compile(r'ã‚µã‚¤ã‚º'))
        if not size_dt:
            return None
        
        size_dd = size_dt.find_next_sibling('dd')
        if not size_dd:
            return None
        
        table = size_dd.find('table')
        if not table:
            return None
        
        rows = table.find_all('tr')
        return '\n'.join([' | '.join([cell.get_text(strip=True) for cell in row.find_all(['th', 'td'])]) for row in rows])
    except:
        return None


# ========== JSONL ç”Ÿæˆ ==========

def product_to_jsonl_entry(product, category_key, collection_id, existing_product_id=None):
    """å°‡å•†å“è³‡æ–™è½‰æ›ç‚º JSONL æ ¼å¼ï¼Œæ”¯æ´æ›´æ–°ç¾æœ‰å•†å“"""
    cat_info = CATEGORIES[category_key]
    
    title = product.get('title', '')
    body_html = product.get('body_html', '')
    handle = product.get('handle', '')
    source_url = f"{SOURCE_URL}/products/{handle}"
    
    size_spec = fetch_size_table(handle)
    translated = translate_with_chatgpt(title, body_html, size_spec or '')
    trans_title = translated['title']
    trans_desc = clean_description(translated['description'])
    
    options = product.get('options', [])
    source_variants = product.get('variants', [])
    images = product.get('images', [])
    
    has_options = len(options) > 0 and not (len(options) == 1 and options[0].get('name') == 'Title')
    
    product_options = []
    if has_options:
        for opt in options:
            opt_name = opt.get('name', '')
            opt_values = opt.get('values', [])
            if opt_values and opt_name != 'Title':
                product_options.append({"name": opt_name, "values": [{"name": v} for v in opt_values]})
    
    image_list = [img['src'] for img in images[:10]] if images else []
    first_image = image_list[0] if image_list else None
    
    files = [{"originalSource": img_url, "contentType": "IMAGE"} for img_url in image_list]
    variant_file = {"originalSource": first_image, "contentType": "IMAGE"} if first_image else None
    
    variants = []
    for sv in source_variants:
        if not sv.get('available', False):
            continue
        
        cost = float(sv.get('price', 0))
        if cost < MIN_PRICE:
            continue
        
        weight = float(sv.get('grams', 0)) / 1000 if sv.get('grams') else DEFAULT_WEIGHT
        selling_price = calculate_selling_price(cost, weight)
        
        sku_parts = [f"bape-{handle}"]
        option_values = []
        
        if sv.get('option1') and len(options) > 0 and options[0].get('name') != 'Title':
            sku_parts.append(sv['option1'])
            option_values.append({"optionName": options[0]['name'], "name": sv['option1']})
        if sv.get('option2') and len(options) > 1:
            sku_parts.append(sv['option2'])
            option_values.append({"optionName": options[1]['name'], "name": sv['option2']})
        if sv.get('option3') and len(options) > 2:
            sku_parts.append(sv['option3'])
            option_values.append({"optionName": options[2]['name'], "name": sv['option3']})
        
        variant = {
            "price": selling_price,
            "sku": '-'.join(sku_parts),
            "inventoryPolicy": "CONTINUE",
            "taxable": False,
            "inventoryItem": {"cost": cost}
        }
        
        if option_values:
            variant["optionValues"] = option_values
        if variant_file:
            variant["file"] = variant_file
        
        variants.append(variant)
    
    if not variants:
        return None
    
    seo_title = f"{trans_title} | BAPE æ—¥æœ¬ä»£è³¼"
    seo_description = f"æ—¥æœ¬ A BATHING APE å®˜æ–¹æ­£å“ä»£è³¼ã€‚{trans_title}ï¼Œå°ç£ç¾è²¨æˆ–æ—¥æœ¬ç›´é€ï¼Œå“è³ªä¿è­‰ã€‚GOYOUTATI å¾¡ç”¨é”æ—¥æœ¬ä¼´æ‰‹ç¦®å°ˆé–€åº—ã€‚"
    
    product_input = {
        "title": trans_title,
        "descriptionHtml": trans_desc,
        "vendor": "BAPE",
        "productType": cat_info['product_type'],
        "status": "ACTIVE",
        "handle": f"bape-{handle}",
        "tags": cat_info['tags'],
        "seo": {"title": seo_title, "description": seo_description},
        "metafields": [{"namespace": "custom", "key": "link", "value": source_url, "type": "url"}]
    }
    
    # å¦‚æœå•†å“å·²å­˜åœ¨ï¼ŒåŠ å…¥ id ä¾†æ›´æ–°
    if existing_product_id:
        product_input["id"] = existing_product_id
    
    if collection_id:
        product_input["collections"] = [collection_id]
    if product_options:
        product_input["productOptions"] = product_options
    if variants:
        product_input["variants"] = variants
    if files:
        product_input["files"] = files
    
    return {"productSet": product_input, "synchronous": True}


# ========== Bulk Operations ==========

def create_staged_upload():
    query = """
    mutation stagedUploadsCreate($input: [StagedUploadInput!]!) {
      stagedUploadsCreate(input: $input) {
        stagedTargets { url resourceUrl parameters { name value } }
        userErrors { field message }
      }
    }
    """
    result = graphql_request(query, {"input": [{"resource": "BULK_MUTATION_VARIABLES", "filename": "products.jsonl", "mimeType": "text/jsonl", "httpMethod": "POST"}]})
    targets = result.get('data', {}).get('stagedUploadsCreate', {}).get('stagedTargets', [])
    return targets[0] if targets else None


def upload_jsonl_to_staged(staged_target, jsonl_path):
    url = staged_target['url']
    params = {p['name']: p['value'] for p in staged_target['parameters']}
    with open(jsonl_path, 'rb') as f:
        response = requests.post(url, data=params, files={'file': ('products.jsonl', f, 'text/jsonl')}, timeout=300)
    return response.status_code in [200, 201, 204]


def run_bulk_mutation(staged_upload_path):
    query = """
    mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) {
      bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) {
        bulkOperation { id status }
        userErrors { field message }
      }
    }
    """
    mutation = """
    mutation call($productSet: ProductSetInput!, $synchronous: Boolean!) {
      productSet(synchronous: $synchronous, input: $productSet) {
        product { id title }
        userErrors { field message }
      }
    }
    """
    return graphql_request(query, {"mutation": mutation, "stagedUploadPath": staged_upload_path})


def check_bulk_operation_status(operation_id=None):
    if operation_id:
        query = """query($id: ID!) { node(id: $id) { ... on BulkOperation { id status errorCode objectCount url } } }"""
        result = graphql_request(query, {"id": operation_id})
        return result.get('data', {}).get('node', {})
    else:
        query = """{ currentBulkOperation(type: MUTATION) { id status errorCode objectCount url } }"""
        result = graphql_request(query)
        return result.get('data', {}).get('currentBulkOperation', {})


# ========== å•†å“ç®¡ç† ==========

def get_all_publications():
    query = """{ publications(first: 20) { edges { node { id name } } } }"""
    result = graphql_request(query)
    return [{'id': edge['node']['id'], 'name': edge['node']['name']} for edge in result.get('data', {}).get('publications', {}).get('edges', [])]


def get_product_id_by_handle(handle):
    """æ ¹æ“š handle æŸ¥è©¢å•†å“ ID"""
    query = """
    query getProductByHandle($handle: String!) {
      productByHandle(handle: $handle) {
        id
        title
      }
    }
    """
    result = graphql_request(query, {"handle": handle})
    product = result.get('data', {}).get('productByHandle')
    if product:
        return product['id']
    return None


def fetch_bape_product_ids():
    all_products = []
    cursor = None
    
    while True:
        if cursor:
            query = """query($cursor: String) { products(first: 250, after: $cursor, query: "vendor:BAPE") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }"""
            result = graphql_request(query, {"cursor": cursor})
        else:
            query = """{ products(first: 250, query: "vendor:BAPE") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }"""
            result = graphql_request(query)
        
        products = result.get('data', {}).get('products', {})
        edges = products.get('edges', [])
        
        for edge in edges:
            node = edge['node']
            all_products.append({'id': node['id'], 'title': node['title'], 'handle': node['handle'], 'status': node['status']})
            cursor = edge['cursor']
        
        if not products.get('pageInfo', {}).get('hasNextPage', False):
            break
        time.sleep(0.5)
    
    return all_products


def set_product_to_draft(product_id):
    mutation = """mutation productUpdate($input: ProductInput!) { productUpdate(input: $input) { product { id status } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"id": product_id, "status": "DRAFT"}})
    return not result.get('data', {}).get('productUpdate', {}).get('userErrors', [])


def delete_product(product_id):
    """åˆªé™¤å–®ä¸€å•†å“"""
    mutation = """
    mutation productDelete($input: ProductDeleteInput!) {
      productDelete(input: $input) {
        deletedProductId
        userErrors { field message }
      }
    }
    """
    result = graphql_request(mutation, {"input": {"id": product_id}})
    user_errors = result.get('data', {}).get('productDelete', {}).get('userErrors', [])
    return not user_errors


def delete_all_bape_products():
    """åˆªé™¤æ‰€æœ‰ BAPE å•†å“"""
    global scrape_status
    
    print("[DELETE] é–‹å§‹åˆªé™¤æ‰€æœ‰ BAPE å•†å“...")
    
    products = fetch_bape_product_ids()
    total = len(products)
    
    if total == 0:
        return {'success': True, 'deleted': 0, 'message': 'æ²’æœ‰ BAPE å•†å“'}
    
    print(f"[DELETE] æ‰¾åˆ° {total} å€‹ BAPE å•†å“")
    
    deleted = 0
    failed = 0
    
    for i, product in enumerate(products):
        scrape_status['current_product'] = f"åˆªé™¤ä¸­ [{i+1}/{total}] {product.get('title', '')[:30]}"
        scrape_status['progress'] = i + 1
        
        if delete_product(product['id']):
            deleted += 1
            print(f"[DELETE] å·²åˆªé™¤: {product.get('title', '')[:30]}")
        else:
            failed += 1
            print(f"[DELETE] åˆªé™¤å¤±æ•—: {product.get('title', '')[:30]}")
        
        time.sleep(0.2)
    
    return {'success': True, 'deleted': deleted, 'failed': failed, 'total': total}


def batch_publish_bape_products():
    products = fetch_bape_product_ids()
    if not products:
        return {'success': False, 'error': 'No products'}
    
    publications = get_all_publications()
    if not publications:
        return {'success': False, 'error': 'No publications'}
    
    publication_inputs = [{"publicationId": pub['id']} for pub in publications]
    results = {'total': len(products), 'success': 0, 'failed': 0, 'errors': []}
    
    mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
    
    for product in products:
        result = graphql_request(mutation, {"id": product['id'], "input": publication_inputs})
        if result.get('data', {}).get('publishablePublish', {}).get('userErrors', []):
            results['failed'] += 1
        else:
            results['success'] += 1
        time.sleep(0.1)
    
    return results


# ========== ä¸»æµç¨‹ ==========

def run_test_single(category='mens'):
    global scrape_status
    
    scrape_status = {"running": True, "phase": "testing", "progress": 0, "total": 1, "current_product": "æ¸¬è©¦å–®å“...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "set_to_draft": 0}
    
    try:
        cat_info = CATEGORIES[category]
        print(f"[TEST] åˆ†é¡: {category}, Collection: {cat_info['collection']}")
        
        scrape_status['current_product'] = f"å–å¾— Collection..."
        collection_id = get_or_create_collection(cat_info['collection'])
        print(f"[TEST] Collection ID: {collection_id}")
        
        if not collection_id:
            scrape_status['errors'].append({'error': 'ç„¡æ³•å»ºç«‹ Collection'})
            scrape_status['current_product'] = 'âŒ ç„¡æ³•å»ºç«‹ Collection'
            return
        
        scrape_status['current_product'] = f"çˆ¬å–å•†å“..."
        
        # å–å¾—æ‰€æœ‰å•†å“ä¸¦åˆ†é¡
        all_by_category = fetch_all_products_by_category()
        products = all_by_category.get(category, [])
        
        print(f"[TEST] {category} æœ‰ {len(products)} å€‹å•†å“")
        
        if not products:
            scrape_status['errors'].append({'error': f'æ²’æœ‰æ‰¾åˆ° {cat_info["name"]} çš„å•†å“'})
            scrape_status['current_product'] = f'âŒ æ²’æœ‰æ‰¾åˆ° {cat_info["name"]} çš„å•†å“'
            return
        
        # å–ç¬¬ä¸€å€‹ç¬¦åˆæ¢ä»¶çš„å•†å“
        test_product = None
        for p in products:
            min_price = min((float(v.get('price', 0)) for v in p.get('variants', [])), default=0)
            if min_price >= MIN_PRICE:
                test_product = p
                break
        
        if not test_product:
            scrape_status['errors'].append({'error': f'æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å•†å“ï¼ˆåƒ¹æ ¼ >= {MIN_PRICE}ï¼‰'})
            scrape_status['current_product'] = 'âŒ æ²’æœ‰ç¬¦åˆæ¢ä»¶çš„å•†å“'
            return
        
        print(f"[TEST] æ¸¬è©¦å•†å“: {test_product.get('title', '')[:50]}")
        
        # æª¢æŸ¥å•†å“æ˜¯å¦å·²å­˜åœ¨
        my_handle = f"bape-{test_product.get('handle', '')}"
        existing_id = get_product_id_by_handle(my_handle)
        if existing_id:
            print(f"[TEST] å•†å“å·²å­˜åœ¨ï¼Œå°‡é€²è¡Œæ›´æ–°: {existing_id}")
        else:
            print(f"[TEST] å•†å“ä¸å­˜åœ¨ï¼Œå°‡å»ºç«‹æ–°å•†å“")
        
        scrape_status['current_product'] = f"ç¿»è­¯: {test_product['title'][:30]}..."
        entry = product_to_jsonl_entry(test_product, category, collection_id, existing_id)
        
        if not entry:
            scrape_status['errors'].append({'error': 'å•†å“è½‰æ›å¤±æ•—ï¼ˆå¯èƒ½æ²’æœ‰æœ‰æ•ˆçš„ variantï¼‰'})
            scrape_status['current_product'] = 'âŒ å•†å“è½‰æ›å¤±æ•—'
            return
        
        product_input = entry['productSet']
        print(f"[TEST] è½‰æ›æˆåŠŸ: {product_input['title']}")
        print(f"[TEST] Variants: {len(product_input.get('variants', []))}")
        
        scrape_status['products'].append({'title': product_input['title'], 'handle': product_input['handle'], 'variants': len(product_input.get('variants', []))})
        
        scrape_status['current_product'] = "ä¸Šå‚³åˆ° Shopify..."
        mutation = """mutation productSet($input: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $input) { product { id title handle } userErrors { field code message } } }"""
        
        # é™¤éŒ¯ï¼šæ‰“å°å®Œæ•´çš„ product_input
        print(f"[TEST] ===== ProductSet Input =====")
        print(f"[TEST] title: {product_input.get('title')}")
        print(f"[TEST] handle: {product_input.get('handle')}")
        print(f"[TEST] vendor: {product_input.get('vendor')}")
        print(f"[TEST] productType: {product_input.get('productType')}")
        print(f"[TEST] productOptions: {product_input.get('productOptions')}")
        print(f"[TEST] variants count: {len(product_input.get('variants', []))}")
        if product_input.get('variants'):
            print(f"[TEST] first variant: {product_input['variants'][0]}")
        print(f"[TEST] collections: {product_input.get('collections')}")
        print(f"[TEST] ================================")
        
        result = graphql_request(mutation, {"input": product_input, "synchronous": True})
        
        # é™¤éŒ¯ï¼šæ‰“å°å®Œæ•´å›æ‡‰
        print(f"[TEST] ===== GraphQL Response =====")
        print(f"[TEST] {json.dumps(result, ensure_ascii=False, indent=2)[:2000]}")
        print(f"[TEST] ================================")
        
        product_set = result.get('data', {}).get('productSet', {})
        user_errors = product_set.get('userErrors', [])
        
        if user_errors:
            error_msg = '; '.join([e.get('message', str(e)) for e in user_errors])
            scrape_status['errors'].append({'error': error_msg})
            scrape_status['current_product'] = f"âŒ å¤±æ•—: {error_msg}"
            print(f"[ERROR] productSet å¤±æ•—: {error_msg}")
        else:
            product = product_set.get('product', {})
            publications = get_all_publications()
            if publications:
                pub_mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
                graphql_request(pub_mutation, {"id": product['id'], "input": [{"publicationId": pub['id']} for pub in publications]})
            scrape_status['current_product'] = f"âœ… æˆåŠŸï¼{product.get('title', '')}"
        
        scrape_status['progress'] = 1
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        import traceback
        traceback.print_exc()
    finally:
        scrape_status['running'] = False


def run_scrape(category):
    global scrape_status
    
    scrape_status = {"running": True, "phase": "scraping", "progress": 0, "total": 0, "current_product": "", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "set_to_draft": 0}
    
    try:
        categories_to_scrape = ['mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not categories_to_scrape:
            scrape_status['errors'].append({'error': f'æœªçŸ¥åˆ†é¡: {category}'})
            return
        
        # å…ˆå–å¾—æ‰€æœ‰ç¾æœ‰å•†å“çš„ handle -> id æ˜ å°„
        scrape_status['current_product'] = 'å–å¾—ç¾æœ‰å•†å“...'
        existing_products = fetch_bape_product_ids()
        existing_handles = {p['handle']: p['id'] for p in existing_products}
        print(f"[SCRAPE] ç¾æœ‰ {len(existing_handles)} å€‹ BAPE å•†å“")
        
        all_jsonl_entries = []
        
        for cat_key in categories_to_scrape:
            cat_info = CATEGORIES[cat_key]
            collection_id = get_or_create_collection(cat_info['collection'])
            if not collection_id:
                continue
            
            products = fetch_all_category_products(cat_key)
            if not products:
                continue
            
            scrape_status['total'] += len(products)
            
            for product in products:
                scrape_status['progress'] += 1
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {product.get('title', '')[:30]}"
                
                try:
                    my_handle = f"bape-{product.get('handle', '')}"
                    existing_id = existing_handles.get(my_handle)
                    
                    entry = product_to_jsonl_entry(product, cat_key, collection_id, existing_id)
                    if entry:
                        all_jsonl_entries.append(entry)
                        scrape_status['products'].append({'title': entry['productSet']['title'], 'handle': entry['productSet']['handle'], 'variants': len(entry['productSet'].get('variants', []))})
                except Exception as e:
                    scrape_status['errors'].append({'error': str(e)})
                
                time.sleep(0.5)
        
        if all_jsonl_entries:
            jsonl_path = os.path.join(JSONL_DIR, f"bape_{category}_{int(time.time())}.jsonl")
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for entry in all_jsonl_entries:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            scrape_status['jsonl_file'] = jsonl_path
        
        scrape_status['current_product'] = f"å®Œæˆï¼å…± {len(all_jsonl_entries)} å€‹å•†å“"
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False
        scrape_status['phase'] = "completed"


def run_bulk_upload(jsonl_path):
    global scrape_status
    
    scrape_status['phase'] = 'uploading'
    scrape_status['running'] = True
    
    try:
        staged = create_staged_upload()
        if not staged or not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³å¤±æ•—'})
            return
        
        staged_path = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), staged.get('resourceUrl', ''))
        result = run_bulk_mutation(staged_path)
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡æ“ä½œå·²å•Ÿå‹•: {bulk_op.get('status', '')}"
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False


def run_test_sync(category='all', limit=10):
    """æ¸¬è©¦ä¸Šæ¶ï¼ˆæ¯å€‹åˆ†é¡åªæŠ“ limit å€‹å•†å“ï¼‰"""
    global scrape_status
    
    print(f"[TEST SYNC] ========== æ¸¬è©¦ä¸Šæ¶ (æ¯åˆ†é¡ {limit} å€‹) ==========")
    
    scrape_status = {"running": True, "phase": "test_sync", "progress": 0, "total": 0, "current_product": "é–‹å§‹æ¸¬è©¦ä¸Šæ¶...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "set_to_draft": 0}
    
    try:
        # å–å¾—ç¾æœ‰å•†å“
        scrape_status['current_product'] = 'å–å¾—ç¾æœ‰å•†å“...'
        existing_products = fetch_bape_product_ids()
        existing_handles = {p['handle']: p for p in existing_products}
        print(f"[TEST SYNC] ç¾æœ‰ {len(existing_handles)} å€‹å•†å“")
        
        # å–å¾—æ‰€æœ‰å•†å“ä¸¦åˆ†é¡
        scrape_status['current_product'] = 'çˆ¬å–å•†å“...'
        all_by_category = fetch_all_products_by_category()
        
        categories_to_scrape = ['mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not categories_to_scrape:
            raise Exception(f'æœªçŸ¥åˆ†é¡: {category}')
        
        all_jsonl_entries = []
        
        for cat_key in categories_to_scrape:
            cat_info = CATEGORIES[cat_key]
            scrape_status['current_product'] = f"è™•ç† {cat_info['collection']} (é™ {limit} å€‹)..."
            
            collection_id = get_or_create_collection(cat_info['collection'])
            if not collection_id:
                scrape_status['errors'].append({'error': f"ç„¡æ³•å–å¾— Collection: {cat_info['collection']}"})
                continue
            
            # å–å¾—è©²åˆ†é¡çš„å•†å“ï¼Œé™åˆ¶æ•¸é‡
            products = all_by_category.get(cat_key, [])[:limit]
            print(f"[TEST SYNC] {cat_info['collection']} å– {len(products)} å€‹å•†å“")
            
            if not products:
                scrape_status['errors'].append({'error': f"{cat_info['collection']} æ²’æœ‰æ‰¾åˆ°å•†å“"})
                continue
            
            scrape_status['total'] += len(products)
            
            for product in products:
                scrape_status['progress'] += 1
                handle = product.get('handle', '')
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {product.get('title', '')[:30]}"
                
                my_handle = f"bape-{handle}"
                existing_info = existing_handles.get(my_handle)
                existing_id = existing_info['id'] if existing_info else None
                
                try:
                    entry = product_to_jsonl_entry(product, cat_key, collection_id, existing_id)
                    if entry:
                        all_jsonl_entries.append(entry)
                        scrape_status['products'].append({
                            'title': entry['productSet']['title'],
                            'handle': entry['productSet']['handle'],
                            'variants': len(entry['productSet'].get('variants', []))
                        })
                        print(f"[TEST SYNC] å·²åŠ å…¥: {entry['productSet']['title'][:30]}")
                except Exception as e:
                    print(f"[TEST SYNC] è½‰æ›å¤±æ•—: {e}")
                    scrape_status['errors'].append({'error': str(e)})
                
                time.sleep(0.3)
        
        print(f"[TEST SYNC] ç¸½å…± {len(all_jsonl_entries)} å€‹å•†å“")
        
        if not all_jsonl_entries:
            raise Exception('æ²’æœ‰çˆ¬å–åˆ°å•†å“')
        
        # å¯«å…¥ JSONL
        jsonl_path = os.path.join(JSONL_DIR, f"bape_test_{category}_{int(time.time())}.jsonl")
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for entry in all_jsonl_entries:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        scrape_status['jsonl_file'] = jsonl_path
        
        # æ‰¹é‡ä¸Šå‚³
        print(f"[TEST SYNC] æ‰¹é‡ä¸Šå‚³ {len(all_jsonl_entries)} å€‹å•†å“...")
        scrape_status['current_product'] = 'æ‰¹é‡ä¸Šå‚³...'
        scrape_status['phase'] = 'uploading'
        
        staged = create_staged_upload()
        if not staged or not upload_jsonl_to_staged(staged, jsonl_path):
            raise Exception('ä¸Šå‚³å¤±æ•—')
        
        staged_path = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), '')
        result = run_bulk_mutation(staged_path)
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        if user_errors:
            raise Exception(f'Bulk Mutation éŒ¯èª¤: {user_errors}')
        
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        
        # ç­‰å¾…å®Œæˆ
        scrape_status['current_product'] = 'ç­‰å¾…å®Œæˆ...'
        for _ in range(60):
            status = check_bulk_operation_status()
            scrape_status['bulk_status'] = status.get('status', '')
            if status.get('status') == 'COMPLETED':
                break
            elif status.get('status') in ['FAILED', 'CANCELED']:
                raise Exception(f'Bulk å¤±æ•—: {status.get("status")}')
            time.sleep(5)
        
        # ç™¼å¸ƒ
        scrape_status['current_product'] = 'ç™¼å¸ƒ...'
        scrape_status['phase'] = 'publishing'
        batch_publish_bape_products()
        
        scrape_status['current_product'] = f"âœ… æ¸¬è©¦å®Œæˆï¼ä¸Šå‚³ {len(all_jsonl_entries)} å€‹å•†å“"
        scrape_status['phase'] = 'completed'
        
        return {'success': True, 'total_products': len(all_jsonl_entries)}
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        scrape_status['current_product'] = f"âŒ éŒ¯èª¤: {str(e)}"
        scrape_status['phase'] = 'error'
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}
    finally:
        scrape_status['running'] = False


def run_full_sync(category='all'):
    global scrape_status
    
    print(f"[CRON] ========== é–‹å§‹åŒæ­¥ ==========")
    
    scrape_status = {"running": True, "phase": "cron_sync", "progress": 0, "total": 0, "current_product": "é–‹å§‹...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "set_to_draft": 0}
    
    try:
        # å–å¾— Shopify ç¾æœ‰å•†å“ï¼ˆåŒ…å« idï¼‰
        scrape_status['current_product'] = 'å–å¾— Shopify ç¾æœ‰å•†å“...'
        existing_products = fetch_bape_product_ids()
        existing_handles = {p['handle']: p for p in existing_products}
        print(f"[CRON] Shopify ç¾æœ‰ {len(existing_handles)} å€‹å•†å“")
        
        # å–å¾—æ‰€æœ‰å•†å“ä¸¦åˆ†é¡
        scrape_status['current_product'] = 'çˆ¬å–å•†å“...'
        all_by_category = fetch_all_products_by_category()
        
        categories_to_scrape = ['mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not categories_to_scrape:
            raise Exception(f'æœªçŸ¥åˆ†é¡: {category}')
        
        all_jsonl_entries = []
        scraped_handles = set()
        
        for cat_key in categories_to_scrape:
            cat_info = CATEGORIES[cat_key]
            scrape_status['current_product'] = f"è™•ç† {cat_info['collection']}..."
            
            collection_id = get_or_create_collection(cat_info['collection'])
            if not collection_id:
                continue
            
            products = all_by_category.get(cat_key, [])
            print(f"[CRON] {cat_info['collection']} å…± {len(products)} å€‹å•†å“")
            
            scrape_status['total'] += len(products)
            
            for product in products:
                scrape_status['progress'] += 1
                handle = product.get('handle', '')
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {product.get('title', '')[:30]}"
                
                my_handle = f"bape-{handle}"
                scraped_handles.add(my_handle)
                
                existing_info = existing_handles.get(my_handle)
                existing_id = existing_info['id'] if existing_info else None
                
                try:
                    entry = product_to_jsonl_entry(product, cat_key, collection_id, existing_id)
                    if entry:
                        all_jsonl_entries.append(entry)
                        scrape_status['products'].append({
                            'title': entry['productSet']['title'],
                            'handle': entry['productSet']['handle'],
                            'variants': len(entry['productSet'].get('variants', []))
                        })
                except Exception as e:
                    scrape_status['errors'].append({'error': str(e)})
                
                time.sleep(0.3)
        
        if not all_jsonl_entries:
            raise Exception('æ²’æœ‰çˆ¬å–åˆ°å•†å“')
        
        # å¯«å…¥ JSONL
        jsonl_path = os.path.join(JSONL_DIR, f"bape_{category}_{int(time.time())}.jsonl")
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for entry in all_jsonl_entries:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        scrape_status['jsonl_file'] = jsonl_path
        
        # æ‰¹é‡ä¸Šå‚³
        print(f"[CRON] æ‰¹é‡ä¸Šå‚³...")
        scrape_status['current_product'] = 'æ‰¹é‡ä¸Šå‚³...'
        scrape_status['phase'] = 'uploading'
        
        staged = create_staged_upload()
        if not staged or not upload_jsonl_to_staged(staged, jsonl_path):
            raise Exception('ä¸Šå‚³å¤±æ•—')
        
        staged_path = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), '')
        result = run_bulk_mutation(staged_path)
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        if user_errors:
            raise Exception(f'Bulk Mutation éŒ¯èª¤: {user_errors}')
        
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        
        # ç­‰å¾…å®Œæˆ
        print(f"[CRON] ç­‰å¾…å®Œæˆ...")
        scrape_status['current_product'] = 'ç­‰å¾…å®Œæˆ...'
        
        for _ in range(120):
            status = check_bulk_operation_status()
            scrape_status['bulk_status'] = status.get('status', '')
            if status.get('status') == 'COMPLETED':
                break
            elif status.get('status') in ['FAILED', 'CANCELED']:
                raise Exception(f'Bulk å¤±æ•—: {status.get("status")}')
            time.sleep(5)
        
        # ç™¼å¸ƒ
        print(f"[CRON] ç™¼å¸ƒ...")
        scrape_status['current_product'] = 'ç™¼å¸ƒ...'
        scrape_status['phase'] = 'publishing'
        batch_publish_bape_products()
        
        # è™•ç†ä¸‹æ¶
        print(f"[CRON] è™•ç†ä¸‹æ¶...")
        scrape_status['current_product'] = 'è™•ç†ä¸‹æ¶...'
        scrape_status['phase'] = 'drafting'
        
        draft_count = 0
        for handle, product_info in existing_handles.items():
            if handle not in scraped_handles and product_info.get('status') == 'ACTIVE':
                print(f"[CRON] è¨­ç‚ºè‰ç¨¿: {handle}")
                if set_product_to_draft(product_info['id']):
                    draft_count += 1
                time.sleep(0.2)
        
        scrape_status['set_to_draft'] = draft_count
        scrape_status['current_product'] = f"âœ… å®Œæˆï¼ä¸Šå‚³ {len(all_jsonl_entries)} å€‹ï¼Œä¸‹æ¶ {draft_count} å€‹"
        scrape_status['phase'] = 'completed'
        
        return {'success': True, 'total_products': len(all_jsonl_entries), 'set_to_draft': draft_count}
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        scrape_status['current_product'] = f"âŒ éŒ¯èª¤: {str(e)}"
        scrape_status['phase'] = 'error'
        import traceback
        traceback.print_exc()
        return {'success': False, 'error': str(e)}
    finally:
        scrape_status['running'] = False


# ========== API Routes ==========

@app.route('/')
def index():
    return '''<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>BAPE çˆ¬èŸ²å·¥å…·</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, sans-serif; background: #f5f5f5; padding: 20px; }
        .container { max-width: 1000px; margin: 0 auto; }
        h1 { color: #333; margin-bottom: 20px; }
        .card { background: white; border-radius: 12px; padding: 20px; margin-bottom: 20px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        .card h2 { color: #444; margin-bottom: 15px; font-size: 18px; }
        .btn { display: inline-block; padding: 12px 24px; border: none; border-radius: 8px; cursor: pointer; font-size: 14px; font-weight: 600; margin: 5px; }
        .btn-primary { background: #0066ff; color: white; }
        .btn-success { background: #00c853; color: white; }
        .btn-warning { background: #ff9800; color: white; }
        .btn-danger { background: #f44336; color: white; }
        .btn:disabled { opacity: 0.5; cursor: not-allowed; }
        .status { background: #f8f9fa; border-radius: 8px; padding: 15px; margin-top: 15px; }
        .progress-bar { height: 20px; background: #e0e0e0; border-radius: 10px; overflow: hidden; margin: 10px 0; }
        .progress-fill { height: 100%; background: linear-gradient(90deg, #0066ff, #00c853); transition: width 0.3s; }
        .log { background: #1e1e1e; color: #d4d4d4; padding: 15px; border-radius: 8px; max-height: 300px; overflow-y: auto; font-family: monospace; font-size: 12px; }
        select { padding: 10px; border-radius: 6px; border: 1px solid #ddd; font-size: 14px; margin-right: 10px; }
        .stats { display: grid; grid-template-columns: repeat(3, 1fr); gap: 15px; margin-top: 15px; }
        .stat-box { background: #f0f4f8; padding: 15px; border-radius: 8px; text-align: center; }
        .stat-value { font-size: 24px; font-weight: bold; }
        .stat-label { font-size: 12px; color: #666; }
        .danger-zone { border: 2px solid #f44336; background: #fff5f5; }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸ¦ BAPE çˆ¬èŸ²å·¥å…·</h1>
        
        <div class="card">
            <h2>âš¡ æ¸¬è©¦å–®å“</h2>
            <p style="color:#666;margin-bottom:10px;">æ¸¬è©¦å–®ä¸€å•†å“ä¸Šå‚³</p>
            <select id="testCat"><option value="mens">ç”·è£</option><option value="womens">å¥³è£</option><option value="kids">ç«¥è£</option></select>
            <button class="btn btn-warning" onclick="startTest()">ğŸ§ª æ¸¬è©¦å–®å“</button>
        </div>
        
        <div class="card">
            <h2>ğŸ§ª æ¸¬è©¦ä¸Šæ¶ï¼ˆæ¯åˆ†é¡ 10 å€‹ï¼‰</h2>
            <p style="color:#666;margin-bottom:10px;">å¿«é€Ÿæ¸¬è©¦ï¼Œæ¯å€‹åˆ†é¡åªæŠ“ 10 å€‹å•†å“</p>
            <select id="testSyncCat"><option value="all">å…¨éƒ¨</option><option value="mens">ç”·è£</option><option value="womens">å¥³è£</option><option value="kids">ç«¥è£</option></select>
            <button class="btn btn-warning" onclick="startTestSync()">ğŸ§ª æ¸¬è©¦ä¸Šæ¶</button>
        </div>
        
        <div class="card">
            <h2>ğŸ”„ å®Œæ•´åŒæ­¥</h2>
            <p style="color:#666;margin-bottom:10px;">çˆ¬å–æ‰€æœ‰å•†å“ä¸¦åŒæ­¥åˆ° Shopify</p>
            <select id="syncCat"><option value="all">å…¨éƒ¨</option><option value="mens">ç”·è£</option><option value="womens">å¥³è£</option><option value="kids">ç«¥è£</option></select>
            <button class="btn btn-success" onclick="startSync()">ğŸ”„ é–‹å§‹åŒæ­¥</button>
        </div>
        
        <div class="card">
            <h2>ğŸ“Š åŸ·è¡Œç‹€æ…‹</h2>
            <div class="progress-bar"><div class="progress-fill" id="progressFill" style="width:0%"></div></div>
            <div class="status">
                <div>éšæ®µï¼š<span id="phase">-</span></div>
                <div>é€²åº¦ï¼š<span id="progress">0/0</span></div>
                <div>ç›®å‰ï¼š<span id="current">-</span></div>
            </div>
            <div class="stats">
                <div class="stat-box"><div class="stat-value" id="productCount">0</div><div class="stat-label">å·²è™•ç†</div></div>
                <div class="stat-box"><div class="stat-value" id="draftCount">0</div><div class="stat-label">å·²ä¸‹æ¶</div></div>
                <div class="stat-box"><div class="stat-value" id="errorCount">0</div><div class="stat-label">éŒ¯èª¤</div></div>
            </div>
        </div>
        
        <div class="card">
            <h2>ğŸ“ æ—¥èªŒ</h2>
            <div class="log" id="log"></div>
        </div>
        
        <div class="card">
            <h2>ğŸ”§ å·¥å…·</h2>
            <button class="btn btn-primary" onclick="testShopify()">æ¸¬è©¦ Shopify</button>
            <button class="btn btn-primary" onclick="testBape()">æ¸¬è©¦ BAPE</button>
            <button class="btn btn-primary" onclick="countProducts()">å•†å“æ•¸é‡</button>
            <button class="btn btn-success" onclick="publishAll()">ç™¼å¸ƒæ‰€æœ‰</button>
        </div>
        
        <div class="card danger-zone">
            <h2>âš ï¸ å±éšªå€åŸŸ</h2>
            <p style="color:#666;margin-bottom:10px;">åˆªé™¤æ“ä½œç„¡æ³•å¾©åŸï¼Œè«‹è¬¹æ…ä½¿ç”¨</p>
            <button class="btn btn-danger" onclick="deleteAll()">ğŸ—‘ï¸ åˆªé™¤æ‰€æœ‰ BAPE å•†å“</button>
        </div>
    </div>
    
    <script>
        let pollInterval;
        
        function log(msg, type='info') {
            const logDiv = document.getElementById('log');
            const time = new Date().toLocaleTimeString();
            const color = type === 'success' ? '#4ec9b0' : type === 'error' ? '#f14c4c' : '#d4d4d4';
            logDiv.innerHTML += `<div style="color:${color}">[${time}] ${msg}</div>`;
            logDiv.scrollTop = logDiv.scrollHeight;
        }
        
        function updateStatus(data) {
            document.getElementById('phase').textContent = data.phase || '-';
            document.getElementById('progress').textContent = `${data.progress||0}/${data.total||0}`;
            document.getElementById('current').textContent = data.current_product || '-';
            document.getElementById('productCount').textContent = data.products?.length || 0;
            document.getElementById('draftCount').textContent = data.set_to_draft || 0;
            document.getElementById('errorCount').textContent = data.errors?.length || 0;
            document.getElementById('progressFill').style.width = data.total > 0 ? (data.progress/data.total*100)+'%' : '0%';
        }
        
        async function pollStatus() {
            try {
                const res = await fetch('/api/status');
                const data = await res.json();
                updateStatus(data);
                if (!data.running) {
                    clearInterval(pollInterval);
                    if (data.phase === 'completed') log('âœ… å®Œæˆï¼', 'success');
                    if (data.errors && data.errors.length > 0) {
                        data.errors.forEach(e => log('âŒ ' + (e.error || JSON.stringify(e)), 'error'));
                    }
                }
            } catch (e) { console.error(e); }
        }
        
        async function startTest() {
            log('ğŸ§ª é–‹å§‹æ¸¬è©¦å–®å“...');
            const res = await fetch('/api/test_single?category=' + document.getElementById('testCat').value);
            const data = await res.json();
            if (data.success) {
                log('æ¸¬è©¦å·²å•Ÿå‹•', 'success');
                pollInterval = setInterval(pollStatus, 1000);
            } else log('âŒ ' + (data.error || 'å•Ÿå‹•å¤±æ•—'), 'error');
        }
        
        async function startTestSync() {
            log('ğŸ§ª é–‹å§‹æ¸¬è©¦ä¸Šæ¶ï¼ˆæ¯åˆ†é¡ 10 å€‹ï¼‰...');
            const res = await fetch('/api/test_sync?category=' + document.getElementById('testSyncCat').value);
            const data = await res.json();
            if (data.success) {
                log('æ¸¬è©¦ä¸Šæ¶å·²å•Ÿå‹•', 'success');
                pollInterval = setInterval(pollStatus, 1000);
            } else log('âŒ ' + (data.error || 'å•Ÿå‹•å¤±æ•—'), 'error');
        }
        
        async function startSync() {
            log('ğŸ”„ é–‹å§‹å®Œæ•´åŒæ­¥...');
            const res = await fetch('/api/auto_sync?category=' + document.getElementById('syncCat').value);
            const data = await res.json();
            if (data.success) {
                log('åŒæ­¥å·²å•Ÿå‹•', 'success');
                pollInterval = setInterval(pollStatus, 1000);
            } else log('âŒ ' + (data.error || 'å•Ÿå‹•å¤±æ•—'), 'error');
        }
        
        async function deleteAll() {
            if (!confirm('ç¢ºå®šè¦åˆªé™¤æ‰€æœ‰ BAPE å•†å“å—ï¼Ÿæ­¤æ“ä½œç„¡æ³•å¾©åŸï¼')) return;
            if (!confirm('çœŸçš„ç¢ºå®šå—ï¼Ÿæ‰€æœ‰å•†å“éƒ½æœƒè¢«åˆªé™¤ï¼')) return;
            
            log('ğŸ—‘ï¸ é–‹å§‹åˆªé™¤æ‰€æœ‰ BAPE å•†å“...', 'error');
            const res = await fetch('/api/delete_all');
            const data = await res.json();
            if (data.success) {
                log('åˆªé™¤å·²å•Ÿå‹•', 'success');
                pollInterval = setInterval(pollStatus, 1000);
            } else log('âŒ ' + (data.error || 'å•Ÿå‹•å¤±æ•—'), 'error');
        }
        
        async function testShopify() {
            log('æ¸¬è©¦ Shopify...');
            const res = await fetch('/api/test');
            const data = await res.json();
            if (data.data?.shop) log('âœ… ' + data.data.shop.name, 'success');
            else log('âŒ é€£ç·šå¤±æ•—', 'error');
        }
        
        async function testBape() {
            log('æ¸¬è©¦ BAPE...');
            const res = await fetch('/api/test_bape');
            const data = await res.json();
            for (const [k, v] of Object.entries(data)) {
                if (v.ok) log(`âœ… ${k}: ${v.products_found} å€‹`, 'success');
                else log(`âŒ ${k}: ${v.error}`, 'error');
            }
        }
        
        async function countProducts() {
            const res = await fetch('/api/count');
            const data = await res.json();
            log('å•†å“æ•¸é‡: ' + data.count, 'success');
        }
        
        async function publishAll() {
            log('ğŸ“¢ ç™¼å¸ƒæ‰€æœ‰å•†å“...');
            const res = await fetch('/api/publish_all');
            const data = await res.json();
            if (data.success) {
                log('ç™¼å¸ƒå·²å•Ÿå‹•', 'success');
                pollInterval = setInterval(pollStatus, 1000);
            } else log('âŒ ' + (data.error || 'å•Ÿå‹•å¤±æ•—'), 'error');
        }
    </script>
</body>
</html>'''


@app.route('/api/status')
def api_status():
    return jsonify(scrape_status)


@app.route('/api/test')
def api_test():
    load_shopify_token()
    return jsonify(graphql_request("{ shop { name } }"))


@app.route('/api/test_single')
def api_test_single():
    category = request.args.get('category', 'mens')
    if scrape_status.get('running'):
        return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    if category not in CATEGORIES:
        return jsonify({'success': False, 'error': 'ç„¡æ•ˆåˆ†é¡'})
    threading.Thread(target=run_test_single, args=(category,)).start()
    return jsonify({'success': True})


@app.route('/api/scrape')
def api_scrape():
    category = request.args.get('category', 'all')
    if scrape_status.get('running'):
        return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_scrape, args=(category,)).start()
    return jsonify({'success': True})


@app.route('/api/upload')
def api_upload():
    jsonl_file = request.args.get('file', '')
    if not jsonl_file or not os.path.exists(jsonl_file):
        return jsonify({'error': 'JSONL ä¸å­˜åœ¨'})
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_bulk_upload, args=(jsonl_file,)).start()
    return jsonify({'started': True})


@app.route('/api/auto_sync')
def api_auto_sync():
    category = request.args.get('category', 'all')
    if scrape_status.get('running'):
        return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_full_sync, args=(category,)).start()
    return jsonify({'success': True})


@app.route('/api/cron_sync')
def api_cron_sync():
    category = request.args.get('category', 'all')
    if scrape_status.get('running'):
        return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    return jsonify(run_full_sync(category))


@app.route('/api/bulk_status')
def api_bulk_status():
    return jsonify(check_bulk_operation_status(scrape_status.get('bulk_operation_id') or None))


@app.route('/api/publish_all')
def api_publish_all():
    if scrape_status.get('running'):
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    def run_publish():
        global scrape_status
        scrape_status['running'] = True
        scrape_status['phase'] = 'publishing'
        try:
            results = batch_publish_bape_products()
            scrape_status['current_product'] = f"å®Œæˆï¼æˆåŠŸ: {results.get('success', 0)}"
        except Exception as e:
            scrape_status['errors'].append({'error': str(e)})
        finally:
            scrape_status['running'] = False
            scrape_status['phase'] = 'idle'
    
    threading.Thread(target=run_publish).start()
    return jsonify({'success': True})


@app.route('/api/delete_all')
def api_delete_all():
    """åˆªé™¤æ‰€æœ‰ BAPE å•†å“"""
    if scrape_status.get('running'):
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    def run_delete():
        global scrape_status
        scrape_status['running'] = True
        scrape_status['phase'] = 'deleting'
        scrape_status['progress'] = 0
        scrape_status['total'] = 0
        scrape_status['current_product'] = 'å–å¾—å•†å“åˆ—è¡¨...'
        scrape_status['errors'] = []
        
        try:
            products = fetch_bape_product_ids()
            scrape_status['total'] = len(products)
            
            results = delete_all_bape_products()
            scrape_status['current_product'] = f"âœ… åˆªé™¤å®Œæˆï¼å·²åˆªé™¤ {results.get('deleted', 0)} å€‹å•†å“"
        except Exception as e:
            scrape_status['errors'].append({'error': str(e)})
            scrape_status['current_product'] = f"âŒ éŒ¯èª¤: {str(e)}"
        finally:
            scrape_status['running'] = False
            scrape_status['phase'] = 'completed'
    
    threading.Thread(target=run_delete).start()
    return jsonify({'success': True, 'message': 'å·²é–‹å§‹åˆªé™¤'})


@app.route('/api/test_sync')
def api_test_sync():
    """æ¸¬è©¦ä¸Šæ¶ï¼ˆæ¯å€‹åˆ†é¡åªæŠ“ 10 å€‹ï¼‰"""
    category = request.args.get('category', 'all')
    limit = int(request.args.get('limit', 10))
    
    if scrape_status.get('running'):
        return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    threading.Thread(target=run_test_sync, args=(category, limit)).start()
    return jsonify({'success': True, 'message': f'æ¸¬è©¦ä¸Šæ¶å·²å•Ÿå‹•ï¼ˆæ¯åˆ†é¡ {limit} å€‹ï¼‰'})


@app.route('/api/count')
def api_count():
    load_shopify_token()
    result = graphql_request("{ productsCount(query: \"vendor:BAPE\") { count } }")
    return jsonify({'count': result.get('data', {}).get('productsCount', {}).get('count', 0)})


@app.route('/api/test_bape')
def api_test_bape():
    """æ¸¬è©¦é€£ç·šåˆ° jp.bape.com ä¸¦é¡¯ç¤ºåˆ†é¡çµ±è¨ˆ"""
    results = {}
    
    try:
        # å…ˆæ¸¬è©¦åŸºæœ¬é€£ç·š
        url = f"{SOURCE_URL}/collections/all/products.json?page=1&limit=10"
        print(f"[TEST BAPE] æ¸¬è©¦é€£ç·š: {url}")
        
        response = requests.get(url, headers=HEADERS, timeout=15)
        
        results['connection'] = {
            'url': url,
            'status': response.status_code,
            'ok': response.status_code == 200
        }
        
        if response.status_code == 200:
            # å–å¾—æ‰€æœ‰å•†å“ä¸¦åˆ†é¡
            all_by_category = fetch_all_products_by_category()
            
            results['categories'] = {
                'mens': {
                    'name': 'ãƒ¡ãƒ³ã‚ºï¼ˆç”·è£ï¼‰',
                    'count': len(all_by_category.get('mens', []))
                },
                'womens': {
                    'name': 'ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹ï¼ˆå¥³è£ï¼‰',
                    'count': len(all_by_category.get('womens', []))
                },
                'kids': {
                    'name': 'ã‚­ãƒƒã‚ºï¼ˆç«¥è£ï¼‰',
                    'count': len(all_by_category.get('kids', []))
                }
            }
            
            total = sum(len(v) for v in all_by_category.values())
            results['total_products'] = total
            
            # é¡¯ç¤ºæ¯å€‹åˆ†é¡çš„å‰ 3 å€‹å•†å“
            for cat_key in ['mens', 'womens', 'kids']:
                products = all_by_category.get(cat_key, [])[:3]
                results['categories'][cat_key]['samples'] = [
                    {'title': p.get('title', '')[:50], 'handle': p.get('handle', '')}
                    for p in products
                ]
        
    except requests.exceptions.Timeout:
        results['error'] = 'é€£ç·šè¶…æ™‚'
    except requests.exceptions.ConnectionError as e:
        results['error'] = f'é€£ç·šå¤±æ•—: {str(e)[:100]}'
    except Exception as e:
        results['error'] = str(e)
        import traceback
        results['traceback'] = traceback.format_exc()
    
    return jsonify(results)


@app.route('/api/test_html')
def api_test_html():
    """ç›´æ¥æ¸¬è©¦ HTML çˆ¬å–"""
    category = request.args.get('category', 'mens')
    
    if category not in CATEGORIES:
        return jsonify({'error': f'ç„¡æ•ˆåˆ†é¡: {category}'})
    
    cat_info = CATEGORIES[category]
    url = f"{SOURCE_URL}{cat_info['base_url']}?{cat_info['filter']}"
    
    result = {
        'category': category,
        'url': url
    }
    
    try:
        print(f"[TEST HTML] è«‹æ±‚: {url}")
        response = requests.get(url, headers=HEADERS, timeout=30)
        
        result['status'] = response.status_code
        result['content_length'] = len(response.text)
        result['headers'] = dict(response.headers)
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # é é¢æ¨™é¡Œ
            title = soup.find('title')
            result['page_title'] = title.get_text()[:100] if title else None
            
            # æ‰€æœ‰é€£çµ
            all_links = soup.find_all('a', href=True)
            result['total_links'] = len(all_links)
            
            # å•†å“é€£çµ
            product_handles = []
            for link in all_links:
                href = link.get('href', '')
                if '/products/' in href:
                    match = re.search(r'/products/([^/?#]+)', href)
                    if match and match.group(1) != 'products':
                        if match.group(1) not in product_handles:
                            product_handles.append(match.group(1))
            
            result['products_found'] = len(product_handles)
            result['product_handles'] = product_handles[:20]
            
            # é€£çµæ¨£æœ¬
            result['sample_links'] = [a.get('href', '')[:80] for a in all_links[:30]]
            
            # HTML ç‰‡æ®µï¼ˆå‰ 2000 å­—å…ƒï¼‰
            result['html_preview'] = response.text[:2000]
        else:
            result['response_text'] = response.text[:1000]
            
    except requests.exceptions.Timeout:
        result['error'] = 'é€£ç·šè¶…æ™‚ (30ç§’)'
    except requests.exceptions.ConnectionError as e:
        result['error'] = f'é€£ç·šå¤±æ•—: {str(e)}'
    except Exception as e:
        result['error'] = str(e)
        import traceback
        result['traceback'] = traceback.format_exc()
    
    return jsonify(result)


if __name__ == '__main__':
    print("BAPE çˆ¬èŸ²å·¥å…·")
    app.run(host='0.0.0.0', port=int(os.environ.get('PORT', 8080)), debug=False)
