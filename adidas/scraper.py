"""
adidas.jp çˆ¬èŸ² (Playwright + Shopify)
======================================
- ä½¿ç”¨ Playwright æ¨¡æ“¬ç€è¦½å™¨çˆ¬å– adidas.jp
- æ”¯æ´ç”·é‹ / å¥³é‹åˆ†é¡
- å®šåƒ¹å…¬å¼: (adidaså”®åƒ¹ + 1250) / 0.7 = Shopifyå”®åƒ¹ (æ—¥å¹£)
- è‡ªå‹•ä¸Šæ¶åˆ° Shopify
- ç¿»è­¯: æ—¥æ–‡ â†’ ç¹é«”ä¸­æ–‡ (ChatGPT API)
"""

import os
import re
import json
import math
import time
import logging
import requests
from datetime import datetime
from urllib.parse import urljoin, unquote

# ============================================================
# Logging
# ============================================================
logging.basicConfig(
    level=logging.INFO,
    format="[%(levelname)s] %(message)s",
)
logger = logging.getLogger("adidas")

# ============================================================
# ç’°å¢ƒè®Šæ•¸
# ============================================================
SHOPIFY_STORE = os.getenv("SHOPIFY_STORE", "")          # e.g. goyoutati
SHOPIFY_ACCESS_TOKEN = os.getenv("SHOPIFY_ACCESS_TOKEN", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
PROXY_URL = os.getenv("PROXY_URL", "")                  # å¯é¸: http://user:pass@host:port

# ============================================================
# adidas.jp åˆ†é¡ URL
# ============================================================
CATEGORIES = {
    "men_originals": {
        "name": "ç”·é‹ Originals",
        "url": "https://www.adidas.jp/%E3%83%A1%E3%83%B3%E3%82%BA-%E3%82%B7%E3%83%A5%E3%83%BC%E3%82%BA%E3%83%BB%E9%9D%B4-%E3%82%AA%E3%83%AA%E3%82%B8%E3%83%8A%E3%83%AB%E3%82%B9",
        "collection": "adidas ç”·é‹ Originals",
    },
    "women_originals": {
        "name": "å¥³é‹ Originals",
        "url": "https://www.adidas.jp/%E3%83%AC%E3%83%87%E3%82%A3%E3%83%BC%E3%82%B9-%E3%82%B7%E3%83%A5%E3%83%BC%E3%82%BA%E3%83%BB%E9%9D%B4-%E3%82%AA%E3%83%AA%E3%82%B8%E3%83%8A%E3%83%AB%E3%82%B9",
        "collection": "adidas å¥³é‹ Originals",
    },
}

# åˆ†é è¨­å®š: æ¯é  48 å€‹å•†å“
ITEMS_PER_PAGE = 48

BASE_URL = "https://www.adidas.jp"


# ============================================================
# å®šåƒ¹å…¬å¼
# ============================================================
def calculate_price(adidas_price_jpy: int) -> int:
    """
    (adidaså”®åƒ¹ + 1250) / 0.7 = Shopifyå”®åƒ¹
    ç„¡æ¢ä»¶é€²ä½åˆ°æ•´æ•¸
    """
    raw = (adidas_price_jpy + 1250) / 0.7
    return math.ceil(raw)


# ============================================================
# ç¿»è­¯ (ChatGPT API)
# ============================================================
def _api_request_with_retry(method: str, url: str, max_retries: int = 3, **kwargs) -> requests.Response:
    """å¸¶ retry çš„ API è«‹æ±‚ï¼ˆè™•ç† 429 rate limitï¼‰"""
    for attempt in range(max_retries):
        resp = requests.request(method, url, **kwargs)
        if resp.status_code == 429:
            retry_after = float(resp.headers.get("Retry-After", 2 * (attempt + 1)))
            logger.warning(f"  â³ Rate limit (429)ï¼Œç­‰å¾… {retry_after}s å¾Œé‡è©¦... ({attempt+1}/{max_retries})")
            time.sleep(retry_after)
            continue
        return resp
    return resp  # æœ€å¾Œä¸€æ¬¡çš„å›æ‡‰


def translate_ja_to_zhtw(text: str) -> str:
    """ç”¨ OpenAI ChatGPT å°‡æ—¥æ–‡ç¿»è­¯ç‚ºç¹é«”ä¸­æ–‡ï¼ˆå« retryï¼‰"""
    if not text or not text.strip():
        return text
    if not OPENAI_API_KEY:
        logger.warning("æœªè¨­å®š OPENAI_API_KEYï¼Œè·³éç¿»è­¯")
        return text

    for attempt in range(3):
        try:
            resp = requests.post(
                "https://api.openai.com/v1/chat/completions",
                headers={
                    "Authorization": f"Bearer {OPENAI_API_KEY}",
                    "Content-Type": "application/json",
                },
                json={
                    "model": "gpt-4o-mini",
                    "messages": [
                        {
                            "role": "system",
                            "content": (
                                "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚"
                                "åš´æ ¼è¦å‰‡ï¼š"
                                "1. åªå›å‚³ç¿»è­¯çµæœï¼Œä¸è¦åŠ ä»»ä½•è§£é‡‹ã€‚"
                                "2. å“ç‰Œåå’Œå‹è™Ÿåä¿ç•™è‹±æ–‡åŸæ–‡ã€‚"
                                "3. è¼¸å‡ºä¸­çµ•å°ä¸èƒ½å‡ºç¾ä»»ä½•æ—¥æ–‡ï¼ˆå¹³å‡åã€ç‰‡å‡åã€æ¼¢å­—æ··æ—¥æ–‡ï¼‰ã€‚"
                                "4. å¦‚æœåŸæ–‡å·²ç¶“æ˜¯è‹±æ–‡æˆ–ä¸­æ–‡ï¼Œç›´æ¥å›å‚³åŸæ–‡ã€‚"
                                "5. å¦‚æœåŸæ–‡åŒ…å«å¤šè¡Œï¼Œä¿æŒç›¸åŒçš„è¡Œæ•¸å’Œæ ¼å¼ã€‚"
                            ),
                        },
                        {"role": "user", "content": text},
                    ],
                    "temperature": 0,
                    "max_tokens": 1000,
                },
                timeout=30,
            )
            if resp.status_code == 200:
                return resp.json()["choices"][0]["message"]["content"].strip()
            elif resp.status_code == 429:
                wait = float(resp.headers.get("Retry-After", 3 * (attempt + 1)))
                logger.warning(f"  â³ OpenAI rate limitï¼Œç­‰å¾… {wait}s... ({attempt+1}/3)")
                time.sleep(wait)
                continue
            else:
                logger.error(f"ç¿»è­¯ API éŒ¯èª¤: {resp.status_code}")
                return text
        except Exception as e:
            logger.error(f"ç¿»è­¯å¤±æ•— (attempt {attempt+1}): {e}")
            if attempt < 2:
                time.sleep(2)
                continue
            return text
    return text


# ============================================================
# Playwright çˆ¬èŸ²æ ¸å¿ƒ
# ============================================================
class AdidasScraper:
    """ä½¿ç”¨ Playwright çˆ¬å– adidas.jp å•†å“"""

    def __init__(self):
        self.browser = None
        self.context = None
        self.page = None

    async def init_browser(self):
        """å•Ÿå‹•ç€è¦½å™¨ï¼ˆå«ååµæ¸¬ï¼‰"""
        from playwright.async_api import async_playwright

        self.pw = await async_playwright().start()
        self._proxy_url = PROXY_URL
        await self._launch_browser()

    async def _launch_browser(self):
        """å•Ÿå‹•æˆ–é‡å•Ÿæ•´å€‹ç€è¦½å™¨ï¼ˆbrowser + context + pageï¼‰"""
        launch_args = {
            "headless": True,
            "args": [
                "--no-sandbox",
                "--disable-setuid-sandbox",
                "--disable-dev-shm-usage",
                "--disable-blink-features=AutomationControlled",
                "--disable-infobars",
                "--window-size=1920,1080",
                "--disable-gpu",
                "--disable-extensions",
            ],
        }
        if self._proxy_url:
            launch_args["proxy"] = {"server": self._proxy_url}

        self.browser = await self.pw.chromium.launch(**launch_args)
        self.context = await self.browser.new_context(
            viewport={"width": 1920, "height": 1080},
            user_agent=(
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            ),
            locale="ja-JP",
            timezone_id="Asia/Tokyo",
            extra_http_headers={
                "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
            },
        )

        # ååµæ¸¬: ç§»é™¤ navigator.webdriver æ¨™è¨˜
        await self.context.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'languages', { get: () => ['ja', 'en-US', 'en'] });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
            window.chrome = { runtime: {} };
        """)

        self.page = await self.context.new_page()

    async def _restart_browser(self):
        """å®Œå…¨é‡å•Ÿç€è¦½å™¨ï¼ˆè§£æ±º Page crash å¾Œ connection æå£çš„å•é¡Œï¼‰"""
        logger.info("  ğŸ”„ å®Œå…¨é‡å•Ÿç€è¦½å™¨...")
        # é—œé–‰èˆŠçš„ï¼ˆå¿½ç•¥æ‰€æœ‰éŒ¯èª¤ï¼‰
        try:
            await self.browser.close()
        except Exception:
            pass
        # ç­‰å¾…ä¸€ä¸‹è®“é€²ç¨‹å®Œå…¨çµæŸ
        import asyncio
        await asyncio.sleep(2)
        # é‡æ–°å•Ÿå‹•
        await self._launch_browser()
        logger.info("  âœ… ç€è¦½å™¨é‡å•Ÿå®Œæˆ")

    async def close_browser(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.browser:
            await self.browser.close()
        if self.pw:
            await self.pw.stop()

    async def scrape_listing_page(self, category_url: str, max_pages: int = 0) -> list:
        """
        çˆ¬å–å•†å“åˆ—è¡¨é ï¼Œä½¿ç”¨ URL åˆ†é  (?start=0, 48, 96, ...)
        max_pages=0 è¡¨ç¤ºçˆ¬å…¨éƒ¨é é¢
        """
        products = []
        page_num = 0

        while True:
            # çµ„åˆåˆ†é  URL
            if page_num == 0:
                url = category_url
            else:
                start = page_num * ITEMS_PER_PAGE
                separator = "&" if "?" in category_url else "?"
                url = f"{category_url}{separator}start={start}"

            logger.info(f"æ­£åœ¨è¼‰å…¥ç¬¬ {page_num + 1} é : {url}")

            try:
                await self.page.goto(url, wait_until="domcontentloaded", timeout=60000)
                # ç­‰ä¹…ä¸€é»è®“ JS æ¸²æŸ“å®Œæˆ
                await self.page.wait_for_timeout(5000)

                # å…ˆå˜—è©¦é—œé–‰å½ˆçª—ï¼ˆå¯èƒ½æ“‹ä½å…§å®¹ï¼‰
                await self._close_popups()
                await self.page.wait_for_timeout(2000)

                # ç­‰å¾…å•†å“å¡ç‰‡å‡ºç¾
                await self.page.wait_for_selector(
                    '[data-testid="plp-product-card"]', timeout=20000
                )
            except Exception as e:
                err_msg = str(e)
                
                # Page crashed â†’ å®Œå…¨é‡å•Ÿç€è¦½å™¨
                if "crash" in err_msg.lower() or "closed" in err_msg.lower() or "object" in err_msg.lower():
                    logger.warning(f"  ğŸ”„ åˆ—è¡¨é å´©æ½°ï¼Œå®Œå…¨é‡å•Ÿç€è¦½å™¨...")
                    try:
                        await self._restart_browser()
                        # é‡è©¦è¼‰å…¥åŒä¸€é 
                        await self.page.goto(url, wait_until="domcontentloaded", timeout=60000)
                        await self.page.wait_for_timeout(5000)
                        await self._close_popups()
                        await self.page.wait_for_timeout(2000)
                        await self.page.wait_for_selector(
                            '[data-testid="plp-product-card"]', timeout=20000
                        )
                    except Exception as retry_e:
                        logger.error(f"  âŒ é‡è©¦å¾Œä»å¤±æ•—: {retry_e}")
                        if page_num == 0:
                            break
                        else:
                            logger.info(f"ç¬¬ {page_num + 1} é è·³éï¼Œç¹¼çºŒä¸‹ä¸€é ")
                            page_num += 1
                            continue
                else:
                    # æˆªåœ– debug
                    screenshot_path = f"/tmp/adidas_debug_page{page_num + 1}.png"
                    try:
                        await self.page.screenshot(path=screenshot_path, full_page=False)
                        logger.info(f"ğŸ“¸ Debug æˆªåœ–å·²å„²å­˜: {screenshot_path}")
                    except Exception:
                        pass

                    # è¨˜éŒ„é é¢æ¨™é¡Œå’Œ URL
                    try:
                        page_title = await self.page.title()
                        page_url = self.page.url
                        page_text = await self.page.inner_text("body")
                        logger.info(f"ğŸ“„ é é¢æ¨™é¡Œ: {page_title}")
                        logger.info(f"ğŸ“„ é é¢ URL: {page_url}")
                        logger.info(f"ğŸ“„ é é¢å‰500å­—: {page_text[:500]}")
                    except Exception:
                        pass

                    if page_num == 0:
                        logger.error(f"ç¬¬ 1 é è¼‰å…¥å¤±æ•—: {e}")
                        break
                    else:
                        logger.info(f"ç¬¬ {page_num + 1} é ç„¡å•†å“ï¼ŒçµæŸåˆ†é ")
                        break

            # æ»¾å‹•é é¢ç¢ºä¿æ‰€æœ‰å•†å“éƒ½è¼‰å…¥
            await self._scroll_page()

            # è§£æå•†å“å¡ç‰‡
            cards = await self.page.query_selector_all('[data-testid="plp-product-card"]')
            logger.info(f"  ç¬¬ {page_num + 1} é æ‰¾åˆ° {len(cards)} å€‹å•†å“")

            if len(cards) == 0:
                logger.info("æ²’æœ‰æ›´å¤šå•†å“ï¼ŒçµæŸåˆ†é ")
                break

            page_product_count = 0
            for card in cards:
                try:
                    product = await self._parse_card(card)
                    if product:
                        # é¿å…é‡è¤‡ï¼ˆè·¨é å¯èƒ½é‡è¤‡ï¼‰
                        if not any(p["sku"] == product["sku"] for p in products):
                            products.append(product)
                            page_product_count += 1
                except Exception as e:
                    logger.warning(f"è§£æå•†å“å¡ç‰‡å¤±æ•—: {e}")
                    continue

            logger.info(f"  ç¬¬ {page_num + 1} é æ–°å¢ {page_product_count} å€‹å•†å“ï¼ˆç´¯è¨ˆ {len(products)} å€‹ï¼‰")

            # å¦‚æœé€™ä¸€é å•†å“æ•¸å°‘æ–¼ ITEMS_PER_PAGEï¼Œä»£è¡¨æ˜¯æœ€å¾Œä¸€é 
            if len(cards) < ITEMS_PER_PAGE:
                logger.info("å·²åˆ°æœ€å¾Œä¸€é ")
                break

            page_num += 1

            # æª¢æŸ¥æ˜¯å¦é”åˆ°æœ€å¤§é æ•¸é™åˆ¶
            if max_pages > 0 and page_num >= max_pages:
                logger.info(f"å·²é”æœ€å¤§é æ•¸é™åˆ¶ ({max_pages} é )")
                break

            # é é–“å»¶é²
            await self.page.wait_for_timeout(2000)

        logger.info(f"ç¸½å…±æ‰¾åˆ° {len(products)} å€‹ä¸é‡è¤‡å•†å“")
        return products

    async def _scroll_page(self):
        """æ»¾å‹•é é¢ç¢ºä¿æ‰€æœ‰å•†å“éƒ½è¼‰å…¥"""
        for _ in range(5):
            await self.page.evaluate("window.scrollBy(0, window.innerHeight)")
            await self.page.wait_for_timeout(500)
        # æ»¾å›é ‚éƒ¨
        await self.page.evaluate("window.scrollTo(0, 0)")
        await self.page.wait_for_timeout(500)

    async def _parse_card(self, card) -> dict | None:
        """è§£æå–®å€‹å•†å“å¡ç‰‡"""
        # å•†å“é€£çµ
        link_el = await card.query_selector('[data-testid="product-card-image-link"]')
        if not link_el:
            return None
        href = await link_el.get_attribute("href")
        if not href:
            return None

        # å¾ URL å–å¾— SKU (ä¾‹å¦‚ /ã‚µãƒ³ãƒ-og-samba-og/B75806.html â†’ B75806)
        sku_match = re.search(r"/([A-Z0-9]{5,10})\.html", href)
        sku = sku_match.group(1) if sku_match else ""

        # å•†å“åç¨±
        title_el = await card.query_selector('[data-testid="product-card-title"]')
        title = await title_el.inner_text() if title_el else ""

        # å‰¯æ¨™é¡Œï¼ˆç³»åˆ—åï¼‰
        subtitle_el = await card.query_selector('[data-testid="product-card-subtitle"]')
        subtitle = await subtitle_el.inner_text() if subtitle_el else ""

        # åƒ¹æ ¼
        price_el = await card.query_selector('[data-testid="main-price"] span:last-child')
        price_text = await price_el.inner_text() if price_el else ""
        price_jpy = self._parse_price(price_text)

        # é¡è‰²æ•¸
        colors_el = await card.query_selector('[data-testid="product-card-colours"]')
        colors_text = await colors_el.inner_text() if colors_el else ""

        # åœ–ç‰‡
        img_el = await card.query_selector('[data-testid="product-card-primary-image"]')
        img_src = await img_el.get_attribute("src") if img_el else ""
        # å–å¾—é«˜è§£æåº¦åœ–ç‰‡ (æ›¿æ›ç‚º w_840)
        hi_res_img = re.sub(r"w_\d+,h_\d+", "w_840,h_840", img_src) if img_src else ""

        if not sku or not price_jpy:
            return None

        return {
            "sku": sku,
            "title": title,
            "subtitle": subtitle,
            "price_jpy": price_jpy,
            "selling_price": calculate_price(price_jpy),
            "colors_text": colors_text,
            "url": urljoin(BASE_URL, href),
            "image": hi_res_img,
            "scraped_at": datetime.now().isoformat(),
        }

    async def scrape_product_detail(self, product_url: str) -> dict | None:
        """
        çˆ¬å–å•†å“è©³ç´°é ï¼Œå–å¾—å®Œæ•´è³‡è¨Šï¼ˆæè¿°ã€æ‰€æœ‰åœ–ç‰‡ã€å°ºç¢¼ç­‰ï¼‰
        å¦‚æœé é¢ crashï¼Œé‡å»ºé é¢å¾Œé‡è©¦
        """
        detail = {}
        page_loaded = False

        # å˜—è©¦è¼‰å…¥é é¢ï¼ˆå…©ç¨®ç­–ç•¥ï¼‰
        for attempt, wait_until in enumerate(["domcontentloaded", "commit"], 1):
            try:
                logger.info(f"  å˜—è©¦è¼‰å…¥è©³ç´°é  (ç­–ç•¥{attempt}: {wait_until}): {product_url}")
                await self.page.goto(product_url, wait_until=wait_until, timeout=30000)
                await self.page.wait_for_timeout(5000)
                # é—œé–‰å½ˆçª—
                await self._close_popups()
                await self.page.wait_for_timeout(2000)
                # æ»¾å‹•è§¸ç™¼æ‡¶è¼‰å…¥åœ–ç‰‡
                await self._scroll_page()
                page_loaded = True
                break
            except Exception as e:
                err_msg = str(e)
                logger.warning(f"  è©³ç´°é è¼‰å…¥ç­–ç•¥{attempt}å¤±æ•—: {err_msg}")
                
                # Page crashed â†’ å®Œå…¨é‡å•Ÿç€è¦½å™¨
                if "crash" in err_msg.lower() or "closed" in err_msg.lower() or "object" in err_msg.lower():
                    try:
                        await self._restart_browser()
                    except Exception as re_err:
                        logger.error(f"  âŒ ç€è¦½å™¨é‡å•Ÿå¤±æ•—: {re_err}")
                        return None

        if not page_loaded:
            logger.error(f"  âŒ è©³ç´°é å®Œå…¨ç„¡æ³•è¼‰å…¥: {product_url}")
            return None

        # æ€§åˆ¥åˆ¤æ–· (ãƒ¡ãƒ³ã‚º / ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹ / ãƒ¦ãƒ‹ã‚»ãƒƒã‚¯ã‚¹)
        try:
            category_el = await self.page.query_selector('[data-auto-id="product-category"] span')
            if category_el:
                category_text = await category_el.inner_text()
                detail["category_text"] = category_text
                if "ãƒ¡ãƒ³ã‚º" in category_text and "ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹" in category_text:
                    detail["gender"] = "unisex"
                elif "ãƒ¡ãƒ³ã‚º" in category_text:
                    detail["gender"] = "men"
                elif "ãƒ¬ãƒ‡ã‚£ãƒ¼ã‚¹" in category_text:
                    detail["gender"] = "women"
                else:
                    detail["gender"] = "unisex"
                logger.info(f"  æ€§åˆ¥åˆ¤æ–·: {category_text} â†’ {detail['gender']}")
            else:
                detail["gender"] = "unisex"
                logger.info("  æ€§åˆ¥æ¨™ç±¤æœªæ‰¾åˆ° â†’ unisex")
        except Exception:
            detail["gender"] = "unisex"

        # å•†å“æè¿°ï¼ˆèª¬æ˜å€å¡Šï¼‰
        try:
            # èª¬æ˜ï¼šå‰¯æ¨™é¡Œ + æè¿°æ–‡å­—
            subtitle_el = await self.page.query_selector('.description_subtitle__5h3_L, [class*="description_subtitle"]')
            desc_text_el = await self.page.query_selector('.description_margin__cBW26, [class*="description_margin"], .description_text-content__zFrZJ p')
            
            subtitle = (await subtitle_el.inner_text()).strip() if subtitle_el else ""
            desc_text = (await desc_text_el.inner_text()).strip() if desc_text_el else ""
            
            # fallback: æ•´å€‹ description å€å¡Š
            if not subtitle and not desc_text:
                desc_el = await self.page.query_selector('[data-testid="product-description"], [class*="description_description"]')
                if desc_el:
                    desc_text = (await desc_el.inner_text()).strip()
            
            detail["subtitle"] = subtitle
            detail["description"] = desc_text
        except Exception:
            detail["subtitle"] = ""
            detail["description"] = ""

        # å•†å“è©³ç´°ï¼ˆè©³ç´°å€å¡Šï¼šè¦æ ¼åˆ—è¡¨ + ç”Ÿç”¢åœ‹ï¼‰
        try:
            # è¦æ ¼ bullet points
            spec_items = await self.page.query_selector_all(
                '[data-testid="specifications-section"] li, [data-auto-id="specifications-section"] li'
            )
            specs = []
            for item in spec_items:
                text = (await item.inner_text()).strip()
                if text:
                    specs.append(text)
            detail["specifications"] = specs
            
            # ç”Ÿç”¢åœ‹
            origin_el = await self.page.query_selector('[data-testid="specifications-table"] [role="cell"] .gl-table__cell-inner')
            detail["origin"] = (await origin_el.inner_text()).strip() if origin_el else ""
            
            if specs:
                logger.info(f"  ğŸ“ èªªæ˜: {len(specs)} é …è¦æ ¼, ç”¢åœ°: {detail['origin']}")
        except Exception:
            detail["specifications"] = []
            detail["origin"] = ""

        # ===== åœ–ç‰‡æŠ“å– =====
        # æŒ‰é é¢ä¸Š hi-res-image å‡ºç¾é †åºæŠ“å–ï¼Œä¿æŒ adidas åŸå§‹æ’åº
        images = []
        seen = set()
        
        sku_match = re.search(r"/([A-Z0-9]{5,10})\.html", product_url)
        sku_for_img = sku_match.group(1) if sku_match else ""

        if sku_for_img:
            try:
                # æ–¹æ³•1: æŒ‰ DOM é †åºæŠ“ hi-res-imageï¼ˆä¿æŒ adidas é é¢æ’åˆ—é †åºï¼‰
                hires_imgs = await self.page.query_selector_all(
                    'img[data-testid="pdp__image-viewer__desktop-zoom__hi-res-image"]'
                )
                for img in hires_imgs:
                    src = await img.get_attribute("src")
                    if src and sku_for_img in src and src not in seen:
                        seen.add(src)
                        images.append(src)
                
                # æ–¹æ³•2: å¦‚æœ hi-res æ²’æŠ“åˆ°ï¼Œå¾ HTML æºç¢¼æŒ‰å‡ºç¾é †åºæŠ“
                if not images:
                    page_content = await self.page.content()
                    # ç”¨ finditer ä¿æŒå‡ºç¾é †åº
                    hires_pattern = rf'(https://assets\.adidas\.com/images/h_2000[^"\'>\s]+{sku_for_img}[^"\'>\s]+\.jpg)'
                    for m in re.finditer(hires_pattern, page_content):
                        url = m.group(1)
                        if url not in seen:
                            seen.add(url)
                            images.append(url)
                
                # æ–¹æ³•3: fallback åˆ° h_840
                if not images:
                    page_content = await self.page.content() if 'page_content' not in dir() else page_content
                    fallback_pattern = rf'(https://assets\.adidas\.com/images/h_840[^"\'>\s]+{sku_for_img}[^"\'>\s]+\.jpg)'
                    for m in re.finditer(fallback_pattern, page_content):
                        url = m.group(1)
                        if url not in seen:
                            seen.add(url)
                            images.append(url)
                
                # Debug
                for i, img_url in enumerate(images[:3]):
                    fname = img_url.split("/")[-1]
                    logger.info(f"    åœ–{i+1}: {fname}")
                
            except Exception as e:
                logger.warning(f"  åœ–ç‰‡æŠ“å–å¤±æ•—: {e}")

        detail["images"] = images
        logger.info(f"  ğŸ“¸ æ‰¾åˆ° {len(images)} å¼µå•†å“åœ–ç‰‡ (SKU: {sku_for_img})")

        # å¯é¸å°ºç¢¼
        try:
            size_buttons = await self.page.query_selector_all(
                '[data-auto-id="size-selector"] button[role="radio"]'
            )
            sizes = []
            for btn in size_buttons:
                size_text = await btn.inner_text()
                aria_label = await btn.get_attribute("aria-label") or ""
                # æœ‰ unavailable class æˆ– aria-label å«ã€Œã”è³¼å…¥ã„ãŸã ã‘ã¾ã›ã‚“ã€= ç¼ºè²¨
                cls = await btn.get_attribute("class") or ""
                is_unavailable = "unavailable" in cls or "ã”è³¼å…¥ã„ãŸã ã‘ã¾ã›ã‚“" in aria_label
                sizes.append({
                    "size": size_text.strip(),
                    "available": not is_unavailable,
                })
            detail["sizes"] = sizes
            available_count = sum(1 for s in sizes if s["available"])
            logger.info(f"  ğŸ‘Ÿ æ‰¾åˆ° {len(sizes)} å€‹å°ºç¢¼ï¼ˆ{available_count} å€‹æœ‰è²¨ï¼‰")
        except Exception:
            detail["sizes"] = []

        # ç•¶å‰é¡è‰²åç¨±
        try:
            color_label = await self.page.query_selector('[data-auto-id="color-label"], [data-testid="color-label"]')
            if color_label:
                detail["color"] = (await color_label.inner_text()).strip()
                logger.info(f"  ğŸ¨ é¡è‰²: {detail['color']}")
            else:
                detail["color"] = ""
        except Exception:
            detail["color"] = ""

        return detail

    async def _close_popups(self):
        """é—œé–‰å¯èƒ½å‡ºç¾çš„å½ˆçª—"""
        popup_selectors = [
            '[data-testid="cookie-banner-accept"]',
            '[data-testid="modal-close"]',
            'button:has-text("åŒæ„")',
            'button:has-text("é–‰ã˜ã‚‹")',
            'button:has-text("Accept")',
            '[class*="cookie"] button',
            '[class*="popup"] [class*="close"]',
        ]
        for selector in popup_selectors:
            try:
                btn = self.page.locator(selector)
                if await btn.count() > 0:
                    await btn.first.click()
                    await self.page.wait_for_timeout(500)
            except Exception:
                continue

    @staticmethod
    def _parse_price(text: str) -> int:
        """è§£æåƒ¹æ ¼æ–‡å­— 'Â¥15,950' â†’ 15950"""
        if not text:
            return 0
        nums = re.findall(r"\d+", text.replace(",", ""))
        return int("".join(nums)) if nums else 0


# ============================================================
# Shopify ä¸Šæ¶
# ============================================================
class ShopifyUploader:
    """å°‡å•†å“ä¸Šæ¶åˆ° Shopify"""

    def __init__(self):
        if not SHOPIFY_STORE or not SHOPIFY_ACCESS_TOKEN:
            logger.warning("æœªè¨­å®š Shopify ç’°å¢ƒè®Šæ•¸ï¼Œä¸Šæ¶åŠŸèƒ½ä¸å¯ç”¨")
        self.base_url = f"https://{SHOPIFY_STORE}.myshopify.com/admin/api/2026-01"
        self.headers = {
            "X-Shopify-Access-Token": SHOPIFY_ACCESS_TOKEN,
            "Content-Type": "application/json",
        }
        self._existing_skus = None
        self._collection_cache = {}
        self._publication_ids = None

    def get_publication_ids(self) -> list:
        """ç”¨ GraphQL å–å¾—æ‰€æœ‰éŠ·å”®ç®¡é“çš„ publication ID"""
        if self._publication_ids is not None:
            return self._publication_ids

        self._publication_ids = []
        graphql_url = f"https://{SHOPIFY_STORE}.myshopify.com/admin/api/2026-01/graphql.json"
        headers = {
            "X-Shopify-Access-Token": SHOPIFY_ACCESS_TOKEN,
            "Content-Type": "application/json",
        }
        
        # å…ˆæŸ¥ access scopes
        scope_query = """{ currentAppInstallation { accessScopes { handle } } }"""
        try:
            resp = requests.post(graphql_url, headers=headers, json={"query": scope_query}, timeout=15)
            if resp.status_code == 200:
                result = resp.json()
                scopes = result.get("data", {}).get("currentAppInstallation", {}).get("accessScopes", [])
                scope_list = [s.get("handle", "") for s in scopes]
                has_pub = any("publication" in s for s in scope_list)
                logger.info(f"API Scopes å« publication: {has_pub}")
                if not has_pub:
                    logger.warning("âš ï¸ Token å¯èƒ½ç¼ºå°‘ write_publications æ¬Šé™ï¼")
        except Exception:
            pass

        query = """{ publications(first: 20) { edges { node { id name } } } }"""

        try:
            resp = requests.post(graphql_url, headers=headers, json={"query": query}, timeout=15)
            if resp.status_code == 200:
                result = resp.json()
                pubs = result.get("data", {}).get("publications", {}).get("edges", [])
                seen_names = set()
                for pub in pubs:
                    name = pub["node"]["name"]
                    if name not in seen_names:
                        seen_names.add(name)
                        self._publication_ids.append(pub["node"]["id"])
                names = list(seen_names)
                logger.info(f"æ‰¾åˆ° {len(self._publication_ids)} å€‹éŠ·å”®ç®¡é“: {', '.join(names)}")
            else:
                logger.error(f"å–å¾—éŠ·å”®ç®¡é“å¤±æ•—: {resp.status_code}")
        except Exception as e:
            logger.error(f"å–å¾—éŠ·å”®ç®¡é“ç•°å¸¸: {e}")

        return self._publication_ids

    def publish_to_all_channels(self, resource_type: str, resource_id: int):
        """
        ç”¨ GraphQL å°‡å•†å“æˆ– Collection ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®ç®¡é“
        resource_type: 'Product' æˆ– 'Collection'
        """
        pub_ids = self.get_publication_ids()
        if not pub_ids:
            logger.warning("æ²’æœ‰æ‰¾åˆ°ä»»ä½•éŠ·å”®ç®¡é“")
            return

        graphql_url = f"https://{SHOPIFY_STORE}.myshopify.com/admin/api/2026-01/graphql.json"
        headers = {
            "X-Shopify-Access-Token": SHOPIFY_ACCESS_TOKEN,
            "Content-Type": "application/json",
        }

        if resource_type == "Product":
            mutation = """
            mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
              publishablePublish(id: $id, input: $input) {
                publishable { availablePublicationsCount { count } }
                userErrors { field message }
              }
            }
            """
            gid = f"gid://shopify/Product/{resource_id}"
            variables = {
                "id": gid,
                "input": [{"publicationId": pid} for pid in pub_ids],
            }

            try:
                resp = requests.post(
                    graphql_url,
                    headers=headers,
                    json={"query": mutation, "variables": variables},
                    timeout=15,
                )
                if resp.status_code == 200:
                    result = resp.json()
                    errors = result.get("data", {}).get("publishablePublish", {}).get("userErrors", [])
                    if errors:
                        for err in errors:
                            logger.warning(f"  ç™¼å¸ƒè­¦å‘Š (Product {resource_id}): {err.get('message')}")
                    else:
                        logger.info(f"  âœ… Product {resource_id} å·²ç™¼å¸ƒåˆ° {len(pub_ids)} å€‹ç®¡é“")
                else:
                    logger.error(f"  ç™¼å¸ƒå¤±æ•—: {resp.status_code}")
            except Exception as e:
                logger.error(f"  ç™¼å¸ƒç•°å¸¸: {e}")

        elif resource_type == "Collection":
            # Collection ä¹Ÿç”¨ publishablePublishï¼ˆCollection æ˜¯ Publishable è³‡æºï¼‰
            mutation = """
            mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
              publishablePublish(id: $id, input: $input) {
                publishable {
                  availablePublicationsCount { count }
                  ... on Collection {
                    resourcePublicationsCount { count }
                  }
                }
                userErrors { field message }
              }
            }
            """
            gid = f"gid://shopify/Collection/{resource_id}"
            variables = {
                "id": gid,
                "input": [{"publicationId": pid} for pid in pub_ids],
            }

            try:
                resp = requests.post(
                    graphql_url,
                    headers=headers,
                    json={"query": mutation, "variables": variables},
                    timeout=15,
                )
                if resp.status_code == 200:
                    result = resp.json()
                    logger.info(f"  ğŸ“¦ Collection publish å›æ‡‰: {json.dumps(result, ensure_ascii=False)[:500]}")
                    data = result.get("data", {}).get("publishablePublish", {})
                    errors = data.get("userErrors", [])
                    if errors:
                        for err in errors:
                            logger.warning(f"  ç™¼å¸ƒè­¦å‘Š (Collection {resource_id}): {err.get('message')}")
                    else:
                        count = data.get("publishable", {}).get("availablePublicationsCount", {}).get("count", "?")
                        logger.info(f"  âœ… Collection {resource_id} å·²ç™¼å¸ƒåˆ° {count} å€‹ç®¡é“")
                else:
                    logger.error(f"  Collection ç™¼å¸ƒå¤±æ•—: {resp.status_code} {resp.text[:200]}")
            except Exception as e:
                logger.error(f"  Collection ç™¼å¸ƒç•°å¸¸: {e}")

    def get_existing_skus(self) -> set:
        """å–å¾— Shopify ä¸Šå·²æœ‰çš„æ‰€æœ‰å•†å“è²¨è™Ÿï¼ˆadidas å•†å“ç·¨è™Ÿï¼‰"""
        if self._existing_skus is not None:
            return self._existing_skus

        skus = set()
        url = f"{self.base_url}/products.json?limit=250&fields=id,variants,tags"
        while url:
            try:
                resp = requests.get(url, headers=self.headers, timeout=30)
                if resp.status_code != 200:
                    logger.error(f"Shopify API éŒ¯èª¤: {resp.status_code}")
                    break
                data = resp.json()
                for product in data.get("products", []):
                    # å¾ variant SKU æå–åŸºç¤è²¨è™Ÿï¼ˆB75807-220 â†’ B75807ï¼‰
                    for variant in product.get("variants", []):
                        sku = variant.get("sku", "")
                        if sku:
                            base_sku = sku.split("-")[0].upper()
                            skus.add(base_sku)
                            skus.add(sku.upper())

                # åˆ†é 
                link_header = resp.headers.get("Link", "")
                if 'rel="next"' in link_header:
                    import re as _re
                    match = _re.search(r'<([^>]+)>;\s*rel="next"', link_header)
                    url = match.group(1) if match else None
                else:
                    url = None
            except Exception as e:
                logger.error(f"å–å¾— SKU å¤±æ•—: {e}")
                break

        logger.info(f"Shopify å·²æœ‰ {len(skus)} å€‹ SKU")
        self._existing_skus = skus
        return skus

    def is_duplicate(self, sku: str) -> bool:
        """æª¢æŸ¥å•†å“è²¨è™Ÿæ˜¯å¦å·²å­˜åœ¨"""
        return sku.upper() in self.get_existing_skus()

    def get_or_create_collection(self, title: str) -> int | None:
        """å–å¾—æˆ–å»ºç«‹ Collectionï¼Œä¸¦ç¢ºä¿ç™¼å¸ƒåˆ°æ‰€æœ‰ç®¡é“"""
        if title in self._collection_cache:
            return self._collection_cache[title]

        # æœå°‹ç¾æœ‰ Collection
        try:
            resp = requests.get(
                f"{self.base_url}/custom_collections.json?title={title}",
                headers=self.headers,
                timeout=30,
            )
            if resp.status_code == 200:
                collections = resp.json().get("custom_collections", [])
                for c in collections:
                    if c["title"] == title:
                        self._collection_cache[title] = c["id"]
                        logger.info(f"æ‰¾åˆ°ç¾æœ‰ Collection: {title} (ID: {c['id']})")
                        # ç¢ºä¿æ—¢æœ‰çš„ä¹Ÿç™¼å¸ƒåˆ°æ‰€æœ‰ç®¡é“
                        self.publish_to_all_channels("Collection", c["id"])
                        return c["id"]
        except Exception:
            pass

        # å»ºç«‹æ–° Collection
        try:
            resp = requests.post(
                f"{self.base_url}/custom_collections.json",
                headers=self.headers,
                json={"custom_collection": {"title": title, "published": True}},
                timeout=30,
            )
            if resp.status_code == 201:
                cid = resp.json()["custom_collection"]["id"]
                self._collection_cache[title] = cid
                logger.info(f"å»ºç«‹æ–° Collection: {title} (ID: {cid})")
                # ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®ç®¡é“
                self.publish_to_all_channels("Collection", cid)
                return cid
        except Exception as e:
            logger.error(f"å»ºç«‹ Collection å¤±æ•—: {e}")

        return None

    def upload_product(
        self,
        product: dict,
        detail: dict | None,
        collection_id: int | None,
        translate: bool = True,
    ) -> dict:
        """ä¸Šæ¶å–®å€‹å•†å“åˆ° Shopify"""
        title = product["title"]
        
        # æ¸…ç†æ¨™é¡Œï¼šç§»é™¤æ—¥æ–‡éƒ¨åˆ†ï¼Œåªä¿ç•™è‹±æ–‡
        # adidas æ¨™é¡Œæ ¼å¼é€šå¸¸æ˜¯ "æ—¥æ–‡å / è‹±æ–‡å" æˆ– "è‹±æ–‡å"
        def clean_title(t: str) -> str:
            """å¾æ¨™é¡Œä¸­æå–è‹±æ–‡éƒ¨åˆ†ï¼Œç§»é™¤æ—¥æ–‡"""
            import unicodedata
            parts = [p.strip() for p in t.split("/")]
            english_parts = []
            seen = set()
            for p in parts:
                # æª¢æŸ¥æ˜¯å¦åŒ…å«æ—¥æ–‡å­—å…ƒï¼ˆå¹³å‡åã€ç‰‡å‡åã€CJKï¼‰
                has_japanese = any(
                    unicodedata.name(c, "").startswith(("HIRAGANA", "KATAKANA", "CJK"))
                    for c in p if c.strip()
                )
                if not has_japanese and p and p.lower() not in seen:
                    seen.add(p.lower())
                    english_parts.append(p)
            return " / ".join(english_parts) if english_parts else t
        
        clean_en_title = clean_title(title)
        
        # å–å‡ºæè¿°å„éƒ¨åˆ†
        subtitle = detail.get("subtitle", "") if detail else ""
        description = detail.get("description", "") if detail else ""
        specs = detail.get("specifications", []) if detail else []
        origin = detail.get("origin", "") if detail else ""
        color = detail.get("color", "") if detail else ""

        # ç¿»è­¯ï¼ˆæ¨™é¡Œç›´æ¥ç”¨è‹±æ–‡ï¼Œä¸ç¶“éç¿»è­¯ï¼‰
        if translate and OPENAI_API_KEY:
            subtitle_zh = translate_ja_to_zhtw(subtitle) if subtitle else ""
            desc_zh = translate_ja_to_zhtw(description) if description else ""
            # è¦æ ¼ä¸€æ¬¡æ€§ç¿»è­¯ï¼ˆåˆä½µé€å‡ºç¯€çœ API å‘¼å«ï¼‰
            if specs:
                specs_text = "\n".join(specs)
                specs_zh = translate_ja_to_zhtw(specs_text)
                specs_zh_list = [s.strip() for s in specs_zh.split("\n") if s.strip()]
            else:
                specs_zh_list = []
            origin_zh = translate_ja_to_zhtw(origin) if origin else ""
        else:
            subtitle_zh = subtitle
            desc_zh = description
            specs_zh_list = specs
            origin_zh = origin

        # æ¨™é¡Œç›´æ¥ç”¨æ¸…ç†å¾Œçš„è‹±æ–‡åï¼ŒåŠ ä¸Šå‰ç¶´
        full_title = f"adidasï½œoriginalï½œåŸå‰µç³»åˆ—ï½œ{clean_en_title}"

        # åœ–ç‰‡
        images = []
        if detail and detail.get("images"):
            for img_url in detail["images"][:20]:  # æœ€å¤š 20 å¼µ
                images.append({"src": img_url})
        elif product.get("image"):
            images.append({"src": product["image"]})

        # çµ„åˆæè¿° HTML
        body_html = self._build_description_html(
            subtitle_zh=subtitle_zh,
            desc_zh=desc_zh,
            specs=specs_zh_list,
            origin=origin_zh,
            color=color,
            sku=product["sku"],
        )

        # æ ¹æ“šæ€§åˆ¥æ±ºå®š Collections
        gender = detail.get("gender", "unisex") if detail else "unisex"
        collection_names = self._get_collection_names_by_gender(gender, product.get("collection_name", ""))

        # å»ºç«‹å°ºç¢¼ variantsï¼ˆå…¨éƒ¨å°ºç¢¼ï¼Œåº«å­˜ä¾æœ‰ç„¡è²¨è¨­å®šï¼‰
        sizes = detail.get("sizes", []) if detail else []
        color = detail.get("color", "") if detail else ""
        
        if sizes:
            variants = []
            for s in sizes:
                variant = {
                    "option1": s["size"],
                    "price": str(product["selling_price"]),
                    "compare_at_price": None,
                    "sku": f"{product['sku']}-{s['size'].replace('.', '').replace('cm', '')}",
                    "inventory_management": "shopify",
                    "requires_shipping": True,
                }
                variants.append(variant)
            options = [{"name": "å°ºç¢¼", "values": [s["size"] for s in sizes]}]
            # è¨˜éŒ„æ¯å€‹å°ºç¢¼çš„åº«å­˜é‡ï¼ˆæœ‰è²¨=2, ç¼ºè²¨=0ï¼‰
            size_stock = {s["size"]: 2 if s["available"] else 0 for s in sizes}
        else:
            variants = [
                {
                    "price": str(product["selling_price"]),
                    "compare_at_price": None,
                    "sku": product["sku"],
                    "inventory_management": "shopify",
                    "requires_shipping": True,
                }
            ]
            options = []
            size_stock = {"__default__": 2}  # ç„¡å°ºç¢¼æ™‚é è¨­åº«å­˜ 2

        # ç”¨ ChatGPT ç”Ÿæˆ SEO meta title å’Œ description
        seo = self._generate_seo(clean_en_title, subtitle_zh, desc_zh, color, product["sku"])

        # Shopify product payload - æ‰€æœ‰éŠ·è·¯ç®¡é“å…¨é–‹
        payload = {
            "product": {
                "title": full_title,
                "body_html": body_html,
                "vendor": "adidas",
                "product_type": "é‹é¡",
                "tags": [
                    "adidas",
                    product.get("subtitle", ""),
                    product["sku"],
                    color,
                ],
                "variants": variants,
                "images": images,
                "status": "active",
                "published": True,
                "published_scope": "global",
                "metafields_global_title_tag": seo.get("title", full_title),
                "metafields_global_description_tag": seo.get("description", ""),
            }
        }
        
        if options:
            payload["product"]["options"] = options

        try:
            resp = _api_request_with_retry(
                "POST",
                f"{self.base_url}/products.json",
                headers=self.headers,
                json=payload,
                timeout=60,
            )
            if resp.status_code == 201:
                shopify_product = resp.json()["product"]
                product_id = shopify_product["id"]

                # è¨­å®šåº«å­˜æ•¸é‡
                if size_stock:
                    self._set_inventory_levels(shopify_product, size_stock)

                # è¨­å®š metafield custom.linkï¼ˆåŸå§‹å•†å“é€£çµï¼‰
                self._set_product_metafield(product_id, product["url"])

                # åŠ å…¥æ‰€æœ‰ç›¸é—œ Collections
                for col_name in collection_names:
                    col_id = self.get_or_create_collection(col_name)
                    if col_id:
                        self._add_to_collection(product_id, col_id)
                        logger.info(f"  ğŸ“‚ åŠ å…¥ Collection: {col_name}")

                # ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®ç®¡é“
                self.publish_to_all_channels("Product", product_id)

                gender_label = {"men": "ç”·", "women": "å¥³", "unisex": "ç”·+å¥³"}
                logger.info(
                    f"âœ… ä¸Šæ¶æˆåŠŸ: {product['sku']} - {title} â†’ Â¥{product['selling_price']} "
                    f"[{gender_label.get(gender, '?')}]"
                )

                self._existing_skus.add(product["sku"].upper())
                return {"success": True, "product_id": product_id}
            else:
                logger.error(f"âŒ ä¸Šæ¶å¤±æ•—: {product['sku']} - {resp.status_code} {resp.text[:200]}")
                return {"success": False, "error": resp.text[:200]}
        except Exception as e:
            logger.error(f"âŒ ä¸Šæ¶ç•°å¸¸: {product['sku']} - {e}")
            return {"success": False, "error": str(e)}

    @staticmethod
    def _generate_seo(title_en: str, subtitle_zh: str, desc_zh: str, color: str, sku: str) -> dict:
        """ç”¨ ChatGPT ç”Ÿæˆ SEO meta title å’Œ descriptionï¼ˆå« retryï¼‰"""
        if not OPENAI_API_KEY:
            return {}
        
        prompt_text = f"""å•†å“åç¨±: {title_en}
å•†å“æè¿°: {subtitle_zh} {desc_zh}
é¡è‰²: {color}
å‹è™Ÿ: {sku}
å“ç‰Œ: adidas Originals
å•†åº—: GOYOUTATI æ—¥æœ¬ä»£è³¼"""

        for attempt in range(3):
            try:
                resp = requests.post(
                    "https://api.openai.com/v1/chat/completions",
                    headers={
                        "Authorization": f"Bearer {OPENAI_API_KEY}",
                        "Content-Type": "application/json",
                    },
                    json={
                        "model": "gpt-4o-mini",
                        "messages": [
                            {
                                "role": "system",
                                "content": (
                                    "ä½ æ˜¯ SEO å°ˆå®¶ã€‚æ ¹æ“šå•†å“è³‡è¨Šç”Ÿæˆæœå°‹å¼•æ“å„ªåŒ–çš„é é¢æ¨™é¡Œå’Œ Meta æè¿°ã€‚"
                                    "è¦å‰‡ï¼š"
                                    "1. é é¢æ¨™é¡Œ(title)ï¼šæœ€å¤š 60 å­—å…ƒï¼ŒåŒ…å«å“ç‰Œåã€å•†å“åã€é—œéµå­—ã€‚æ ¼å¼ç¯„ä¾‹ï¼šadidas Samba OG ç¶“å…¸é‹æ¬¾ï½œGOYOUTATI æ—¥æœ¬ä»£è³¼"
                                    "2. Meta æè¿°(description)ï¼šæœ€å¤š 155 å­—å…ƒï¼Œè‡ªç„¶æµæš¢çš„ç¹é«”ä¸­æ–‡ï¼ŒåŒ…å«å•†å“è³£é»ã€é¡è‰²ã€é©åˆå ´æ™¯ï¼Œå¸å¼•é»æ“Šã€‚"
                                    "3. ä¸è¦å‡ºç¾æ—¥æ–‡ã€‚"
                                    "4. åªå›å‚³ JSON æ ¼å¼ï¼š{\"title\": \"...\", \"description\": \"...\"}"
                                    "5. ä¸è¦åŠ  markdown æ ¼å¼æˆ–åå¼•è™Ÿã€‚"
                                ),
                            },
                            {"role": "user", "content": prompt_text},
                        ],
                        "temperature": 0,
                        "max_tokens": 300,
                    },
                    timeout=30,
                )
                if resp.status_code == 200:
                    content = resp.json()["choices"][0]["message"]["content"].strip()
                    content = content.replace("```json", "").replace("```", "").strip()
                    import json
                    seo = json.loads(content)
                    logger.info(f"  ğŸ” SEO: {seo.get('title', '')[:50]}...")
                    return seo
                elif resp.status_code == 429:
                    wait = float(resp.headers.get("Retry-After", 3 * (attempt + 1)))
                    logger.warning(f"  â³ OpenAI rate limit (SEO)ï¼Œç­‰å¾… {wait}s...")
                    time.sleep(wait)
                    continue
                else:
                    logger.warning(f"  âš ï¸ SEO ç”Ÿæˆå¤±æ•—: {resp.status_code}")
                    return {}
            except Exception as e:
                logger.warning(f"  âš ï¸ SEO ç”Ÿæˆå¤±æ•— (attempt {attempt+1}): {e}")
                if attempt < 2:
                    time.sleep(2)
                    continue
                return {}
        return {}

    def _set_product_metafield(self, product_id: int, url: str):
        """è¨­å®šå•†å“ metafield custom.link"""
        try:
            resp = _api_request_with_retry(
                "POST",
                f"{self.base_url}/products/{product_id}/metafields.json",
                headers=self.headers,
                json={
                    "metafield": {
                        "namespace": "custom",
                        "key": "link",
                        "value": url,
                        "type": "url",
                    }
                },
                timeout=30,
            )
            if resp.status_code in (200, 201):
                logger.info(f"  ğŸ”— Metafield custom.link å·²è¨­å®š")
            else:
                logger.warning(f"  âš ï¸ Metafield è¨­å®šå¤±æ•—: {resp.status_code} {resp.text[:100]}")
        except Exception as e:
            logger.warning(f"  âš ï¸ Metafield è¨­å®šå¤±æ•—: {e}")

    def _set_inventory_levels(self, shopify_product: dict, size_stock: dict):
        """è¨­å®šæ¯å€‹ variant çš„åº«å­˜æ•¸é‡"""
        try:
            # å¾ç¬¬ä¸€å€‹ variant çš„ inventory_item_id æŸ¥å‡º location_id
            first_variant = shopify_product.get("variants", [{}])[0]
            first_inv_id = first_variant.get("inventory_item_id")
            
            if not first_inv_id:
                logger.warning("  âš ï¸ æ‰¾ä¸åˆ° inventory_item_id")
                return

            # æ–¹æ³•1: é€é inventory_levels å–å¾— location_id
            inv_url = f"{self.base_url}/inventory_levels.json?inventory_item_ids={first_inv_id}"
            inv_resp = _api_request_with_retry("GET", inv_url, headers=self.headers, timeout=30)
            
            inv_levels = inv_resp.json().get("inventory_levels", [])
            
            if inv_levels:
                location_id = inv_levels[0]["location_id"]
            else:
                # æ–¹æ³•2: locations API
                loc_url = f"{self.base_url}/locations.json"
                loc_resp = _api_request_with_retry("GET", loc_url, headers=self.headers, timeout=30)
                
                locations = loc_resp.json().get("locations", [])
                if not locations:
                    logger.warning(f"  âš ï¸ æ‰¾ä¸åˆ° location ({inv_resp.status_code})")
                    return
                location_id = locations[0]["id"]

            # è¨­å®šåº«å­˜
            in_stock = 0
            out_stock = 0
            errors = 0
            has_default = "__default__" in size_stock
            for variant in shopify_product.get("variants", []):
                size_name = variant.get("option1", "")
                if has_default:
                    qty = size_stock["__default__"]
                else:
                    qty = size_stock.get(size_name, 0)
                inventory_item_id = variant.get("inventory_item_id")
                if not inventory_item_id:
                    continue

                resp = _api_request_with_retry(
                    "POST",
                    f"{self.base_url}/inventory_levels/set.json",
                    headers=self.headers,
                    json={
                        "location_id": location_id,
                        "inventory_item_id": inventory_item_id,
                        "available": qty,
                    },
                    timeout=30,
                )
                if resp.status_code != 200:
                    if errors == 0:  # åªå°ç¬¬ä¸€å€‹éŒ¯èª¤
                        logger.warning(f"    åº«å­˜è¨­å®šå¤±æ•— {size_name}: {resp.status_code} {resp.text[:200]}")
                    errors += 1
                    continue
                    
                if qty > 0:
                    in_stock += 1
                else:
                    out_stock += 1

            if errors:
                logger.warning(f"  âš ï¸ åº«å­˜è¨­å®š: {errors} å€‹å¤±æ•—")
            logger.info(f"  ğŸ“¦ åº«å­˜è¨­å®šå®Œæˆ: {in_stock} å€‹æœ‰è²¨(2), {out_stock} å€‹ç¼ºè²¨(0)")
        except Exception as e:
            logger.warning(f"  âš ï¸ åº«å­˜è¨­å®šå¤±æ•—: {e}")
            import traceback
            logger.warning(traceback.format_exc())

    @staticmethod
    def _get_collection_names_by_gender(gender: str, default_collection: str) -> list:
        """æ ¹æ“šæ€§åˆ¥æ±ºå®šè¦åŠ å…¥å“ªäº› Collections"""
        if gender == "men":
            return ["adidas ç”·é‹"]
        elif gender == "women":
            return ["adidas å¥³é‹"]
        else:
            # unisex æˆ–æœªçŸ¥ â†’ å…©å€‹éƒ½åŠ 
            return ["adidas ç”·é‹", "adidas å¥³é‹"]

    def _add_to_collection(self, product_id: int, collection_id: int):
        """å°‡å•†å“åŠ å…¥ Collection"""
        try:
            requests.post(
                f"{self.base_url}/collects.json",
                headers=self.headers,
                json={
                    "collect": {
                        "product_id": product_id,
                        "collection_id": collection_id,
                    }
                },
                timeout=30,
            )
        except Exception:
            pass

    @staticmethod
    def _build_description_html(
        subtitle_zh: str,
        desc_zh: str,
        specs: list,
        origin: str,
        color: str,
        sku: str,
    ) -> str:
        """çµ„åˆå•†å“æè¿° HTML"""
        parts = []

        # èª¬æ˜å€å¡Š
        if subtitle_zh:
            parts.append(f"<h3>{subtitle_zh}</h3>")
        if desc_zh:
            parts.append(f"<p>{desc_zh}</p>")

        # è©³ç´°è¦æ ¼
        if specs:
            parts.append("<h3>å•†å“è©³ç´°</h3>")
            parts.append("<ul>")
            for spec in specs:
                parts.append(f"  <li>{spec}</li>")
            parts.append("</ul>")

        # å•†å“è³‡è¨Šè¡¨
        info_rows = []
        if color:
            info_rows.append(f'<tr><td><strong>é¡è‰²</strong></td><td>{color}</td></tr>')
        info_rows.append(f'<tr><td><strong>å‹è™Ÿ</strong></td><td>{sku}</td></tr>')
        if origin:
            info_rows.append(f'<tr><td><strong>ç”¢åœ°</strong></td><td>{origin}</td></tr>')
        
        if info_rows:
            parts.append("<table>")
            parts.extend(info_rows)
            parts.append("</table>")

        return "\n".join(parts)
