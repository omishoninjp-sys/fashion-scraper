"""
BEAMS å®˜ç¶²çˆ¬èŸ² â†’ Shopify è‡ªå‹•ä¸Šæ¶ç³»çµ±
åŠŸèƒ½ï¼š
1. ç²¾é¸åˆ†é¡çˆ¬èŸ²ï¼ˆæ‰‹å‹•é¸åˆ†é¡ï¼‰
2. æ—¥æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡ï¼ˆGoogle Translate API / DeepLï¼‰
3. ä»£è³¼åƒ¹æ ¼è‡ªå‹•è¨ˆç®—ï¼ˆæ—¥å¹£â†’å°å¹£ + æ‰‹çºŒè²» + åœ‹éš›é‹è²»ï¼‰
4. åº«å­˜åŒæ­¥
5. é‡è¤‡å•†å“æª¢æŸ¥
6. éƒ¨ç½²æ–¼ Zeabur
"""

import os
import re
import json
import time
import hashlib
import logging
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from datetime import datetime
from typing import Optional

# ============================================================
# è¨­å®š
# ============================================================

logging.basicConfig(
    level=logging.DEBUG,  # â† DEBUG æ¨¡å¼æ–¹ä¾¿æ’æŸ¥å•é¡Œï¼Œæ­£å¼ä¸Šç·šæ”¹å› INFO
    format="%(asctime)s [%(levelname)s] %(message)s",
)
logger = logging.getLogger(__name__)

# --- ç’°å¢ƒè®Šæ•¸ï¼ˆéƒ¨ç½²æ™‚åœ¨ Zeabur è¨­å®šï¼‰---
SHOPIFY_STORE = os.getenv("SHOPIFY_SHOP", "your-store.myshopify.com")  # é…åˆç¾æœ‰ Zeabur è®Šæ•¸å
SHOPIFY_ACCESS_TOKEN = os.getenv("SHOPIFY_ACCESS_TOKEN", "")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o-mini")  # çœéŒ¢ç”¨ miniï¼Œå¤ æº–
SHIPPING_RATE_PER_KG = int(os.getenv("SHIPPING_RATE_PER_KG", "1250"))  # æ¯å…¬æ–¤åœ‹éš›é‹è²»(æ—¥å¹£)
MARGIN_DIVISOR = float(os.getenv("MARGIN_DIVISOR", "0.7"))  # åˆ©æ½¤é™¤æ•¸ï¼ˆÃ·0.7 = ç´„43%åˆ©æ½¤ï¼‰
SCRAPE_DELAY = float(os.getenv("SCRAPE_DELAY", "2.0"))  # æ¯æ¬¡è«‹æ±‚é–“éš”(ç§’)

# Proxy è¨­å®šï¼ˆè§£æ±ºé›²ç«¯ IP è¢« BEAMS å°é–çš„å•é¡Œï¼‰
# æ ¼å¼: http://user:pass@host:port æˆ– socks5://user:pass@host:port
PROXY_URL = os.getenv("PROXY_URL", "")  # ç•™ç©º = ä¸ä½¿ç”¨ proxy

BASE_URL = "https://www.beams.co.jp"
CDN_URL = "https://cdn.beams.co.jp"

# BEAMS å¯é¸åˆ†é¡å°ç…§è¡¨
CATEGORIES = {
    # ç”·è£
    "men_tshirt": {"path": "/category/t-shirt/", "sex": "M", "name": "ç”·è£ï½œTæ¤"},
    "men_shirt": {"path": "/category/shirt/", "sex": "M", "name": "ç”·è£ï½œè¥¯è¡«"},
    "men_tops": {"path": "/category/tops/", "sex": "M", "name": "ç”·è£ï½œä¸Šè¡£"},
    "men_jacket": {"path": "/category/jacket/", "sex": "M", "name": "ç”·è£ï½œå¤–å¥—"},
    "men_blouson": {"path": "/category/blouson/", "sex": "M", "name": "ç”·è£ï½œå¤¾å…‹"},
    "men_coat": {"path": "/category/coat/", "sex": "M", "name": "ç”·è£ï½œå¤§è¡£"},
    "men_pants": {"path": "/category/pants/", "sex": "M", "name": "ç”·è£ï½œè¤²å­"},
    "men_bag": {"path": "/category/bag/", "sex": "M", "name": "ç”·è£ï½œåŒ…åŒ…"},
    "men_shoes": {"path": "/category/shoes/", "sex": "M", "name": "ç”·è£ï½œé‹å­"},
    "men_hat": {"path": "/category/hat/", "sex": "M", "name": "ç”·è£ï½œå¸½å­"},
    "men_accessory": {"path": "/category/accessory/", "sex": "M", "name": "ç”·è£ï½œé£¾å“"},
    "men_wallet": {"path": "/category/wallet/", "sex": "M", "name": "ç”·è£ï½œçš®å¤¾"},
    "men_watch": {"path": "/category/watch/", "sex": "M", "name": "ç”·è£ï½œæ‰‹éŒ¶"},
    # å¥³è£
    "women_tshirt": {"path": "/category/t-shirt/", "sex": "W", "name": "å¥³è£ï½œTæ¤"},
    "women_shirt": {"path": "/category/shirt/", "sex": "W", "name": "å¥³è£ï½œè¥¯è¡«"},
    "women_tops": {"path": "/category/tops/", "sex": "W", "name": "å¥³è£ï½œä¸Šè¡£"},
    "women_jacket": {"path": "/category/jacket/", "sex": "W", "name": "å¥³è£ï½œå¤–å¥—"},
    "women_skirt": {"path": "/category/skirt/", "sex": "W", "name": "å¥³è£ï½œè£™å­"},
    "women_onepiece": {"path": "/category/one-piece/", "sex": "W", "name": "å¥³è£ï½œæ´‹è£"},
    "women_pants": {"path": "/category/pants/", "sex": "W", "name": "å¥³è£ï½œè¤²å­"},
    "women_bag": {"path": "/category/bag/", "sex": "W", "name": "å¥³è£ï½œåŒ…åŒ…"},
    "women_shoes": {"path": "/category/shoes/", "sex": "W", "name": "å¥³è£ï½œé‹å­"},
    # ç«¥è£
    "kids_tshirt": {"path": "/category/t-shirt/", "sex": "K", "name": "ç«¥è£ï½œTæ¤"},
    "kids_tops": {"path": "/category/tops/", "sex": "K", "name": "ç«¥è£ï½œä¸Šè¡£"},
    "kids_pants": {"path": "/category/pants/", "sex": "K", "name": "ç«¥è£ï½œè¤²å­"},
}

# ============================================================
# HTTP Sessionï¼ˆæ¨¡æ“¬ç€è¦½å™¨ + Proxy æ”¯æ´ï¼‰
# ============================================================

def create_session() -> requests.Session:
    """å»ºç«‹å¸¶æœ‰åˆç† Headers çš„ Sessionï¼Œæ”¯æ´ Proxy"""
    session = requests.Session()
    session.headers.update({
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "ja,ja-JP;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "Connection": "keep-alive",
        "Sec-Fetch-Dest": "document",
        "Sec-Fetch-Mode": "navigate",
        "Sec-Fetch-Site": "none",
        "Sec-Fetch-User": "?1",
        "Upgrade-Insecure-Requests": "1",
        "Cache-Control": "max-age=0",
    })

    # è¨­å®š Proxy
    if PROXY_URL:
        session.proxies = {
            "http": PROXY_URL,
            "https": PROXY_URL,
        }
        logger.info(f"ğŸŒ ä½¿ç”¨ Proxy: {PROXY_URL.split('@')[-1] if '@' in PROXY_URL else PROXY_URL}")
    else:
        logger.warning("âš ï¸ æœªè¨­å®š PROXY_URL â€” é›²ç«¯ IP å¯èƒ½è¢« BEAMS å°é–ï¼")

    return session


# ============================================================
# ç¿»è­¯æ¨¡çµ„ï¼ˆOpenAI ChatGPTï¼‰
# ============================================================

def translate_ja_to_zhtw(text: str) -> str:
    """ä½¿ç”¨ ChatGPT å°‡æ—¥æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡"""
    if not text or not text.strip():
        return text

    if OPENAI_API_KEY:
        return _translate_openai(text)

    logger.warning("æœªè¨­å®š OPENAI_API_KEYï¼Œå›å‚³åŸæ–‡")
    return text


def _translate_openai(text: str) -> str:
    """å‘¼å« OpenAI API ç¿»è­¯"""
    try:
        resp = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json",
            },
            json={
                "model": OPENAI_MODEL,
                "messages": [
                    {
                        "role": "system",
                        "content": (
                            "ä½ æ˜¯æ—¥æœ¬æœé£¾é›»å•†çš„å°ˆæ¥­ç¿»è­¯ã€‚"
                            "å°‡æ—¥æ–‡å•†å“åç¨±å’Œæè¿°ç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚"
                            "ä¿ç•™å“ç‰Œåã€å‹è™Ÿã€è‹±æ–‡ä¸ç¿»è­¯ã€‚"
                            "ç¿»è­¯è¦è‡ªç„¶é€šé †ï¼Œé©åˆå°ç£æ¶ˆè²»è€…é–±è®€ã€‚"
                            "åªå›å‚³ç¿»è­¯çµæœï¼Œä¸è¦åŠ ä»»ä½•è§£é‡‹ã€‚"
                        ),
                    },
                    {
                        "role": "user",
                        "content": f"ç¿»è­¯ä»¥ä¸‹æ—¥æ–‡ï¼š\n{text}",
                    },
                ],
                "temperature": 0.3,
                "max_tokens": 1000,
            },
            timeout=30,
        )
        resp.raise_for_status()
        result = resp.json()["choices"][0]["message"]["content"].strip()
        return result
    except Exception as e:
        logger.warning(f"OpenAI ç¿»è­¯å¤±æ•—: {e}")
        return text  # å¤±æ•—æ™‚å›å‚³åŸæ–‡


# ============================================================
# æœé£¾é‡é‡åƒè€ƒè¡¨ï¼ˆkgï¼‰
# æ ¹æ“š ZenMarketã€Spoketravelã€Printful ç­‰å¤šä¾†æºäº¤å‰æ¯”å°
# ============================================================

WEIGHT_TABLE = {
    # === ä¸Šè¡£é¡ ===
    "t-shirt":    0.20,   # Tæ¤ãƒ»ã‚«ãƒƒãƒˆã‚½ãƒ¼: 150-250g, å–ä¸­é–“åä¸Š
    "shirt":      0.25,   # è¥¯è¡«ãƒ»ãƒ–ãƒ©ã‚¦ã‚¹: 200-300g
    "tops":       0.35,   # æ¯›è¡£ãƒ»é‡ç¹”è¡«ãƒ»ã‚«ãƒ¼ãƒ‡ã‚£ã‚¬ãƒ³: 300-450g
    "blouson":    0.80,   # å¤¾å…‹ãƒ»ãƒ–ãƒ«ã‚¾ãƒ³ï¼ˆMA-1ç­‰ï¼‰: 700-1000g
    "jacket":     0.70,   # å¤–å¥—ãƒ»ã‚¸ãƒ£ã‚±ãƒƒãƒˆï¼ˆè¥¿è£å¤–å¥—ç­‰ï¼‰: 500-900g
    "coat":       1.50,   # å¤§è¡£ãƒ»ã‚³ãƒ¼ãƒˆï¼ˆå†¬å­£åšå¤§è¡£ï¼‰: 1.2-2.0kg

    # === ä¸‹èº«é¡ ===
    "pants":      0.50,   # é•·è¤²ãƒ»ãƒ‘ãƒ³ãƒ„ï¼ˆå«ç‰›ä»”è¤²ï¼‰: 400-700g
    "skirt":      0.30,   # è£™å­ãƒ»ã‚¹ã‚«ãƒ¼ãƒˆ: 200-450g
    "one-piece":  0.40,   # æ´‹è£ãƒ»ãƒ¯ãƒ³ãƒ”ãƒ¼ã‚¹: 300-600g
    "suit":       1.50,   # è¥¿è£å¥—è£ï¼ˆä¸Šä¸‹ï¼‰: 1.3-1.8kg

    # === é…ä»¶é¡ ===
    "bag":        0.60,   # åŒ…åŒ…ãƒ»ãƒãƒƒã‚°: 300-1000g, ä¸­å‹åŒ…å–å‡
    "shoes":      0.80,   # é‹å­ãƒ»ã‚·ãƒ¥ãƒ¼ã‚ºï¼ˆå–®éš»Ã—2ï¼‰: 600-1000g
    "hat":        0.15,   # å¸½å­: 100-200g
    "watch":      0.20,   # æ‰‹éŒ¶: 100-300g (å«ç›’)
    "wallet":     0.15,   # çš®å¤¾ãƒ»å°ç‰©: 100-200g
    "accessory":  0.10,   # é£¾å“ãƒ»ã‚¢ã‚¯ã‚»ã‚µãƒªãƒ¼: 50-150g
    "fashiongoods": 0.15, # æ™‚å°šå°ç‰©: 100-200g
    "legwear":    0.10,   # è¥ªå­: 50-100g
    "underwear":  0.15,   # å…§è‘—: 100-200g
    "hair-accessory": 0.05, # é«®é£¾: 30-80g

    # === ç”Ÿæ´»ãƒ»å…¶ä»– ===
    "interior":   0.80,   # å®¤å…§ç”¨å“: è®ŠåŒ–å¤§, å–ä¸­
    "outdoor":    0.70,   # æˆ¶å¤–é‹å‹•ç”¨å“: è®ŠåŒ–å¤§
    "tablewear":  0.50,   # é£Ÿå™¨ãƒ»å»šå…·
    "hobby":      0.30,   # é›œè²¨
    "cosmetics":  0.20,   # åŒ–å¦å“
    "music":      0.20,   # éŸ³æ¨‚ãƒ»æ›¸ç±
    "maternity":  0.30,   # å­•å©¦è£
    "etc":        0.30,   # å…¶ä»–ï¼ˆé è¨­ï¼‰
}

# é è¨­é‡é‡ï¼ˆæ‰¾ä¸åˆ°å°æ‡‰åˆ†é¡æ™‚ä½¿ç”¨ï¼‰
DEFAULT_WEIGHT_KG = 0.30


def get_estimated_weight(item_type: str) -> float:
    """
    æ ¹æ“šå•†å“åˆ†é¡å–å¾—é ä¼°é‡é‡(kg)
    item_type: BEAMS URL ä¸­çš„åˆ†é¡åï¼Œå¦‚ "t-shirt", "pants", "coat"
    """
    # ç›´æ¥åŒ¹é…
    if item_type in WEIGHT_TABLE:
        return WEIGHT_TABLE[item_type]

    # æ¨¡ç³ŠåŒ¹é…ï¼ˆè™•ç†å­åˆ†é¡å¦‚ "bag_03" â†’ "bag"ï¼‰
    base_type = item_type.split("_")[0] if "_" in item_type else item_type
    if base_type in WEIGHT_TABLE:
        return WEIGHT_TABLE[base_type]

    return DEFAULT_WEIGHT_KG


# ============================================================
# åƒ¹æ ¼è¨ˆç®—æ¨¡çµ„
# å…¬å¼ï¼š(å•†å“åƒ¹æ ¼ + (å•†å“é‡é‡kg Ã— 1250)) Ã· 0.7
# å”®åƒ¹å¹£åˆ¥ï¼šæ—¥å¹£ï¼ˆJPYï¼‰
# ============================================================

def calculate_proxy_price(price_jpy: int, weight_kg: float = DEFAULT_WEIGHT_KG) -> dict:
    """
    ä»£è³¼åƒ¹æ ¼è¨ˆç®—ï¼ˆæ—¥å¹£å”®åƒ¹ï¼‰
    å…¬å¼: (å•†å“åƒ¹æ ¼ + (å•†å“é‡é‡ Ã— æ¯å…¬æ–¤é‹è²»)) Ã· åˆ©æ½¤é™¤æ•¸
    çµæœç„¡æ¢ä»¶é€²ä½åˆ°ç™¾ä½æ—¥å¹£
    """
    import math

    shipping_jpy = weight_kg * SHIPPING_RATE_PER_KG
    subtotal = price_jpy + shipping_jpy
    final_price_raw = subtotal / MARGIN_DIVISOR

    # ç„¡æ¢ä»¶é€²ä½åˆ°ç™¾ä½æ—¥å¹£ï¼ˆçœ‹èµ·ä¾†æ•´é½Šï¼‰
    final_price = math.ceil(final_price_raw / 100) * 100

    return {
        "original_jpy": price_jpy,
        "weight_kg": weight_kg,
        "shipping_jpy": round(shipping_jpy),
        "subtotal_jpy": round(subtotal),
        "margin_divisor": MARGIN_DIVISOR,
        "final_jpy": final_price,
    }


# ============================================================
# BEAMS çˆ¬èŸ²æ ¸å¿ƒ
# ============================================================

class BeamsScraper:
    def __init__(self):
        self.session = create_session()
        self.scraped_items = []

    def scrape_category(self, category_key: str, max_pages: int = 5) -> list[dict]:
        """
        çˆ¬å–æŒ‡å®šåˆ†é¡çš„å•†å“åˆ—è¡¨
        """
        if category_key not in CATEGORIES:
            logger.error(f"æœªçŸ¥åˆ†é¡: {category_key}")
            return []

        cat = CATEGORIES[category_key]
        logger.info(f"ğŸ“¦ é–‹å§‹çˆ¬å–åˆ†é¡: {cat['name']}")

        all_items = []
        page = 1

        while page <= max_pages:
            url = f"{BASE_URL}{cat['path']}"
            # âš ï¸ BEAMS åˆ†é åƒæ•¸æ˜¯ "p" ä¸æ˜¯ "page"
            params = {"sex": cat["sex"]}
            if page > 1:
                params["p"] = page

            full_url = f"{url}?{'&'.join(f'{k}={v}' for k, v in params.items())}"
            logger.info(f"  ğŸ“„ æ­£åœ¨çˆ¬å–ç¬¬ {page} é ... URL: {full_url}")

            try:
                resp = self.session.get(url, params=params, timeout=30)
                resp.raise_for_status()
                logger.debug(f"  ğŸ“¡ HTTP {resp.status_code}, å…§å®¹é•·åº¦: {len(resp.text)} bytes")
            except requests.RequestException as e:
                logger.error(f"  âŒ è«‹æ±‚å¤±æ•—: {e}")
                break

            soup = BeautifulSoup(resp.text, "html.parser")
            items = self._parse_category_page(soup, cat)

            if not items:
                logger.info(f"  âœ… ç¬¬ {page} é ç„¡æ›´å¤šå•†å“ï¼ŒçµæŸ")
                break

            all_items.extend(items)
            logger.info(f"  âœ… ç¬¬ {page} é æ‰¾åˆ° {len(items)} ä»¶å•†å“")

            page += 1
            time.sleep(SCRAPE_DELAY)

        logger.info(f"ğŸ“Š åˆ†é¡ [{cat['name']}] å…±æ‰¾åˆ° {len(all_items)} ä»¶å•†å“")
        return all_items

    def _parse_category_page(self, soup: BeautifulSoup, category: dict) -> list[dict]:
        """è§£æåˆ†é¡é é¢çš„å•†å“åˆ—è¡¨"""
        items = []

        # ========== DEBUG: é é¢çµæ§‹åˆ†æ ==========
        page_title = soup.find("title")
        logger.debug(f"  ğŸ” [DEBUG] é é¢æ¨™é¡Œ: {page_title.text.strip() if page_title else 'ç„¡'}")
        logger.debug(f"  ğŸ” [DEBUG] HTML ç¸½é•·åº¦: {len(str(soup))} chars")

        # æª¢æŸ¥æ‰€æœ‰ <a> æ¨™ç±¤ä¸­å« /item/ çš„é€£çµ
        all_a_tags = soup.find_all("a", href=True)
        item_hrefs = [a["href"] for a in all_a_tags if "/item/" in a.get("href", "")]
        logger.debug(f"  ğŸ” [DEBUG] å…¨éƒ¨ <a> æ¨™ç±¤: {len(all_a_tags)} å€‹, å« /item/ é€£çµ: {len(item_hrefs)} å€‹")

        if item_hrefs:
            logger.debug(f"  ğŸ” [DEBUG] å‰3å€‹ /item/ é€£çµ: {item_hrefs[:3]}")
        else:
            # æ²’æ‰¾åˆ° /item/ é€£çµï¼Œè¼¸å‡ºæ›´å¤šåµéŒ¯è³‡è¨Š
            logger.warning(f"  âš ï¸ [DEBUG] é é¢ä¸­æ‰¾ä¸åˆ°ä»»ä½• /item/ é€£çµï¼")
            # è¼¸å‡ºå‰å¹¾å€‹ <a> href çœ‹çœ‹é é¢çµæ§‹
            sample_hrefs = [a["href"] for a in all_a_tags[:10]]
            logger.warning(f"  âš ï¸ [DEBUG] å‰10å€‹ <a> href: {sample_hrefs}")
            # è¼¸å‡º HTML å‰ 2000 å­—å¹«åŠ©é™¤éŒ¯
            html_snippet = str(soup)[:2000]
            logger.warning(f"  âš ï¸ [DEBUG] HTML å‰ 2000 å­—:\n{html_snippet}")

        # ========== å•†å“åˆ—è¡¨è§£æ ==========
        # BEAMS å•†å“é€£çµæ ¼å¼: /item/{label}/{category}/{item_code}/?color=XX
        product_links = soup.find_all("a", href=re.compile(r"/item/[^/]+/[^/]+/\d+"))

        logger.debug(f"  ğŸ” [DEBUG] regex åŒ¹é…åˆ°çš„å•†å“é€£çµ: {len(product_links)} å€‹")

        seen_codes = set()
        for link in product_links:
            href = link.get("href", "")
            # æå–å•†å“ç·¨è™Ÿ
            match = re.search(r"/item/([^/]+)/([^/]+)/(\d+)", href)
            if not match:
                logger.debug(f"  ğŸ” [DEBUG] regex ç„¡æ³•åŒ¹é…: {href}")
                continue

            label = match.group(1)  # e.g., "beams", "beamsplus"
            item_type = match.group(2)  # e.g., "t-shirt", "pants"
            item_code = match.group(3)  # e.g., "11041456366"

            if item_code in seen_codes:
                continue
            seen_codes.add(item_code)

            # å˜—è©¦å¾åˆ—è¡¨é å–å¾—åŸºæœ¬è³‡æ–™
            # æ¸…ç† hrefï¼Œç§»é™¤ ?color= åƒæ•¸
            clean_href = re.sub(r"\?.*$", "/", href)
            item_data = {
                "item_code": item_code,
                "label": label,
                "item_type": item_type,
                "url": urljoin(BASE_URL, clean_href),
                "category_name": category["name"],
                "sex": category["sex"],
            }

            # å˜—è©¦å–å¾—åƒ¹æ ¼æ–‡å­— â€” å¾ <a> çš„æ–‡å­—å…§å®¹ä¸­æœå°‹
            link_text = link.get_text()
            price_match = re.search(r"[Â¥ï¿¥]\s*([\d,]+)", link_text)
            if price_match:
                price_text = price_match.group(1).replace(",", "")
                if price_text:
                    item_data["price_jpy"] = int(price_text)

            # å˜—è©¦å–å¾—åœ–ç‰‡ URL
            img = link.find("img")
            if img:
                src = img.get("src", "") or img.get("data-src", "")
                if src and "svg" not in src:
                    item_data["thumbnail"] = src if src.startswith("http") else f"https:{src}"

            items.append(item_data)

        logger.info(f"  ğŸ“‹ [è§£æçµæœ] å»é‡å¾Œå•†å“æ•¸: {len(items)} ä»¶ï¼ˆåŸå§‹é€£çµ {len(product_links)} å€‹ï¼‰")
        if items:
            sample = items[0]
            logger.info(f"  ğŸ“‹ [ç¯„ä¾‹å•†å“] code={sample['item_code']}, label={sample['label']}, price={sample.get('price_jpy', 'æœªçŸ¥')}")

        return items

    def scrape_product_detail(self, item: dict) -> dict:
        """çˆ¬å–å–®ä¸€å•†å“çš„è©³ç´°è³‡è¨Š"""
        url = item["url"]
        logger.info(f"  ğŸ” çˆ¬å–å•†å“è©³æƒ…: {item['item_code']}")

        try:
            resp = self.session.get(url, timeout=30)
            resp.raise_for_status()
        except requests.RequestException as e:
            logger.error(f"  âŒ å•†å“è©³æƒ…è«‹æ±‚å¤±æ•—: {e}")
            return item

        soup = BeautifulSoup(resp.text, "html.parser")

        # --- å•†å“åç¨± ---
        title_el = soup.find("h1") or soup.find("title")
        if title_el:
            raw_title = title_el.get_text(strip=True)
            # æ¸…ç†æ¨™é¡Œï¼ˆç§»é™¤ "| BEAMS" ç­‰å¾Œç¶´ï¼‰
            raw_title = re.sub(r"\s*[|ï½œ].*$", "", raw_title)
            item["title_ja"] = raw_title

        # --- åƒ¹æ ¼ï¼ˆå¦‚æœåˆ—è¡¨é æ²’æŠ“åˆ°ï¼‰---
        if "price_jpy" not in item:
            price_el = soup.find(string=re.compile(r"[Â¥ï¿¥]\s*[\d,]+"))
            if price_el:
                price_text = re.sub(r"[^0-9]", "", str(price_el))
                if price_text:
                    item["price_jpy"] = int(price_text)

        # --- å•†å“åœ–ç‰‡ ---
        item["images"] = self._extract_images(soup, item["item_code"])

        # --- å•†å“æè¿° ---
        desc_el = soup.find("div", class_=re.compile(r"item[-_]?desc|product[-_]?desc|detail"))
        if desc_el:
            item["description_ja"] = desc_el.get_text(separator="\n", strip=True)

        # --- å°ºå¯¸/é¡è‰² è®Šé«” ---
        item["variants"] = self._extract_variants(soup)

        # --- åº«å­˜ç‹€æ…‹ ---
        # å¦‚æœæœ‰ã€Œã‚«ãƒ¼ãƒˆã¸å…¥ã‚Œã‚‹ã€æŒ‰éˆ•è¡¨ç¤ºæœ‰åº«å­˜
        cart_btn = soup.find(string=re.compile(r"ã‚«ãƒ¼ãƒˆã¸å…¥ã‚Œã‚‹|ADD TO CART", re.IGNORECASE))
        item["in_stock"] = cart_btn is not None

        # ã€Œå“åˆ‡ã‚Œã€æˆ– ã€ŒSOLD OUTã€ è¡¨ç¤ºç„¡åº«å­˜
        sold_out = soup.find(string=re.compile(r"å“åˆ‡ã‚Œ|SOLD\s*OUT", re.IGNORECASE))
        if sold_out:
            item["in_stock"] = False

        time.sleep(SCRAPE_DELAY)
        return item

    def _extract_images(self, soup: BeautifulSoup, item_code: str) -> list[str]:
        """æå–å•†å“åœ–ç‰‡ URL"""
        images = []
        seen = set()

        # æ‰¾æ‰€æœ‰åŒ…å«å•†å“ç·¨è™Ÿçš„åœ–ç‰‡
        for img in soup.find_all("img"):
            src = img.get("src", "") or img.get("data-src", "")
            if not src or "svg" in src:
                continue
            if item_code in src:
                full_url = src if src.startswith("http") else f"https:{src}"
                # å–å¾—é«˜è§£æåº¦ç‰ˆæœ¬ï¼ˆS1 â†’ L1ï¼‰
                full_url = full_url.replace("/S1/", "/L1/").replace("/S2/", "/L1/")
                if full_url not in seen:
                    seen.add(full_url)
                    images.append(full_url)

        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œç”¨ CDN è¦å‰‡æ¨ç®—
        if not images:
            for suffix in ["C_1", "C_2", "C_3", "D_1", "D_2", "D_3"]:
                img_url = f"{CDN_URL}/img/goods/{item_code}/L1/{item_code}_{suffix}.jpg"
                images.append(img_url)

        return images[:10]  # æœ€å¤š10å¼µ

    def _extract_variants(self, soup: BeautifulSoup) -> list[dict]:
        """æå–å°ºå¯¸/é¡è‰²è®Šé«”"""
        variants = []

        # é¡è‰²é¸é …
        color_els = soup.find_all("img", alt=True, src=re.compile(r"_C_\d+"))
        colors = []
        for el in color_els:
            color_name = el.get("alt", "").strip()
            if color_name and color_name not in colors:
                colors.append(color_name)

        # å°ºå¯¸é¸é …ï¼ˆé€šå¸¸åœ¨ select æˆ– radio ä¸­ï¼‰
        sizes = []
        size_options = soup.find_all(
            ["option", "label", "span"],
            string=re.compile(r"^(XXS|XS|S|M|L|XL|XXL|FREE|F|\d{2,3})$", re.IGNORECASE),
        )
        for opt in size_options:
            size = opt.get_text(strip=True).upper()
            if size and size not in sizes and size != "é¸æŠã—ã¦ãã ã•ã„":
                sizes.append(size)

        # çµ„åˆè®Šé«”
        if colors and sizes:
            for color in colors:
                for size in sizes:
                    variants.append({"color": color, "size": size})
        elif colors:
            for color in colors:
                variants.append({"color": color, "size": "FREE"})
        elif sizes:
            for size in sizes:
                variants.append({"color": "Default", "size": size})
        else:
            variants.append({"color": "Default", "size": "FREE"})

        return variants


# ============================================================
# Shopify ä¸Šæ¶æ¨¡çµ„
# ============================================================

class ShopifyUploader:
    def __init__(self):
        self.api_base = f"https://{SHOPIFY_STORE}/admin/api/2024-01"
        self.headers = {
            "X-Shopify-Access-Token": SHOPIFY_ACCESS_TOKEN,
            "Content-Type": "application/json",
        }
        self._existing_skus: Optional[set] = None

    def get_existing_skus(self) -> set:
        """å–å¾— Shopify ä¸Šå·²æœ‰çš„æ‰€æœ‰ SKUï¼ˆç”¨æ–¼é‡è¤‡æª¢æŸ¥ï¼‰"""
        if self._existing_skus is not None:
            return self._existing_skus

        logger.info("ğŸ“‹ æ­£åœ¨è¼‰å…¥ Shopify å·²æœ‰å•†å“ SKU...")
        skus = set()
        url = f"{self.api_base}/products.json"
        params = {"limit": 250, "fields": "id,variants"}

        while url:
            resp = requests.get(url, headers=self.headers, params=params, timeout=30)
            resp.raise_for_status()
            data = resp.json()

            for product in data.get("products", []):
                for variant in product.get("variants", []):
                    sku = variant.get("sku", "")
                    if sku:
                        skus.add(sku)

            # åˆ†é 
            link_header = resp.headers.get("Link", "")
            next_match = re.search(r'<([^>]+)>;\s*rel="next"', link_header)
            url = next_match.group(1) if next_match else None
            params = {}  # next URL å·²åŒ…å«åƒæ•¸

            time.sleep(0.5)  # Shopify API rate limit

        self._existing_skus = skus
        logger.info(f"ğŸ“‹ å·²è¼‰å…¥ {len(skus)} å€‹ç¾æœ‰ SKU")
        return skus

    def is_duplicate(self, item_code: str) -> bool:
        """æª¢æŸ¥å•†å“æ˜¯å¦å·²å­˜åœ¨"""
        sku = f"BEAMS-{item_code}"
        return sku in self.get_existing_skus()

    def upload_product(self, item: dict) -> Optional[dict]:
        """ä¸Šæ¶å–®ä¸€å•†å“åˆ° Shopify"""
        item_code = item["item_code"]
        sku = f"BEAMS-{item_code}"

        # é‡è¤‡æª¢æŸ¥
        if self.is_duplicate(item_code):
            logger.info(f"  â­ï¸ è·³éé‡è¤‡å•†å“: {item_code}")
            return None

        # ç¿»è­¯
        title_zh = translate_ja_to_zhtw(item.get("title_ja", ""))
        desc_zh = translate_ja_to_zhtw(item.get("description_ja", ""))

        # è¨ˆç®—ä»£è³¼åƒ¹æ ¼ï¼ˆå«é‡é‡ï¼‰
        price_jpy = item.get("price_jpy", 0)
        if not price_jpy:
            logger.warning(f"  âš ï¸ å•†å“ {item_code} ç„¡åƒ¹æ ¼ï¼Œè·³é")
            return None

        weight_kg = get_estimated_weight(item.get("item_type", "etc"))
        pricing = calculate_proxy_price(price_jpy, weight_kg)

        # å»ºç«‹å•†å“æè¿°ï¼ˆåŒ…å«åŸå§‹æ—¥å¹£åƒ¹æ ¼å’Œä»£è³¼è³‡è¨Šï¼‰
        body_html = self._build_description(item, title_zh, desc_zh, pricing)

        # å»ºç«‹ Shopify è®Šé«”
        shopify_variants = []
        for i, v in enumerate(item.get("variants", [{"color": "Default", "size": "FREE"}])):
            shopify_variants.append({
                "option1": v.get("color", "Default"),
                "option2": v.get("size", "FREE"),
                "price": str(pricing["final_jpy"]),
                "sku": f"{sku}-{v.get('color', 'D')}-{v.get('size', 'F')}",
                "inventory_management": "shopify",
                "inventory_quantity": 5 if item.get("in_stock", True) else 0,
                "requires_shipping": True,
                "weight": weight_kg,
                "weight_unit": "kg",
            })

        # å»ºç«‹åœ–ç‰‡åˆ—è¡¨
        shopify_images = [{"src": img} for img in item.get("images", [])[:10]]

        # å»ºç«‹ Shopify å•†å“
        product_data = {
            "product": {
                "title": f"ã€BEAMSã€‘{title_zh}" if title_zh else f"ã€BEAMSã€‘{item.get('title_ja', item_code)}",
                "body_html": body_html,
                "vendor": "BEAMS",
                "product_type": item.get("category_name", "æ—¥æœ¬æœé£¾"),
                "tags": self._build_tags(item),
                "options": [
                    {"name": "é¡è‰²", "values": list({v.get("color", "Default") for v in item.get("variants", [])})},
                    {"name": "å°ºå¯¸", "values": list({v.get("size", "FREE") for v in item.get("variants", [])})},
                ],
                "variants": shopify_variants,
                "images": shopify_images,
                "metafields": [
                    {
                        "namespace": "beams",
                        "key": "original_url",
                        "value": item["url"],
                        "type": "url",
                    },
                    {
                        "namespace": "beams",
                        "key": "item_code",
                        "value": item_code,
                        "type": "single_line_text_field",
                    },
                    {
                        "namespace": "beams",
                        "key": "price_jpy",
                        "value": str(price_jpy),
                        "type": "single_line_text_field",
                    },
                ],
            }
        }

        try:
            resp = requests.post(
                f"{self.api_base}/products.json",
                headers=self.headers,
                json=product_data,
                timeout=30,
            )
            resp.raise_for_status()
            result = resp.json()
            product_id = result["product"]["id"]
            logger.info(f"  âœ… ä¸Šæ¶æˆåŠŸ: {item_code} â†’ Product ID: {product_id}")
            self._existing_skus.add(sku)
            time.sleep(0.5)  # Rate limit
            return result
        except requests.RequestException as e:
            logger.error(f"  âŒ ä¸Šæ¶å¤±æ•—: {item_code} â†’ {e}")
            if hasattr(e, "response") and e.response is not None:
                logger.error(f"     å›æ‡‰: {e.response.text[:500]}")
            return None

    def update_inventory(self, item: dict) -> bool:
        """æ›´æ–°å·²å­˜åœ¨å•†å“çš„åº«å­˜ç‹€æ…‹"""
        item_code = item["item_code"]
        sku = f"BEAMS-{item_code}"

        # æ‰¾åˆ°å°æ‡‰çš„ Shopify å•†å“
        try:
            resp = requests.get(
                f"{self.api_base}/products.json",
                headers=self.headers,
                params={"fields": "id,variants", "limit": 1},
                timeout=30,
            )
            # é€™è£¡ç°¡åŒ–è™•ç†ï¼Œå¯¦éš›æ‡‰ç”¨å»ºè­°ç”¨ metafield æŸ¥è©¢
            # æˆ–ç¶­è­·ä¸€å€‹æœ¬åœ° mapping è³‡æ–™åº«
            logger.info(f"  ğŸ”„ åº«å­˜åŒæ­¥åŠŸèƒ½å·²é å‚™ï¼Œéœ€æ­é…è³‡æ–™åº«ä½¿ç”¨")
            return True
        except Exception as e:
            logger.error(f"  âŒ åº«å­˜æ›´æ–°å¤±æ•—: {e}")
            return False

    def _build_description(self, item: dict, title_zh: str, desc_zh: str, pricing: dict) -> str:
        """å»ºç«‹ Shopify å•†å“æè¿° HTML"""
        return f"""
<div class="beams-product">
  <div class="proxy-info" style="background:#fff3cd;padding:15px;border-radius:8px;margin-bottom:20px;">
    <p style="font-weight:bold;font-size:16px;">ğŸ‡¯ğŸ‡µ æ—¥æœ¬ BEAMS å®˜ç¶²ä»£è³¼</p>
    <p>æ—¥æœ¬å®˜ç¶²å”®åƒ¹ï¼šÂ¥{pricing['original_jpy']:,}</p>
    <p>é ä¼°é‡é‡ï¼š{pricing['weight_kg']}kg ï½œ åœ‹éš›é‹è²»ï¼šÂ¥{pricing['shipping_jpy']:,}</p>
    <p>ä»£è³¼å”®åƒ¹ï¼š<strong style="color:#e74c3c;font-size:20px;">Â¥{pricing['final_jpy']:,}</strong></p>
    <p style="font-size:12px;color:#666;">
      è¨ˆç®—æ–¹å¼ï¼š(å•†å“åƒ¹æ ¼ + é‡é‡Ã—Â¥{SHIPPING_RATE_PER_KG:,}/kg) Ã· {MARGIN_DIVISOR}
    </p>
  </div>

  <div class="product-description">
    <h3>å•†å“èªªæ˜</h3>
    <p>{desc_zh or 'è«‹åƒè€ƒåœ–ç‰‡'}</p>
  </div>

  <div class="original-info" style="margin-top:20px;padding:10px;background:#f8f9fa;border-radius:5px;">
    <p style="font-size:12px;color:#888;">
      ğŸ“Œ åŸå§‹å•†å“åï¼š{item.get('title_ja', '')}
      <br>ğŸ“Œ å“ç‰Œ/Labelï¼š{item.get('label', 'BEAMS').upper()}
      <br>ğŸ“Œ å•†å“ç·¨è™Ÿï¼š{item.get('item_code', '')}
      <br>ğŸ“Œ <a href="{item.get('url', '')}" target="_blank">æŸ¥çœ‹æ—¥æœ¬å®˜ç¶²</a>
    </p>
  </div>

  <div class="notice" style="margin-top:15px;padding:10px;border:1px solid #ddd;border-radius:5px;">
    <p style="font-size:13px;">âš ï¸ ä»£è³¼æ³¨æ„äº‹é …</p>
    <ul style="font-size:12px;color:#666;">
      <li>æœ¬å•†å“ç‚ºæ—¥æœ¬ä»£è³¼ï¼Œä¸‹å–®å¾Œç´„ 7-14 å€‹å·¥ä½œå¤©åˆ°è²¨</li>
      <li>åº«å­˜å³æ™‚åŒæ­¥æ—¥æœ¬å®˜ç¶²ï¼Œå¦‚é‡ç¼ºè²¨å°‡å…¨é¡é€€æ¬¾</li>
      <li>å•†å“åœ–ç‰‡ä¾†æºç‚ºæ—¥æœ¬ BEAMS å®˜ç¶²</li>
      <li>å› ä»£è³¼å•†å“æ€§è³ªï¼Œæ•ä¸æ¥å—é€€æ›è²¨</li>
      <li>é‡é‡ç‚ºé ä¼°å€¼ï¼Œå¯¦éš›é‹è²»ä»¥å‡ºè²¨æ™‚ç§¤é‡ç‚ºæº–</li>
    </ul>
  </div>
</div>
"""

    def _build_tags(self, item: dict) -> str:
        """å»ºç«‹å•†å“æ¨™ç±¤"""
        tags = ["BEAMS", "æ—¥æœ¬ä»£è³¼", "æ—¥ç³»æœé£¾"]

        label = item.get("label", "").upper()
        if label:
            tags.append(label)

        sex = item.get("sex", "")
        sex_map = {"M": "ç”·è£", "W": "å¥³è£", "K": "ç«¥è£"}
        if sex in sex_map:
            tags.append(sex_map[sex])

        cat_name = item.get("category_name", "")
        if "ï½œ" in cat_name:
            tags.append(cat_name.split("ï½œ")[1])

        return ", ".join(tags)


# ============================================================
# ä¸»æµç¨‹
# ============================================================

def run_scraper(categories: list[str], max_pages: int = 3, dry_run: bool = False):
    """
    ä¸»è¦åŸ·è¡Œæµç¨‹

    Args:
        categories: è¦çˆ¬å–çš„åˆ†é¡ key åˆ—è¡¨
        max_pages: æ¯å€‹åˆ†é¡æœ€å¤šçˆ¬å¹¾é 
        dry_run: True = åªçˆ¬ä¸ä¸Šæ¶ï¼ˆæ¸¬è©¦ç”¨ï¼‰
    """
    # ========== æ“·å– log åˆ°è¨˜æ†¶é«”ï¼Œæ–¹ä¾¿å›å‚³çµ¦å‰ç«¯ ==========
    import io
    log_capture = io.StringIO()
    log_handler = logging.StreamHandler(log_capture)
    log_handler.setLevel(logging.DEBUG)
    log_handler.setFormatter(logging.Formatter("%(levelname)s | %(message)s"))
    logger.addHandler(log_handler)

    scraper = BeamsScraper()
    uploader = ShopifyUploader() if not dry_run else None

    results = {
        "start_time": datetime.now().isoformat(),
        "categories": categories,
        "total_found": 0,
        "total_uploaded": 0,
        "total_skipped_duplicate": 0,
        "total_skipped_no_price": 0,
        "total_failed": 0,
        "items": [],
        "debug_logs": [],  # â† æ–°å¢ï¼šå›å‚³ debug logs çµ¦å‰ç«¯
    }

    for cat_key in categories:
        if cat_key not in CATEGORIES:
            logger.warning(f"âš ï¸ æœªçŸ¥åˆ†é¡: {cat_key}ï¼Œè·³é")
            continue

        # Step 1: çˆ¬å–åˆ†é¡é é¢
        items = scraper.scrape_category(cat_key, max_pages=max_pages)
        results["total_found"] += len(items)

        # Step 2: çˆ¬å–æ¯ä»¶å•†å“çš„è©³ç´°è³‡è¨Š
        for item in items:
            item = scraper.scrape_product_detail(item)

            if dry_run:
                # æ¸¬è©¦æ¨¡å¼ï¼šç¿»è­¯ä¸¦è¨ˆç®—åƒ¹æ ¼ä½†ä¸ä¸Šæ¶
                if item.get("title_ja"):
                    item["title_zh"] = translate_ja_to_zhtw(item["title_ja"])
                if item.get("price_jpy"):
                    weight_kg = get_estimated_weight(item.get("item_type", "etc"))
                    item["weight_kg"] = weight_kg
                    item["pricing"] = calculate_proxy_price(item["price_jpy"], weight_kg)
                results["items"].append(item)
                logger.info(
                    f"  [DRY RUN] {item.get('item_code')} | "
                    f"{item.get('title_ja', '?')} | "
                    f"{item.get('item_type', '?')} ({item.get('weight_kg', '?')}kg) | "
                    f"Â¥{item.get('price_jpy', '?')} â†’ "
                    f"Â¥{item.get('pricing', {}).get('final_jpy', '?')}"
                )
                continue

            # Step 3: é‡è¤‡æª¢æŸ¥ + ä¸Šæ¶
            if uploader.is_duplicate(item["item_code"]):
                results["total_skipped_duplicate"] += 1
                # åº«å­˜åŒæ­¥
                uploader.update_inventory(item)
                continue

            if not item.get("price_jpy"):
                results["total_skipped_no_price"] += 1
                continue

            result = uploader.upload_product(item)
            if result:
                results["total_uploaded"] += 1
            else:
                results["total_failed"] += 1

    results["end_time"] = datetime.now().isoformat()

    # è¼¸å‡ºçµæœæ‘˜è¦
    logger.info("=" * 60)
    logger.info("ğŸ“Š çˆ¬èŸ²åŸ·è¡Œçµæœæ‘˜è¦")
    logger.info(f"  åˆ†é¡æ•¸é‡: {len(categories)}")
    logger.info(f"  ç™¼ç¾å•†å“: {results['total_found']}")
    logger.info(f"  æˆåŠŸä¸Šæ¶: {results['total_uploaded']}")
    logger.info(f"  è·³éé‡è¤‡: {results['total_skipped_duplicate']}")
    logger.info(f"  è·³éç„¡åƒ¹: {results['total_skipped_no_price']}")
    logger.info(f"  ä¸Šæ¶å¤±æ•—: {results['total_failed']}")
    logger.info("=" * 60)

    # ========== æ“·å– debug logs ==========
    logger.removeHandler(log_handler)
    results["debug_logs"] = log_capture.getvalue().split("\n")
    log_capture.close()

    return results


# ============================================================
# CLI å…¥å£
# ============================================================

if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="BEAMS çˆ¬èŸ² â†’ Shopify ä¸Šæ¶")
    parser.add_argument(
        "--categories", "-c",
        nargs="+",
        default=["men_tshirt"],
        help=f"è¦çˆ¬å–çš„åˆ†é¡ï¼Œå¯é¸: {', '.join(CATEGORIES.keys())}",
    )
    parser.add_argument("--max-pages", "-p", type=int, default=3, help="æ¯åˆ†é¡æœ€å¤šé æ•¸")
    parser.add_argument("--dry-run", "-d", action="store_true", help="æ¸¬è©¦æ¨¡å¼ï¼ˆä¸ä¸Šæ¶ï¼‰")
    parser.add_argument("--list-categories", "-l", action="store_true", help="åˆ—å‡ºæ‰€æœ‰å¯é¸åˆ†é¡")

    args = parser.parse_args()

    if args.list_categories:
        print("\nğŸ“‹ å¯é¸åˆ†é¡:")
        for key, cat in CATEGORIES.items():
            print(f"  {key:25s} â†’ {cat['name']}")
        print()
    else:
        run_scraper(
            categories=args.categories,
            max_pages=args.max_pages,
            dry_run=args.dry_run,
        )
