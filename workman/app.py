"""
WORKMAN å•†å“çˆ¬èŸ² + Shopify Bulk Operations ä¸Šæ¶å·¥å…·
ä¾†æºï¼šworkman.jp
åŠŸèƒ½ï¼š
1. çˆ¬å– workman.jp å„åˆ†é¡å•†å“
2. ç¿»è­¯ä¸¦ç”¢ç”Ÿ JSONL æª”æ¡ˆ
3. ä½¿ç”¨ Shopify Bulk Operations API æ‰¹é‡ä¸Šå‚³ï¼ˆæ•¸åƒå•†å“/åˆ†é˜ï¼‰
"""

from flask import Flask, jsonify, send_file
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
import threading

app = Flask(__name__)

# ========== è¨­å®š ==========
SHOPIFY_SHOP = ""
SHOPIFY_ACCESS_TOKEN = ""

SOURCE_URL = "https://workman.jp"
CATEGORIES = {
    'work': {'url': '/shop/c/c51/', 'collection': 'WORKMAN ä½œæ¥­æœ', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ä½œæ¥­æœ', 'å·¥ä½œæœ']},
    'mens': {'url': '/shop/c/c52/', 'collection': 'WORKMAN ç”·è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ç”·è£']},
    'womens': {'url': '/shop/c/c53/', 'collection': 'WORKMAN å¥³è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å¥³è£']},
    'kids': {'url': '/shop/c/c54/', 'collection': 'WORKMAN å…’ç«¥æœ', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å…’ç«¥æœ', 'ç«¥è£']}
}

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
DEFAULT_WEIGHT = 0.5
JSONL_DIR = "/tmp/workman_jsonl"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja,en;q=0.9',
}

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(JSONL_DIR, exist_ok=True)

scrape_status = {
    "running": False,
    "phase": "",  # "scraping" | "uploading"
    "progress": 0,
    "total": 0,
    "current_product": "",
    "products": [],
    "errors": [],
    "jsonl_file": "",
    "bulk_operation_id": "",
    "bulk_status": "",
}

# ========== å·¥å…·å‡½æ•¸ ==========

def load_shopify_token():
    global SHOPIFY_SHOP, SHOPIFY_ACCESS_TOKEN
    if not SHOPIFY_SHOP:
        SHOPIFY_SHOP = os.environ.get("SHOPIFY_SHOP", "")
    if not SHOPIFY_ACCESS_TOKEN:
        SHOPIFY_ACCESS_TOKEN = os.environ.get("SHOPIFY_ACCESS_TOKEN", "")


def graphql_request(query, variables=None):
    """åŸ·è¡Œ GraphQL è«‹æ±‚"""
    load_shopify_token()
    url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/graphql.json"
    headers = {
        'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN,
        'Content-Type': 'application/json',
    }
    payload = {'query': query}
    if variables:
        payload['variables'] = variables
    
    response = requests.post(url, headers=headers, json=payload, timeout=60)
    return response.json()


def calculate_selling_price(cost, weight):
    """è¨ˆç®—å”®åƒ¹"""
    shipping_cost = 800 + (weight * 400)
    base_price = cost + shipping_cost
    selling_price = base_price * 1.15
    return round(selling_price / 10) * 10


def contains_japanese(text):
    if not text:
        return False
    hiragana = re.search(r'[\u3040-\u309F]', text)
    katakana = re.search(r'[\u30A0-\u30FF]', text)
    return bool(hiragana or katakana)


def remove_japanese(text):
    if not text:
        return text
    cleaned = re.sub(r'[\u3040-\u309F\u30A0-\u30FF]+', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


# ========== ç¿»è­¯ ==========

def translate_with_chatgpt(title, description, size_spec=''):
    """ç¿»è­¯å•†å“è³‡è¨Š"""
    size_spec_section = f"\nå°ºå¯¸è¦æ ¼è¡¨ï¼š\n{size_spec}" if size_spec else ""
    
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬æœé£¾å“ç‰Œå•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼š{description[:1500] if description else ''}{size_spec_section}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{
    "title": "ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆç¹é«”ä¸­æ–‡ï¼Œå‰é¢åŠ ä¸Š WORKMANï¼‰",
    "description": "ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆHTMLæ ¼å¼ï¼Œç”¨<br>æ›è¡Œï¼‰",
    "size_spec_translated": "ç¿»è­¯å¾Œçš„å°ºå¯¸è¦æ ¼ï¼ˆæ ¼å¼ï¼šåˆ—1|åˆ—2|åˆ—3ï¼Œæ¯è¡Œæ›è¡Œåˆ†éš”ï¼‰"
}}

è¦å‰‡ï¼š
1. çµ•å°ç¦æ­¢æ—¥æ–‡ï¼ˆå¹³å‡åã€ç‰‡å‡åï¼‰
2. å•†å“åç¨±é–‹é ­å¿…é ˆæ˜¯ã€ŒWORKMANã€
3. å°ºå¯¸æ¬„ä½ç¿»è­¯ï¼šã‚µã‚¤ã‚ºâ†’å°ºå¯¸ã€ç€ä¸ˆâ†’è¡£é•·ã€èº«å¹…â†’èº«å¯¬ã€è‚©å¹…â†’è‚©å¯¬ã€è¢–ä¸ˆâ†’è¢–é•·
4. åªå›å‚³ JSON"""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è¼¸å‡ºç¦æ­¢ä»»ä½•æ—¥æ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0,
                "max_tokens": 1500
            },
            timeout=60
        )
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.strip()
            if content.startswith('```'):
                content = content.split('\n', 1)[1]
            if content.endswith('```'):
                content = content.rsplit('```', 1)[0]
            
            translated = json.loads(content.strip())
            
            trans_title = translated.get('title', title)
            trans_desc = translated.get('description', description)
            trans_size = translated.get('size_spec_translated', '')
            
            # ç§»é™¤æ—¥æ–‡
            if contains_japanese(trans_title):
                trans_title = remove_japanese(trans_title)
            if contains_japanese(trans_desc):
                trans_desc = remove_japanese(trans_desc)
            
            if not trans_title.startswith('WORKMAN'):
                trans_title = f"WORKMAN {trans_title}"
            
            # å»ºç«‹å°ºå¯¸è¡¨ HTML
            size_html = build_size_table_html(trans_size) if trans_size else ''
            if size_html:
                trans_desc += '<br><br>' + size_html
            
            return {'success': True, 'title': trans_title, 'description': trans_desc}
        else:
            return {'success': False, 'title': f"WORKMAN {title}", 'description': description}
            
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {'success': False, 'title': f"WORKMAN {title}", 'description': description}


def build_size_table_html(size_spec_text):
    """å»ºç«‹å°ºå¯¸è¡¨ HTML"""
    if not size_spec_text:
        return ''
    
    lines = [line.strip() for line in size_spec_text.strip().split('\n') if line.strip()]
    if not lines:
        return ''
    
    html = '<div class="size-spec"><h3>ğŸ“ å°ºå¯¸è¦æ ¼</h3>'
    html += '<table style="border-collapse:collapse;width:100%;margin:10px 0;">'
    
    for i, line in enumerate(lines):
        cells = [c.strip() for c in line.split('|')]
        if i == 0:
            html += '<tr style="background:#f5f5f5;">'
            for cell in cells:
                html += f'<th style="border:1px solid #ddd;padding:8px;text-align:center;">{cell}</th>'
            html += '</tr>'
        else:
            html += '<tr>'
            for j, cell in enumerate(cells):
                style = 'border:1px solid #ddd;padding:8px;'
                if j == 0:
                    style += 'font-weight:bold;background:#fafafa;'
                else:
                    style += 'text-align:center;'
                html += f'<td style="{style}">{cell}</td>'
            html += '</tr>'
    
    html += '</table><p style="font-size:12px;color:#666;">â€» å°ºå¯¸å¯èƒ½æœ‰äº›è¨±èª¤å·®</p></div>'
    return html


# ========== çˆ¬å–å‡½æ•¸ ==========

def get_total_pages(category_url):
    """å–å¾—åˆ†é¡ç¸½é æ•¸"""
    url = SOURCE_URL + category_url
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            last_link = soup.find('a', string='æœ€å¾Œ')
            if last_link and last_link.get('href'):
                match = re.search(r'_p(\d+)', last_link['href'])
                if match:
                    return int(match.group(1))
            return 1
    except:
        pass
    return 1


def fetch_all_product_links(category_key):
    """å–å¾—åˆ†é¡å…§æ‰€æœ‰å•†å“é€£çµ"""
    category = CATEGORIES[category_key]
    base_url = category['url']
    total_pages = get_total_pages(base_url)
    
    print(f"[INFO] {category['collection']} å…± {total_pages} é ")
    
    all_links = []
    for page in range(1, total_pages + 1):
        if page == 1:
            page_url = SOURCE_URL + base_url
        else:
            page_url = SOURCE_URL + base_url.replace('/', f'_p{page}/', 1)
        
        try:
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                product_links = soup.find_all('a', class_='block-link')
                for link in product_links:
                    href = link.get('href', '')
                    if '/shop/g/' in href:
                        full_url = SOURCE_URL + href if href.startswith('/') else href
                        if full_url not in all_links:
                            all_links.append(full_url)
        except Exception as e:
            print(f"[ERROR] é é¢ {page} è¼‰å…¥å¤±æ•—: {e}")
        
        time.sleep(0.3)
    
    print(f"[INFO] {category['collection']} å…± {len(all_links)} å€‹å•†å“")
    return all_links


def parse_product_page(url):
    """è§£æå•†å“é é¢"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code != 200:
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ¨™é¡Œ
        title_elem = soup.find('h1', class_='block-goods-name')
        title = title_elem.get_text(strip=True) if title_elem else ''
        
        # åƒ¹æ ¼
        price = 0
        price_elem = soup.find('p', class_='block-goods-price')
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            match = re.search(r'[\d,]+', price_text)
            if match:
                price = int(match.group().replace(',', ''))
        
        # ç®¡ç†ç•ªè™Ÿ
        manage_code = ''
        code_dt = soup.find('dt', string='ç®¡ç†ç•ªå·')
        if code_dt:
            code_dd = code_dt.find_next_sibling('dd')
            if code_dd:
                manage_code = code_dd.get_text(strip=True)
        
        if not manage_code or price < 1000:
            return None
        
        # å•†å“èªªæ˜
        description = ''
        size_spec = ''
        
        comment1 = soup.find('dl', class_='block-goods-comment1')
        if comment1:
            desc_dd = comment1.find('dd', class_='js-goods-tabContents')
            if desc_dd:
                for tag in desc_dd.find_all(['script', 'style']):
                    tag.decompose()
                desc_content = []
                for elem in desc_dd.children:
                    if hasattr(elem, 'name') and elem.name in ['p', 'div']:
                        text = elem.get_text(strip=True)
                        if text:
                            desc_content.append(str(elem))
                description = '\n'.join(desc_content)
        
        comment2 = soup.find('dl', class_='block-goods-comment2')
        if comment2:
            spec_dd = comment2.find('dd', class_='js-goods-tabContents')
            if spec_dd:
                table = spec_dd.find('table')
                if table:
                    rows = table.find_all('tr')
                    for row in rows:
                        cells = row.find_all(['th', 'td'])
                        row_text = ' | '.join([c.get_text(strip=True) for c in cells])
                        size_spec += row_text + '\n'
        
        # é¡è‰²å’Œåœ–ç‰‡
        colors = []
        images = []
        
        slider = soup.find('div', class_='js-goods-detail-goods-slider')
        if slider:
            for img in slider.find_all('img', class_='js-zoom'):
                img_src = img.get('src', '')
                if img_src:
                    full_url = SOURCE_URL + img_src
                    if '_t1.' in img_src:
                        images.insert(0, full_url)
                    elif full_url not in images:
                        images.append(full_url)
        
        gallery = soup.find('ul', class_='js-goods-detail-gallery-slider')
        if gallery:
            for item in gallery.find_all('li', class_='block-goods-gallery--color-variation-src'):
                color_elem = item.find('p', class_='block-goods-detail--color-variation-goods-color-name')
                if color_elem:
                    color = color_elem.get_text(strip=True)
                    if color and color not in colors:
                        colors.append(color)
        
        if not colors:
            colors = ['æ¨™æº–']
        
        # å°ºå¯¸
        sizes = []
        size_dt = soup.find('dt', string='ã‚µã‚¤ã‚ºãƒ»ã‚¹ãƒšãƒƒã‚¯')
        if size_dt:
            size_dd = size_dt.find_next_sibling('dd')
            if size_dd:
                table = size_dd.find('table')
                if table:
                    first_row = table.find('tr')
                    if first_row:
                        for th in first_row.find_all('th')[1:]:
                            size = th.get_text(strip=True)
                            if size and size not in sizes:
                                sizes.append(size)
        
        if not sizes:
            sizes = ['FREE']
        
        images = list(dict.fromkeys(images))[:10]
        
        if not images and manage_code:
            images.append(f"{SOURCE_URL}/img/goods/L/{manage_code}_t1.jpg")
        
        return {
            'url': url,
            'title': title,
            'price': price,
            'manage_code': manage_code,
            'description': description,
            'size_spec': size_spec,
            'colors': colors,
            'sizes': sizes,
            'images': images
        }
        
    except Exception as e:
        print(f"[ERROR] è§£æå¤±æ•— {url}: {e}")
        return None


def product_to_jsonl_entry(product_data, tags):
    """å°‡å•†å“è³‡æ–™è½‰æ›ç‚º JSONL æ ¼å¼ï¼ˆShopify GraphQL ProductInputï¼‰"""
    
    # ç¿»è­¯
    translated = translate_with_chatgpt(
        product_data['title'],
        product_data['description'],
        product_data.get('size_spec', '')
    )
    
    title = translated['title']
    description = translated['description']
    manage_code = product_data['manage_code']
    cost = product_data['price']
    colors = product_data['colors']
    sizes = product_data['sizes']
    images = product_data['images']
    
    selling_price = calculate_selling_price(cost, DEFAULT_WEIGHT)
    
    # å»ºç«‹ variants
    variants = []
    
    if len(colors) > 1 and len(sizes) > 1:
        # é¡è‰² Ã— å°ºå¯¸
        for color in colors:
            for size in sizes:
                variants.append({
                    "price": str(selling_price),
                    "sku": f"{manage_code}-{color}-{size}",
                    "inventoryPolicy": "CONTINUE",
                    "inventoryManagement": None,
                    "weight": DEFAULT_WEIGHT,
                    "weightUnit": "KILOGRAMS",
                    "options": [color, size]
                })
    elif len(colors) > 1:
        for color in colors:
            variants.append({
                "price": str(selling_price),
                "sku": f"{manage_code}-{color}",
                "inventoryPolicy": "CONTINUE",
                "inventoryManagement": None,
                "weight": DEFAULT_WEIGHT,
                "weightUnit": "KILOGRAMS",
                "options": [color]
            })
    elif len(sizes) > 1:
        for size in sizes:
            variants.append({
                "price": str(selling_price),
                "sku": f"{manage_code}-{size}",
                "inventoryPolicy": "CONTINUE",
                "inventoryManagement": None,
                "weight": DEFAULT_WEIGHT,
                "weightUnit": "KILOGRAMS",
                "options": [size]
            })
    else:
        variants.append({
            "price": str(selling_price),
            "sku": manage_code,
            "inventoryPolicy": "CONTINUE",
            "inventoryManagement": None,
            "weight": DEFAULT_WEIGHT,
            "weightUnit": "KILOGRAMS",
        })
    
    # å»ºç«‹ options
    options = []
    if len(colors) > 1 or (len(colors) == 1 and colors[0] != 'æ¨™æº–'):
        options.append("é¡è‰²")
    if len(sizes) > 1 or (len(sizes) == 1 and sizes[0] != 'FREE'):
        options.append("å°ºå¯¸")
    
    # å»ºç«‹ imagesï¼ˆä½¿ç”¨åŸå§‹ URLï¼ŒShopify æœƒè‡ªå‹•æŠ“å–ï¼‰
    image_inputs = []
    for img_url in images:
        image_inputs.append({"src": img_url})
    
    # ProductInput çµæ§‹
    product_input = {
        "title": title,
        "descriptionHtml": description,
        "vendor": "WORKMAN",
        "productType": "",
        "status": "ACTIVE",
        "handle": f"workman-{manage_code}",
        "tags": tags,
    }
    
    if options:
        product_input["options"] = options
    
    if variants:
        product_input["variants"] = variants
    
    if image_inputs:
        product_input["images"] = image_inputs
    
    # Metafield for source URL
    product_input["metafields"] = [
        {
            "namespace": "custom",
            "key": "link",
            "value": product_data['url'],
            "type": "url"
        }
    ]
    
    return {"input": product_input}


# ========== Bulk Operations ==========

def create_staged_upload():
    """å»ºç«‹ Staged Upload URL"""
    query = """
    mutation stagedUploadsCreate($input: [StagedUploadInput!]!) {
      stagedUploadsCreate(input: $input) {
        stagedTargets {
          url
          resourceUrl
          parameters {
            name
            value
          }
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "input": [{
            "resource": "BULK_MUTATION_VARIABLES",
            "filename": "products.jsonl",
            "mimeType": "text/jsonl",
            "httpMethod": "POST"
        }]
    }
    
    result = graphql_request(query, variables)
    
    if 'errors' in result:
        print(f"[Staged Upload Error] {result['errors']}")
        return None
    
    targets = result.get('data', {}).get('stagedUploadsCreate', {}).get('stagedTargets', [])
    if targets:
        return targets[0]
    return None


def upload_jsonl_to_staged(staged_target, jsonl_path):
    """ä¸Šå‚³ JSONL åˆ° Staged URL"""
    url = staged_target['url']
    params = {p['name']: p['value'] for p in staged_target['parameters']}
    
    with open(jsonl_path, 'rb') as f:
        files = {'file': ('products.jsonl', f, 'text/jsonl')}
        response = requests.post(url, data=params, files=files, timeout=300)
    
    return response.status_code in [200, 201, 204]


def run_bulk_mutation(staged_upload_path):
    """åŸ·è¡Œ Bulk Mutation"""
    query = """
    mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) {
      bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) {
        bulkOperation {
          id
          status
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    # productCreate mutation
    mutation = """
    mutation call($input: ProductInput!) {
      productCreate(input: $input) {
        product {
          id
          title
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "mutation": mutation,
        "stagedUploadPath": staged_upload_path
    }
    
    result = graphql_request(query, variables)
    return result


def check_bulk_operation_status(operation_id=None):
    """æª¢æŸ¥ Bulk Operation ç‹€æ…‹"""
    if operation_id:
        query = """
        query($id: ID!) {
          node(id: $id) {
            ... on BulkOperation {
              id
              status
              errorCode
              createdAt
              completedAt
              objectCount
              fileSize
              url
              partialDataUrl
            }
          }
        }
        """
        result = graphql_request(query, {"id": operation_id})
        return result.get('data', {}).get('node', {})
    else:
        # å–å¾—æœ€æ–°çš„ bulk operation
        query = """
        {
          currentBulkOperation(type: MUTATION) {
            id
            status
            errorCode
            createdAt
            completedAt
            objectCount
            fileSize
            url
          }
        }
        """
        result = graphql_request(query)
        return result.get('data', {}).get('currentBulkOperation', {})


# ========== æ‰¹é‡åˆªé™¤åŠŸèƒ½ ==========

def fetch_workman_product_ids():
    """å–å¾—æ‰€æœ‰ WORKMAN å•†å“çš„ IDï¼ˆä½¿ç”¨åˆ†é æŸ¥è©¢ï¼‰"""
    all_ids = []
    cursor = None
    
    while True:
        if cursor:
            query = """
            query($cursor: String) {
              products(first: 250, after: $cursor, query: "vendor:WORKMAN") {
                edges {
                  node {
                    id
                    title
                    handle
                  }
                  cursor
                }
                pageInfo {
                  hasNextPage
                }
              }
            }
            """
            result = graphql_request(query, {"cursor": cursor})
        else:
            query = """
            {
              products(first: 250, query: "vendor:WORKMAN") {
                edges {
                  node {
                    id
                    title
                    handle
                  }
                  cursor
                }
                pageInfo {
                  hasNextPage
                }
              }
            }
            """
            result = graphql_request(query)
        
        products = result.get('data', {}).get('products', {})
        edges = products.get('edges', [])
        
        for edge in edges:
            node = edge['node']
            all_ids.append({
                'id': node['id'],
                'title': node['title'],
                'handle': node['handle']
            })
            cursor = edge['cursor']
        
        if not products.get('pageInfo', {}).get('hasNextPage', False):
            break
        
        time.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶
    
    print(f"[INFO] æ‰¾åˆ° {len(all_ids)} å€‹ WORKMAN å•†å“")
    return all_ids


def create_delete_jsonl(product_ids):
    """ç”¢ç”Ÿåˆªé™¤ç”¨çš„ JSONL æª”æ¡ˆ"""
    jsonl_filename = f"delete_workman_{int(time.time())}.jsonl"
    jsonl_path = os.path.join(JSONL_DIR, jsonl_filename)
    
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for product in product_ids:
            # productDelete çš„ input æ ¼å¼
            entry = {"input": {"id": product['id']}}
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    print(f"[INFO] åˆªé™¤ JSONL å·²ç”¢ç”Ÿ: {jsonl_path} ({len(product_ids)} å€‹å•†å“)")
    return jsonl_path


def run_bulk_delete_mutation(staged_upload_path):
    """åŸ·è¡Œ Bulk Delete Mutation"""
    query = """
    mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) {
      bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) {
        bulkOperation {
          id
          status
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    # productDelete mutation
    mutation = """
    mutation call($input: ProductDeleteInput!) {
      productDelete(input: $input) {
        deletedProductId
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "mutation": mutation,
        "stagedUploadPath": staged_upload_path
    }
    
    result = graphql_request(query, variables)
    return result


def run_delete_workman_products():
    """åŸ·è¡Œæ‰¹é‡åˆªé™¤ WORKMAN å•†å“"""
    global scrape_status
    
    scrape_status = {
        "running": True,
        "phase": "deleting",
        "progress": 0,
        "total": 0,
        "current_product": "æ­£åœ¨æŸ¥è©¢ WORKMAN å•†å“...",
        "products": [],
        "errors": [],
        "jsonl_file": "",
        "bulk_operation_id": "",
        "bulk_status": "",
    }
    
    try:
        # 1. æŸ¥è©¢æ‰€æœ‰ WORKMAN å•†å“
        print("[Delete] æŸ¥è©¢ WORKMAN å•†å“...")
        product_ids = fetch_workman_product_ids()
        
        if not product_ids:
            scrape_status['current_product'] = 'æ²’æœ‰æ‰¾åˆ° WORKMAN å•†å“'
            scrape_status['running'] = False
            return
        
        scrape_status['total'] = len(product_ids)
        scrape_status['current_product'] = f'æ‰¾åˆ° {len(product_ids)} å€‹å•†å“ï¼Œæº–å‚™åˆªé™¤...'
        
        # è¨˜éŒ„è¦åˆªé™¤çš„å•†å“
        for p in product_ids[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
            scrape_status['products'].append({
                'title': p['title'],
                'handle': p['handle'],
                'variants': 0
            })
        
        # 2. ç”¢ç”Ÿåˆªé™¤ JSONL
        print("[Delete] ç”¢ç”Ÿåˆªé™¤ JSONL...")
        jsonl_path = create_delete_jsonl(product_ids)
        scrape_status['jsonl_file'] = jsonl_path
        
        # 3. å»ºç«‹ Staged Upload
        print("[Delete] å»ºç«‹ Staged Upload...")
        scrape_status['current_product'] = 'ä¸Šå‚³åˆªé™¤æ¸…å–®...'
        staged = create_staged_upload()
        
        if not staged:
            scrape_status['errors'].append({'error': 'å»ºç«‹ Staged Upload å¤±æ•—'})
            scrape_status['running'] = False
            return
        
        # 4. ä¸Šå‚³ JSONL
        print("[Delete] ä¸Šå‚³ JSONL...")
        if not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³ JSONL å¤±æ•—'})
            scrape_status['running'] = False
            return
        
        # 5. åŸ·è¡Œ Bulk Delete
        print("[Delete] åŸ·è¡Œæ‰¹é‡åˆªé™¤...")
        scrape_status['current_product'] = 'åŸ·è¡Œæ‰¹é‡åˆªé™¤...'
        
        staged_path = None
        for param in staged['parameters']:
            if param['name'] == 'key':
                staged_path = param['value']
                break
        
        if not staged_path:
            staged_path = staged.get('resourceUrl', '')
        
        result = run_bulk_delete_mutation(staged_path)
        
        if 'errors' in result:
            scrape_status['errors'].append({'error': str(result['errors'])})
            scrape_status['running'] = False
            return
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        
        if user_errors:
            scrape_status['errors'].append({'error': str(user_errors)})
            scrape_status['running'] = False
            return
        
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡åˆªé™¤å·²å•Ÿå‹•ï¼æ­£åœ¨åˆªé™¤ {len(product_ids)} å€‹å•†å“..."
        
        print(f"[Delete] æ“ä½œ ID: {bulk_op.get('id')}, ç‹€æ…‹: {bulk_op.get('status')}")
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        print(f"[ERROR] {e}")
    finally:
        scrape_status['running'] = False


# ========== ä¸»æµç¨‹ ==========

def run_scrape(category):
    """åŸ·è¡Œçˆ¬å–ï¼Œç”¢ç”Ÿ JSONL æª”æ¡ˆ"""
    global scrape_status
    
    scrape_status = {
        "running": True,
        "phase": "scraping",
        "progress": 0,
        "total": 0,
        "current_product": "",
        "products": [],
        "errors": [],
        "jsonl_file": "",
        "bulk_operation_id": "",
        "bulk_status": "",
    }
    
    try:
        categories_to_scrape = []
        if category == 'all':
            categories_to_scrape = ['work', 'mens', 'womens', 'kids']
        elif category in CATEGORIES:
            categories_to_scrape = [category]
        else:
            scrape_status['errors'].append({'error': f'æœªçŸ¥åˆ†é¡: {category}'})
            scrape_status['running'] = False
            return
        
        all_jsonl_entries = []
        
        for cat_key in categories_to_scrape:
            cat_info = CATEGORIES[cat_key]
            tags = cat_info['tags']
            
            scrape_status['current_product'] = f"æ­£åœ¨å–å¾— {cat_info['collection']} å•†å“é€£çµ..."
            product_links = fetch_all_product_links(cat_key)
            scrape_status['total'] += len(product_links)
            
            for idx, link in enumerate(product_links):
                scrape_status['progress'] += 1
                scrape_status['current_product'] = f"è™•ç†ä¸­: {link[-30:]}"
                
                product_data = parse_product_page(link)
                
                if not product_data:
                    scrape_status['errors'].append({'url': link, 'error': 'è§£æå¤±æ•—'})
                    continue
                
                try:
                    print(f"[ç¿»è­¯] {product_data['title'][:30]}...")
                    entry = product_to_jsonl_entry(product_data, tags)
                    all_jsonl_entries.append(entry)
                    
                    scrape_status['products'].append({
                        'title': entry['input']['title'],
                        'handle': entry['input']['handle'],
                        'variants': len(entry['input'].get('variants', []))
                    })
                    print(f"[OK] {entry['input']['title'][:30]}")
                except Exception as e:
                    print(f"[ERROR] {product_data['title'][:20]}: {e}")
                    scrape_status['errors'].append({'url': link, 'error': str(e)})
                
                time.sleep(0.5)  # é¿å…ç¿»è­¯ API éè¼‰
        
        # å¯«å…¥ JSONL æª”æ¡ˆ
        if all_jsonl_entries:
            jsonl_filename = f"workman_{category}_{int(time.time())}.jsonl"
            jsonl_path = os.path.join(JSONL_DIR, jsonl_filename)
            
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for entry in all_jsonl_entries:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            scrape_status['jsonl_file'] = jsonl_path
            print(f"[å®Œæˆ] JSONL æª”æ¡ˆå·²ç”¢ç”Ÿ: {jsonl_path} ({len(all_jsonl_entries)} å€‹å•†å“)")
        
        scrape_status['current_product'] = f"å®Œæˆï¼å…± {len(all_jsonl_entries)} å€‹å•†å“"
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        print(f"[ERROR] {e}")
    finally:
        scrape_status['running'] = False
        scrape_status['phase'] = "completed"


def run_bulk_upload(jsonl_path):
    """åŸ·è¡Œ Bulk Upload"""
    global scrape_status
    
    scrape_status['phase'] = 'uploading'
    scrape_status['running'] = True
    scrape_status['current_product'] = 'æ­£åœ¨æº–å‚™ä¸Šå‚³...'
    
    try:
        # 1. å»ºç«‹ Staged Upload
        print("[Bulk] å»ºç«‹ Staged Upload...")
        scrape_status['current_product'] = 'å»ºç«‹ä¸Šå‚³é€£çµ...'
        staged = create_staged_upload()
        
        if not staged:
            scrape_status['errors'].append({'error': 'å»ºç«‹ Staged Upload å¤±æ•—'})
            return
        
        # 2. ä¸Šå‚³ JSONL
        print("[Bulk] ä¸Šå‚³ JSONL æª”æ¡ˆ...")
        scrape_status['current_product'] = 'ä¸Šå‚³ JSONL æª”æ¡ˆ...'
        
        if not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³ JSONL å¤±æ•—'})
            return
        
        # 3. åŸ·è¡Œ Bulk Mutation
        print("[Bulk] åŸ·è¡Œæ‰¹é‡å»ºç«‹...")
        scrape_status['current_product'] = 'åŸ·è¡Œæ‰¹é‡å»ºç«‹...'
        
        # æ‰¾åˆ° key åƒæ•¸ä½œç‚º stagedUploadPath
        staged_path = None
        for param in staged['parameters']:
            if param['name'] == 'key':
                staged_path = param['value']
                break
        
        if not staged_path:
            staged_path = staged.get('resourceUrl', '')
        
        print(f"[Bulk] Staged path: {staged_path}")
        result = run_bulk_mutation(staged_path)
        
        if 'errors' in result:
            scrape_status['errors'].append({'error': str(result['errors'])})
            return
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        
        if user_errors:
            scrape_status['errors'].append({'error': str(user_errors)})
            return
        
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡æ“ä½œå·²å•Ÿå‹•: {bulk_op.get('status', '')}"
        
        print(f"[Bulk] æ“ä½œ ID: {bulk_op.get('id')}, ç‹€æ…‹: {bulk_op.get('status')}")
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        print(f"[ERROR] {e}")
    finally:
        scrape_status['running'] = False


# ========== Flask è·¯ç”± ==========

@app.route('/')
def index():
    return '''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>WORKMAN çˆ¬èŸ² (Bulk Operations)</title>
    <style>
        * { box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, sans-serif; max-width: 1000px; margin: 0 auto; padding: 20px; background: #f5f5f5; }
        h1 { color: #d32f2f; }
        .card { background: white; border-radius: 12px; padding: 20px; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        .btn { padding: 12px 24px; border: none; border-radius: 8px; cursor: pointer; font-size: 16px; margin: 5px; transition: all 0.2s; }
        .btn:hover { transform: translateY(-2px); }
        .btn-work { background: #1976d2; color: white; }
        .btn-mens { background: #388e3c; color: white; }
        .btn-womens { background: #d81b60; color: white; }
        .btn-kids { background: #f57c00; color: white; }
        .btn-all { background: #7b1fa2; color: white; }
        .btn-upload { background: #d32f2f; color: white; font-size: 18px; padding: 15px 30px; }
        .btn-check { background: #455a64; color: white; }
        .btn-delete { background: #b71c1c; color: white; }
        .btn:disabled { background: #ccc; cursor: not-allowed; transform: none; }
        #status { padding: 15px; background: #e3f2fd; border-radius: 8px; margin: 15px 0; }
        #log { height: 300px; overflow-y: auto; background: #263238; color: #aed581; padding: 15px; border-radius: 8px; font-family: monospace; font-size: 13px; }
        .progress { height: 8px; background: #e0e0e0; border-radius: 4px; margin: 10px 0; }
        .progress-bar { height: 100%; background: linear-gradient(90deg, #4caf50, #8bc34a); border-radius: 4px; transition: width 0.3s; }
        .progress-bar-delete { background: linear-gradient(90deg, #f44336, #ff5722); }
        table { width: 100%; border-collapse: collapse; margin-top: 15px; }
        th, td { padding: 10px; text-align: left; border-bottom: 1px solid #eee; }
        th { background: #f5f5f5; }
        .phase { display: inline-block; padding: 4px 12px; border-radius: 20px; font-size: 12px; font-weight: bold; }
        .phase-scraping { background: #fff3e0; color: #e65100; }
        .phase-uploading { background: #e3f2fd; color: #1565c0; }
        .phase-deleting { background: #ffebee; color: #c62828; }
        .phase-completed { background: #e8f5e9; color: #2e7d32; }
        .warning-box { background: #fff3e0; border: 2px solid #ff9800; border-radius: 8px; padding: 15px; margin: 10px 0; }
    </style>
</head>
<body>
    <h1>ğŸ­ WORKMAN çˆ¬èŸ² (Bulk Operations ç‰ˆ)</h1>
    
    <div class="card">
        <h3>ğŸ“¥ ç¬¬ä¸€æ­¥ï¼šçˆ¬å–å•†å“ â†’ ç”¢ç”Ÿ JSONL</h3>
        <p>é¸æ“‡åˆ†é¡é–‹å§‹çˆ¬å–ï¼Œå®Œæˆå¾Œæœƒç”¢ç”Ÿ JSONL æª”æ¡ˆ</p>
        <button class="btn btn-work" onclick="startScrape('work')">ğŸ”§ ä½œæ¥­æœ</button>
        <button class="btn btn-mens" onclick="startScrape('mens')">ğŸ‘” ç”·è£</button>
        <button class="btn btn-womens" onclick="startScrape('womens')">ğŸ‘— å¥³è£</button>
        <button class="btn btn-kids" onclick="startScrape('kids')">ğŸ‘¶ å…’ç«¥æœ</button>
        <button class="btn btn-all" onclick="startScrape('all')">ğŸš€ å…¨éƒ¨</button>
    </div>
    
    <div class="card">
        <h3>ğŸ“¤ ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ä¸Šå‚³åˆ° Shopify</h3>
        <p>çˆ¬å–å®Œæˆå¾Œï¼Œé»æ“Šä¸‹æ–¹æŒ‰éˆ•æ‰¹é‡ä¸Šå‚³ï¼ˆæ•¸åƒå•†å“åªéœ€å¹¾åˆ†é˜ï¼‰</p>
        <button class="btn btn-upload" id="uploadBtn" onclick="startUpload()" disabled>ğŸ“¤ æ‰¹é‡ä¸Šå‚³åˆ° Shopify</button>
        <button class="btn btn-check" onclick="checkStatus()">ğŸ” æª¢æŸ¥ä¸Šå‚³ç‹€æ…‹</button>
    </div>
    
    <div class="card">
        <h3>ğŸ—‘ï¸ æ‰¹é‡åˆªé™¤ WORKMAN å•†å“</h3>
        <div class="warning-box">
            âš ï¸ <strong>è­¦å‘Šï¼šæ­¤æ“ä½œæœƒåˆªé™¤ Shopify ä¸­æ‰€æœ‰ vendor ç‚º "WORKMAN" çš„å•†å“ï¼</strong>
        </div>
        <button class="btn btn-delete" onclick="startDelete()">ğŸ—‘ï¸ åˆªé™¤æ‰€æœ‰ WORKMAN å•†å“</button>
        <button class="btn btn-check" onclick="countProducts()">ğŸ“Š æŸ¥è©¢å•†å“æ•¸é‡</button>
    </div>
    
    <div class="card">
        <h3>ğŸ“Š åŸ·è¡Œç‹€æ…‹</h3>
        <div id="status">ç­‰å¾…é–‹å§‹...</div>
        <div class="progress"><div class="progress-bar" id="progressBar" style="width:0%"></div></div>
    </div>
    
    <div class="card">
        <h3>ğŸ“‹ åŸ·è¡Œè¨˜éŒ„</h3>
        <div id="log"></div>
    </div>
    
    <script>
        let currentJsonlFile = '';
        
        function startScrape(category) {
            if (!confirm(`ç¢ºå®šè¦çˆ¬å– ${category} åˆ†é¡ï¼Ÿ`)) return;
            
            // é‡ç½®ç‹€æ…‹
            resetTracking();
            document.getElementById('uploadBtn').disabled = true;
            document.getElementById('log').innerHTML = '';
            
            fetch('/api/start?category=' + category)
                .then(r => r.json())
                .then(data => {
                    log('ğŸš€ é–‹å§‹çˆ¬å–: ' + category);
                    pollStatus();
                });
        }
        
        function startUpload() {
            if (!currentJsonlFile) {
                alert('è«‹å…ˆå®Œæˆçˆ¬å–ï¼');
                return;
            }
            if (!confirm('ç¢ºå®šè¦æ‰¹é‡ä¸Šå‚³åˆ° Shopifyï¼Ÿ')) return;
            
            resetTracking();
            
            fetch('/api/upload?file=' + encodeURIComponent(currentJsonlFile))
                .then(r => r.json())
                .then(data => {
                    log('ğŸš€ é–‹å§‹æ‰¹é‡ä¸Šå‚³...');
                    pollStatus();
                });
        }
        
        function checkStatus() {
            fetch('/api/bulk_status')
                .then(r => r.json())
                .then(data => {
                    let status = data.status || 'UNKNOWN';
                    let count = data.objectCount || 0;
                    log(`ğŸ“Š Bulk ç‹€æ…‹: ${status}, è™•ç†æ•¸: ${count}`);
                    if (data.errorCode) {
                        log(`âŒ éŒ¯èª¤ç¢¼: ${data.errorCode}`);
                    }
                });
        }
        
        function startDelete() {
            if (!confirm('âš ï¸ è­¦å‘Šï¼\\n\\næ­¤æ“ä½œæœƒåˆªé™¤ Shopify ä¸­æ‰€æœ‰ WORKMAN å•†å“ï¼\\n\\nç¢ºå®šè¦ç¹¼çºŒå—ï¼Ÿ')) return;
            if (!confirm('å†æ¬¡ç¢ºèªï¼šçœŸçš„è¦åˆªé™¤æ‰€æœ‰ WORKMAN å•†å“å—ï¼Ÿ')) return;
            
            resetTracking();
            document.getElementById('log').innerHTML = '';
            
            fetch('/api/delete')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ éŒ¯èª¤: ' + data.error);
                    } else {
                        log('ğŸ—‘ï¸ é–‹å§‹æ‰¹é‡åˆªé™¤...');
                        pollStatus();
                    }
                });
        }
        
        function countProducts() {
            log('ğŸ“Š æ­£åœ¨æŸ¥è©¢ WORKMAN å•†å“æ•¸é‡...');
            fetch('/api/count')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ éŒ¯èª¤: ' + data.error);
                    } else {
                        log(`ğŸ“Š ç›®å‰æœ‰ ${data.count} å€‹ WORKMAN å•†å“`);
                    }
                });
        }
        
        function resetTracking() {
            lastProductCount = 0;
            lastProgress = 0;
            lastPhase = '';
        }
        
        function pollStatus() {
            fetch('/api/status')
                .then(r => r.json())
                .then(data => {
                    updateUI(data);
                    if (data.running) {
                        setTimeout(pollStatus, 1000);  // 1 ç§’æ›´æ–°ä¸€æ¬¡
                    }
                });
        }
        
        let lastProductCount = 0;
        let lastProgress = 0;
        let lastPhase = '';
        
        function updateUI(data) {
            let phaseClass = 'phase-' + data.phase;
            let phaseText = {scraping: 'çˆ¬å–ä¸­', uploading: 'ä¸Šå‚³ä¸­', deleting: 'åˆªé™¤ä¸­', completed: 'å®Œæˆ'}[data.phase] || data.phase;
            
            // éšæ®µè®ŠåŒ–æ™‚è¨˜éŒ„
            if (data.phase !== lastPhase) {
                if (data.phase === 'scraping') log('ğŸ“¥ é–‹å§‹çˆ¬å–å•†å“...');
                else if (data.phase === 'uploading') log('ğŸ“¤ é–‹å§‹ä¸Šå‚³åˆ° Shopify...');
                else if (data.phase === 'deleting') log('ğŸ—‘ï¸ é–‹å§‹åˆªé™¤å•†å“...');
                else if (data.phase === 'completed') log('âœ… ä½œæ¥­å®Œæˆï¼');
                lastPhase = data.phase;
            }
            
            let statusHtml = `<span class="phase ${phaseClass}">${phaseText}</span> `;
            statusHtml += data.current_product || '';
            
            if (data.total > 0) {
                statusHtml += `<br>é€²åº¦: ${data.progress} / ${data.total}`;
                let pct = (data.progress / data.total * 100).toFixed(1);
                statusHtml += ` (${pct}%)`;
            }
            if (data.jsonl_file) {
                statusHtml += `<br>ğŸ“„ JSONL: ${data.jsonl_file.split('/').pop()}`;
                currentJsonlFile = data.jsonl_file;
                document.getElementById('uploadBtn').disabled = false;
            }
            if (data.bulk_operation_id) {
                statusHtml += `<br>ğŸ”„ Bulk ID: ${data.bulk_operation_id.split('/').pop()}`;
                statusHtml += `<br>ğŸ“Š ç‹€æ…‹: ${data.bulk_status}`;
            }
            if (data.errors.length > 0) {
                statusHtml += `<br>âš ï¸ éŒ¯èª¤: ${data.errors.length} å€‹`;
            }
            
            document.getElementById('status').innerHTML = statusHtml;
            
            let pct = data.total > 0 ? (data.progress / data.total * 100) : 0;
            document.getElementById('progressBar').style.width = pct + '%';
            
            // é€²åº¦è®ŠåŒ–æ™‚è¨˜éŒ„
            if (data.progress > lastProgress && data.progress % 10 === 0) {
                log(`ğŸ“Š é€²åº¦: ${data.progress} / ${data.total}`);
            }
            lastProgress = data.progress;
            
            // æ–°å•†å“æ™‚è¨˜éŒ„
            if (data.products.length > lastProductCount) {
                let newProducts = data.products.slice(lastProductCount);
                for (let p of newProducts) {
                    log(`âœ“ ${p.title}`);
                }
                lastProductCount = data.products.length;
            }
            
            // éŒ¯èª¤è¨˜éŒ„
            if (data.errors.length > 0) {
                let lastError = data.errors[data.errors.length - 1];
                if (lastError.url) {
                    log(`âŒ å¤±æ•—: ${lastError.url.split('/').pop()}`);
                }
            }
        }
        
        function log(msg) {
            let logDiv = document.getElementById('log');
            let time = new Date().toLocaleTimeString();
            // é¿å…é‡è¤‡è¨Šæ¯
            if (!logDiv.innerHTML.includes(msg)) {
                logDiv.innerHTML = `[${time}] ${msg}\n` + logDiv.innerHTML;
            }
        }
        
        // åˆå§‹è¼‰å…¥ç‹€æ…‹
        pollStatus();
    </script>
</body>
</html>'''


@app.route('/api/status')
def api_status():
    return jsonify(scrape_status)


@app.route('/api/start')
def api_start():
    from flask import request
    category = request.args.get('category', 'mens')
    
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_scrape, args=(category,))
    thread.start()
    
    return jsonify({'started': True, 'category': category})


@app.route('/api/upload')
def api_upload():
    from flask import request
    jsonl_file = request.args.get('file', '')
    
    if not jsonl_file or not os.path.exists(jsonl_file):
        return jsonify({'error': 'JSONL æª”æ¡ˆä¸å­˜åœ¨'})
    
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_bulk_upload, args=(jsonl_file,))
    thread.start()
    
    return jsonify({'started': True, 'file': jsonl_file})


@app.route('/api/bulk_status')
def api_bulk_status():
    op_id = scrape_status.get('bulk_operation_id', '')
    status = check_bulk_operation_status(op_id if op_id else None)
    return jsonify(status)


@app.route('/api/test')
def api_test():
    """æ¸¬è©¦ Shopify é€£ç·š"""
    load_shopify_token()
    result = graphql_request("{ shop { name } }")
    return jsonify(result)


@app.route('/api/delete')
def api_delete():
    """æ‰¹é‡åˆªé™¤æ‰€æœ‰ WORKMAN å•†å“"""
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_delete_workman_products)
    thread.start()
    
    return jsonify({'started': True})


@app.route('/api/count')
def api_count():
    """æŸ¥è©¢ WORKMAN å•†å“æ•¸é‡"""
    try:
        load_shopify_token()
        query = """
        {
          productsCount(query: "vendor:WORKMAN") {
            count
          }
        }
        """
        result = graphql_request(query)
        count = result.get('data', {}).get('productsCount', {}).get('count', 0)
        return jsonify({'count': count})
    except Exception as e:
        return jsonify({'error': str(e)})


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
