"""
WORKMAN å•†å“çˆ¬èŸ² + Shopify Bulk Operations ä¸Šæ¶å·¥å…· + åº«å­˜åŒæ­¥
ä¾†æºï¼šworkman.jp
åŠŸèƒ½ï¼š
1. çˆ¬å– workman.jp å„åˆ†é¡å•†å“
2. ç¿»è­¯ä¸¦ç”¢ç”Ÿ JSONL æª”æ¡ˆ
3. ä½¿ç”¨ Shopify Bulk Operations API æ‰¹é‡ä¸Šå‚³
4. åº«å­˜åŒæ­¥ï¼šæª¢æŸ¥å®˜ç¶²åº«å­˜ç‹€æ…‹ï¼Œç¼ºè²¨å•†å“è‡ªå‹•ä¸‹æ¶
"""

from flask import Flask, jsonify, send_file
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
import threading

app = Flask(__name__)

# ========== è¨­å®š ==========
SHOPIFY_SHOP = ""
SHOPIFY_ACCESS_TOKEN = ""

SOURCE_URL = "https://workman.jp"
CATEGORIES = {
    'work': {'url': '/shop/c/c51/', 'collection': 'WORKMAN ä½œæ¥­æœ', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ä½œæ¥­æœ', 'å·¥ä½œæœ']},
    'mens': {'url': '/shop/c/c52/', 'collection': 'WORKMAN ç”·è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ç”·è£']},
    'womens': {'url': '/shop/c/c53/', 'collection': 'WORKMAN å¥³è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å¥³è£']},
    'kids': {'url': '/shop/c/c54/', 'collection': 'WORKMAN å…’ç«¥', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å…’ç«¥', 'ç«¥è£']}
}

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
DEFAULT_WEIGHT = 0.5
JSONL_DIR = "/tmp/workman_jsonl"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja,en;q=0.9',
}

# ç¼ºè²¨é—œéµå­—
OUT_OF_STOCK_KEYWORDS = [
    'åº—èˆ—ã®ã¿ã®ãŠå–ã‚Šæ‰±ã„',
    'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢è²©å£²çµ‚äº†',
    'åº—èˆ—åœ¨åº«ã‚’ç¢ºèªã™ã‚‹',
    'äºˆç´„å—ä»˜ã¯çµ‚äº†',
    'å—ä»˜çµ‚äº†',
    'å–ã‚Šæ‰±ã„ã‚’çµ‚äº†',
]

os.makedirs(JSONL_DIR, exist_ok=True)

scrape_status = {
    "running": False, "phase": "", "progress": 0, "total": 0,
    "current_product": "", "products": [], "errors": [],
    "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "",
}

inventory_sync_status = {
    "running": False, "phase": "", "progress": 0, "total": 0,
    "current_product": "",
    "results": {"checked": 0, "in_stock": 0, "out_of_stock": 0, "draft_set": 0, "inventory_zeroed": 0, "errors": 0, "page_gone": 0},
    "details": [], "errors": [],
}

def reset_inventory_sync_status():
    global inventory_sync_status
    inventory_sync_status = {
        "running": False, "phase": "", "progress": 0, "total": 0,
        "current_product": "",
        "results": {"checked": 0, "in_stock": 0, "out_of_stock": 0, "draft_set": 0, "inventory_zeroed": 0, "errors": 0, "page_gone": 0},
        "details": [], "errors": [],
    }

# ========== å·¥å…·å‡½æ•¸ ==========

def load_shopify_token():
    global SHOPIFY_SHOP, SHOPIFY_ACCESS_TOKEN
    if not SHOPIFY_SHOP:
        SHOPIFY_SHOP = os.environ.get("SHOPIFY_SHOP", "")
    if not SHOPIFY_ACCESS_TOKEN:
        SHOPIFY_ACCESS_TOKEN = os.environ.get("SHOPIFY_ACCESS_TOKEN", "")

def graphql_request(query, variables=None):
    load_shopify_token()
    url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/graphql.json"
    headers = {'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN, 'Content-Type': 'application/json'}
    payload = {'query': query}
    if variables:
        payload['variables'] = variables
    response = requests.post(url, headers=headers, json=payload, timeout=60)
    return response.json()

_collection_id_cache = {}

def get_or_create_collection(collection_name):
    global _collection_id_cache
    if collection_name in _collection_id_cache:
        return _collection_id_cache[collection_name]
    query = """
    query findCollection($title: String!) {
      collections(first: 1, query: $title) { edges { node { id title } } }
    }
    """
    result = graphql_request(query, {"title": f"title:{collection_name}"})
    edges = result.get('data', {}).get('collections', {}).get('edges', [])
    for edge in edges:
        if edge['node']['title'] == collection_name:
            collection_id = edge['node']['id']
            _collection_id_cache[collection_name] = collection_id
            return collection_id
    mutation = """
    mutation createCollection($input: CollectionInput!) {
      collectionCreate(input: $input) { collection { id title } userErrors { field message } }
    }
    """
    result = graphql_request(mutation, {"input": {"title": collection_name, "descriptionHtml": f"<p>{collection_name} å•†å“ç³»åˆ—</p>"}})
    collection = result.get('data', {}).get('collectionCreate', {}).get('collection')
    if collection:
        collection_id = collection['id']
        _collection_id_cache[collection_name] = collection_id
        publish_collection_to_all_channels(collection_id)
        return collection_id
    return None

def publish_collection_to_all_channels(collection_id):
    publication_ids = get_all_publication_ids()
    if not publication_ids:
        return
    publication_inputs = [{"publicationId": pub_id} for pub_id in publication_ids]
    mutation = """
    mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
      publishablePublish(id: $id, input: $input) { publishable { availablePublicationsCount { count } } userErrors { field message } }
    }
    """
    graphql_request(mutation, {"id": collection_id, "input": publication_inputs})

def get_all_publication_ids():
    query = '{ publications(first: 20) { edges { node { id name } } } }'
    result = graphql_request(query)
    return [edge['node']['id'] for edge in result.get('data', {}).get('publications', {}).get('edges', [])]

def calculate_selling_price(cost, weight):
    shipping_cost = weight * 1250
    base_price = cost + shipping_cost
    selling_price = base_price / 0.7
    return int(selling_price)

def contains_japanese(text):
    if not text:
        return False
    return bool(re.search(r'[\u3040-\u309F]', text) or re.search(r'[\u30A0-\u30FF]', text))

def remove_japanese(text):
    if not text:
        return text
    cleaned = re.sub(r'[\u3040-\u309F\u30A0-\u30FF]+', '', text)
    return re.sub(r'\s+', ' ', cleaned).strip()

# ========== ç¿»è­¯ ==========

def translate_with_chatgpt(title, description, size_spec=''):
    size_spec_section = f"\nå°ºå¯¸è¦æ ¼è¡¨ï¼š\n{size_spec}" if size_spec else ""
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬æœé£¾å“ç‰Œå•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼š{description[:1500] if description else ''}{size_spec_section}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{
    "title": "ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆç¹é«”ä¸­æ–‡ï¼Œå‰é¢åŠ ä¸Š WORKMANï¼‰",
    "description": "ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆHTMLæ ¼å¼ï¼Œç”¨<br>æ›è¡Œï¼‰",
    "size_spec_translated": "ç¿»è­¯å¾Œçš„å°ºå¯¸è¦æ ¼ï¼ˆæ ¼å¼ï¼šåˆ—1|åˆ—2|åˆ—3ï¼Œæ¯è¡Œæ›è¡Œåˆ†éš”ï¼‰"
}}

è¦å‰‡ï¼š
1. çµ•å°ç¦æ­¢æ—¥æ–‡ï¼ˆå¹³å‡åã€ç‰‡å‡åï¼‰
2. å•†å“åç¨±é–‹é ­å¿…é ˆæ˜¯ã€ŒWORKMANã€
3. å°ºå¯¸æ¬„ä½ç¿»è­¯ï¼šã‚µã‚¤ã‚ºâ†’å°ºå¯¸ã€ç€ä¸ˆâ†’è¡£é•·ã€èº«å¹…â†’èº«å¯¬ã€è‚©å¹…â†’è‚©å¯¬ã€è¢–ä¸ˆâ†’è¢–é•·
4. å®Œå…¨å¿½ç•¥æ³¨æ„äº‹é …ï¼ˆã”æ³¨æ„ã€æ³¨æ„äº‹é …ã€ã”äº†æ‰¿ã€â€»è¨˜è™Ÿé–‹é ­çš„è­¦å‘Šæ–‡å­—ç­‰ï¼‰
5. å®Œå…¨å¿½ç•¥åƒ¹æ ¼ç›¸é—œå…§å®¹ï¼ˆå††ã€æ—¥åœ“ã€OFFã€å‰²å¼•ã€å€¤ä¸‹ã’ç­‰ï¼‰
6. åªå›å‚³ JSON"""

    try:
        response = requests.post("https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
            json={"model": "gpt-4o-mini", "messages": [
                {"role": "system", "content": "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è¼¸å‡ºç¦æ­¢ä»»ä½•æ—¥æ–‡ã€‚"},
                {"role": "user", "content": prompt}
            ], "temperature": 0, "max_tokens": 1500}, timeout=60)
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content'].strip()
            if content.startswith('```'):
                content = content.split('\n', 1)[1]
            if content.endswith('```'):
                content = content.rsplit('```', 1)[0]
            translated = json.loads(content.strip())
            trans_title = translated.get('title', title)
            trans_desc = translated.get('description', description)
            trans_size = translated.get('size_spec_translated', '')
            if contains_japanese(trans_title):
                trans_title = remove_japanese(trans_title)
            if contains_japanese(trans_desc):
                trans_desc = remove_japanese(trans_desc)
            if not trans_title.startswith('WORKMAN'):
                trans_title = f"WORKMAN {trans_title}"
            size_html = build_size_table_html(trans_size) if trans_size else ''
            if size_html:
                trans_desc += '<br><br>' + size_html
            return {'success': True, 'title': trans_title, 'description': trans_desc}
        else:
            return {'success': False, 'title': f"WORKMAN {title}", 'description': description}
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {'success': False, 'title': f"WORKMAN {title}", 'description': description}

def build_size_table_html(size_spec_text):
    if not size_spec_text:
        return ''
    lines = [line.strip() for line in size_spec_text.strip().split('\n') if line.strip()]
    if not lines:
        return ''
    html = '<div class="size-spec"><h3>ğŸ“ å°ºå¯¸è¦æ ¼</h3>'
    html += '<table style="border-collapse:collapse;width:100%;margin:10px 0;">'
    for i, line in enumerate(lines):
        cells = [c.strip() for c in line.split('|')]
        if i == 0:
            html += '<tr style="background:#f5f5f5;">'
            for cell in cells:
                html += f'<th style="border:1px solid #ddd;padding:8px;text-align:center;">{cell}</th>'
            html += '</tr>'
        else:
            html += '<tr>'
            for j, cell in enumerate(cells):
                style = 'border:1px solid #ddd;padding:8px;'
                if j == 0:
                    style += 'font-weight:bold;background:#fafafa;'
                else:
                    style += 'text-align:center;'
                html += f'<td style="{style}">{cell}</td>'
            html += '</tr>'
    html += '</table><p style="font-size:12px;color:#666;">â€» å°ºå¯¸å¯èƒ½æœ‰äº›è¨±èª¤å·®</p></div>'
    return html

# ========== çˆ¬å–å‡½æ•¸ ==========

def get_total_pages(category_url):
    url = SOURCE_URL + category_url
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            last_link = soup.find('a', string='æœ€å¾Œ')
            if last_link and last_link.get('href'):
                match = re.search(r'_p(\d+)', last_link['href'])
                if match:
                    return int(match.group(1))
            pagination = soup.find_all('a', href=re.compile(r'_p\d+'))
            max_page = 1
            for link in pagination:
                match = re.search(r'_p(\d+)', link.get('href', ''))
                if match:
                    max_page = max(max_page, int(match.group(1)))
            if max_page > 1:
                return max_page
            pager = soup.find('div', class_=re.compile(r'pager|pagination'))
            if pager:
                for link in pager.find_all('a'):
                    text = link.get_text(strip=True)
                    if text.isdigit():
                        max_page = max(max_page, int(text))
                return max_page
            return 1
    except Exception as e:
        print(f"[ERROR] å–å¾—ç¸½é æ•¸å¤±æ•—: {e}")
    return 1

def fetch_all_product_links(category_key):
    category = CATEGORIES[category_key]
    base_url = category['url']
    total_pages = get_total_pages(base_url)
    print(f"[INFO] {category['collection']} å…± {total_pages} é ")
    all_links = []
    for page in range(1, total_pages + 1):
        if page == 1:
            page_url = SOURCE_URL + base_url
        else:
            page_url = SOURCE_URL + base_url.rstrip('/') + f'_p{page}/'
        try:
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    if '/shop/g/' in href:
                        full_url = SOURCE_URL + href if href.startswith('/') else href
                        full_url = full_url.split('?')[0]
                        if full_url not in all_links:
                            all_links.append(full_url)
            elif response.status_code == 404:
                break
        except Exception as e:
            print(f"[ERROR] é é¢ {page} è¼‰å…¥å¤±æ•—: {e}")
        time.sleep(0.5)
    print(f"[INFO] {category['collection']} å…± {len(all_links)} å€‹å•†å“")
    return all_links

def parse_product_page(url):
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code != 200:
            return None
        soup = BeautifulSoup(response.text, 'html.parser')
        # === ç¼ºè²¨æª¢æŸ¥ï¼šç™¼ç¾ç¼ºè²¨é—œéµå­—ç›´æ¥è·³éä¸ä¸Šæ¶ ===
        page_text = soup.get_text()
        for keyword in OUT_OF_STOCK_KEYWORDS:
            if keyword in page_text:
                print(f"[è·³é] ç¼ºè²¨ï¼ˆ{keyword}ï¼‰: {url}")
                return None
        if 'å£²ã‚Šåˆ‡ã‚Œ' in page_text or 'å“åˆ‡ã‚Œ' in page_text:
            print(f"[è·³é] ç¼ºè²¨ï¼ˆå£²ã‚Šåˆ‡ã‚Œ/å“åˆ‡ã‚Œ/äºˆç´„å—ä»˜ã¯çµ‚äº†ï¼‰: {url}")
            return None
        # === ç¼ºè²¨æª¢æŸ¥çµæŸ ===
        title = ''
        title_elem = soup.find('h1', class_='block-goods-name')
        if title_elem:
            title = title_elem.get_text(strip=True)
        else:
            title_elem = soup.find('h1')
            if title_elem:
                title = title_elem.get_text(strip=True)
        price = 0
        price_elem = soup.find('p', class_='block-goods-price')
        if not price_elem:
            price_elem = soup.find(class_=re.compile(r'price'))
        if price_elem:
            match = re.search(r'[\d,]+', price_elem.get_text(strip=True))
            if match:
                price = int(match.group().replace(',', ''))
        manage_code = ''
        code_dt = soup.find('dt', string='ç®¡ç†ç•ªå·')
        if code_dt:
            code_dd = code_dt.find_next_sibling('dd')
            if code_dd:
                manage_code = code_dd.get_text(strip=True)
        if not manage_code:
            match = re.search(r'/g/g(\d+)/', url)
            if match:
                manage_code = match.group(1)
        if not manage_code:
            return None
        if price == 0:
            price = 1500
        description = ''
        size_spec = ''
        comment1 = soup.find('dl', class_='block-goods-comment1')
        if comment1:
            desc_dd = comment1.find('dd', class_='js-goods-tabContents')
            if desc_dd:
                for tag in desc_dd.find_all(['script', 'style']):
                    tag.decompose()
                desc_content = []
                for elem in desc_dd.children:
                    if hasattr(elem, 'name') and elem.name in ['p', 'div']:
                        text = elem.get_text(strip=True)
                        if text:
                            desc_content.append(str(elem))
                description = '\n'.join(desc_content)
        comment2 = soup.find('dl', class_='block-goods-comment2')
        if comment2:
            spec_dd = comment2.find('dd', class_='js-goods-tabContents')
            if spec_dd:
                table = spec_dd.find('table')
                if table:
                    for row in table.find_all('tr'):
                        cells = row.find_all(['th', 'td'])
                        size_spec += ' | '.join([c.get_text(strip=True) for c in cells]) + '\n'
        colors = []
        images = []
        slider = soup.find('div', class_='js-goods-detail-goods-slider')
        if slider:
            for img in slider.find_all('img', class_='js-zoom'):
                img_src = img.get('src', '')
                if img_src:
                    full_url = SOURCE_URL + img_src
                    if '_t1.' in img_src:
                        images.insert(0, full_url)
                    elif full_url not in images:
                        images.append(full_url)
        gallery = soup.find('ul', class_='js-goods-detail-gallery-slider')
        if gallery:
            for item in gallery.find_all('li', class_='block-goods-gallery--color-variation-src'):
                color_elem = item.find('p', class_='block-goods-detail--color-variation-goods-color-name')
                if color_elem:
                    color = color_elem.get_text(strip=True)
                    if color and color not in colors:
                        colors.append(color)
        if not colors:
            colors = ['æ¨™æº–']
        sizes = []
        size_dt = soup.find('dt', string='ã‚µã‚¤ã‚ºãƒ»ã‚¹ãƒšãƒƒã‚¯')
        if size_dt:
            size_dd = size_dt.find_next_sibling('dd')
            if size_dd:
                table = size_dd.find('table')
                if table:
                    first_row = table.find('tr')
                    if first_row:
                        for th in first_row.find_all('th')[1:]:
                            size = th.get_text(strip=True)
                            if size and size not in sizes:
                                sizes.append(size)
        if not sizes:
            sizes = ['FREE']
        images = list(dict.fromkeys(images))[:10]
        if not images and manage_code:
            images.append(f"{SOURCE_URL}/img/goods/L/{manage_code}_t1.jpg")
        return {'url': url, 'title': title, 'price': price, 'manage_code': manage_code,
                'description': description, 'size_spec': size_spec, 'colors': colors, 'sizes': sizes, 'images': images}
    except Exception as e:
        print(f"[ERROR] è§£æå¤±æ•— {url}: {e}")
        return None

def product_to_jsonl_entry(product_data, tags, category_key, collection_id, existing_product_id=None):
    PRODUCT_TYPES = {'work': 'WORKMAN ä½œæ¥­æœ', 'mens': 'WORKMAN ç”·è£', 'womens': 'WORKMAN å¥³è£', 'kids': 'WORKMAN å…’ç«¥'}
    product_type = PRODUCT_TYPES.get(category_key, 'WORKMAN')
    translated = translate_with_chatgpt(product_data['title'], product_data['description'], product_data.get('size_spec', ''))
    title = translated['title']
    description = translated['description']
    description = re.sub(r'<a[^>]*>.*?</a>', '', description)
    description = re.sub(r'[^<>]*\d+[,ï¼Œ]?\d*\s*æ—¥åœ“[^<>]*', '', description)
    description = re.sub(r'[^<>]*\d+[,ï¼Œ]?\d*\s*å††[^<>]*', '', description)
    description = re.sub(r'[^<>]*\d+%\s*OFF[^<>]*', '', description, flags=re.IGNORECASE)
    description = re.sub(r'[^<>]*é™åƒ¹[^<>]*', '', description)
    description = re.sub(r'[^<>]*å¤§å¹…[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ³¨æ„äº‹é …[^<>]*', '', description)
    description = re.sub(r'[^<>]*è«‹æ³¨æ„[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ•¬è«‹è«’è§£[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ•¬è«‹è¦‹è«’[^<>]*', '', description)
    description = re.sub(r'[^<>]*â€»[^<>]*', '', description)
    description = re.sub(r'<p>\s*</p>', '', description)
    description = re.sub(r'<br\s*/?>\s*<br\s*/?>', '<br>', description)
    description = re.sub(r'^\s*(<br\s*/?>)+', '', description)
    description = re.sub(r'(<br\s*/?>)+\s*$', '', description)
    description = re.sub(r'\n\s*\n', '\n', description).strip()
    notice = """
<br><br>
<p><strong>ã€è«‹æ³¨æ„ä»¥ä¸‹äº‹é …ã€‘</strong></p>
<p>â€»ä¸æ¥å—é€€æ›è²¨</p>
<p>â€»é–‹ç®±è«‹å…¨ç¨‹éŒ„å½±</p>
<p>â€»å› åº«å­˜æœ‰é™ï¼Œè¨‚è³¼æ™‚é–“ä¸åŒå¯èƒ½æœƒå‡ºç¾ç¼ºè²¨æƒ…æ³ã€‚</p>
"""
    description = description + notice
    manage_code = product_data['manage_code']
    cost = product_data['price']
    colors = product_data['colors']
    sizes = product_data['sizes']
    images = product_data['images']
    source_url = product_data['url']
    selling_price = calculate_selling_price(cost, DEFAULT_WEIGHT)
    product_options = []
    has_color_option = len(colors) > 1 or (len(colors) == 1 and colors[0] != 'æ¨™æº–')
    has_size_option = len(sizes) > 1 or (len(sizes) == 1 and sizes[0] != 'FREE')
    if has_color_option:
        product_options.append({"name": "é¡è‰²", "values": [{"name": c} for c in colors]})
    if has_size_option:
        product_options.append({"name": "å°ºå¯¸", "values": [{"name": s} for s in sizes]})
    image_list = images[:10] if images else []
    first_image = image_list[0] if image_list else None
    files = [{"originalSource": img_url, "contentType": "IMAGE"} for img_url in image_list]
    variant_file = {"originalSource": first_image, "contentType": "IMAGE"} if first_image else None
    variants = []
    if has_color_option and has_size_option:
        for color in colors:
            for size in sizes:
                v = {"price": selling_price, "sku": f"{manage_code}-{color}-{size}", "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}, "optionValues": [{"optionName": "é¡è‰²", "name": color}, {"optionName": "å°ºå¯¸", "name": size}]}
                if variant_file: v["file"] = variant_file
                variants.append(v)
    elif has_color_option:
        for color in colors:
            v = {"price": selling_price, "sku": f"{manage_code}-{color}", "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}, "optionValues": [{"optionName": "é¡è‰²", "name": color}]}
            if variant_file: v["file"] = variant_file
            variants.append(v)
    elif has_size_option:
        for size in sizes:
            v = {"price": selling_price, "sku": f"{manage_code}-{size}", "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}, "optionValues": [{"optionName": "å°ºå¯¸", "name": size}]}
            if variant_file: v["file"] = variant_file
            variants.append(v)
    else:
        v = {"price": selling_price, "sku": manage_code, "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}}
        if variant_file: v["file"] = variant_file
        variants.append(v)
    seo_title = f"{title} | WORKMAN æ—¥æœ¬ä»£è³¼"
    seo_description = f"æ—¥æœ¬ WORKMAN å®˜æ–¹æ­£å“ä»£è³¼ã€‚{title}ï¼Œå°ç£ç¾è²¨æˆ–æ—¥æœ¬ç›´é€ï¼Œå“è³ªä¿è­‰ã€‚GOYOUTATI å¾¡ç”¨é”æ—¥æœ¬ä¼´æ‰‹ç¦®å°ˆé–€åº—ã€‚"
    product_input = {
        "title": title, "descriptionHtml": description, "vendor": "WORKMAN",
        "productType": product_type, "status": "ACTIVE", "handle": f"workman-{manage_code}", "tags": tags,
        "seo": {"title": seo_title, "description": seo_description},
        "metafields": [{"namespace": "custom", "key": "link", "value": source_url, "type": "url"}]
    }
    if existing_product_id: product_input["id"] = existing_product_id
    if collection_id: product_input["collections"] = [collection_id]
    if product_options: product_input["productOptions"] = product_options
    if variants: product_input["variants"] = variants
    if files: product_input["files"] = files
    return {"productSet": product_input, "synchronous": True}

# ========== Bulk Operations ==========

def create_staged_upload():
    query = """mutation stagedUploadsCreate($input: [StagedUploadInput!]!) { stagedUploadsCreate(input: $input) { stagedTargets { url resourceUrl parameters { name value } } userErrors { field message } } }"""
    variables = {"input": [{"resource": "BULK_MUTATION_VARIABLES", "filename": "products.jsonl", "mimeType": "text/jsonl", "httpMethod": "POST"}]}
    result = graphql_request(query, variables)
    if 'errors' in result: return None
    targets = result.get('data', {}).get('stagedUploadsCreate', {}).get('stagedTargets', [])
    return targets[0] if targets else None

def upload_jsonl_to_staged(staged_target, jsonl_path):
    url = staged_target['url']
    params = {p['name']: p['value'] for p in staged_target['parameters']}
    with open(jsonl_path, 'rb') as f:
        files = {'file': ('products.jsonl', f, 'text/jsonl')}
        response = requests.post(url, data=params, files=files, timeout=300)
    return response.status_code in [200, 201, 204]

def run_bulk_mutation(staged_upload_path):
    query = """mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) { bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) { bulkOperation { id status } userErrors { field message } } }"""
    mutation = """mutation call($productSet: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $productSet) { product { id title } userErrors { field message } } }"""
    return graphql_request(query, {"mutation": mutation, "stagedUploadPath": staged_upload_path})

def check_bulk_operation_status(operation_id=None):
    if operation_id:
        query = """query($id: ID!) { node(id: $id) { ... on BulkOperation { id status errorCode createdAt completedAt objectCount fileSize url partialDataUrl } } }"""
        result = graphql_request(query, {"id": operation_id})
        return result.get('data', {}).get('node', {})
    else:
        query = '{ currentBulkOperation(type: MUTATION) { id status errorCode createdAt completedAt objectCount fileSize url } }'
        result = graphql_request(query)
        return result.get('data', {}).get('currentBulkOperation', {})

def get_bulk_operation_results():
    status = check_bulk_operation_status()
    results = {'status': status.get('status'), 'objectCount': status.get('objectCount'), 'errorCode': status.get('errorCode'), 'url': status.get('url')}
    if status.get('url'):
        try:
            response = requests.get(status['url'], timeout=30)
            if response.status_code == 200:
                lines = response.text.strip().split('\n')
                results['total_results'] = len(lines)
                errors, successes = [], []
                for line in lines[:50]:
                    try:
                        data = json.loads(line)
                        if 'data' in data and 'productSet' in data.get('data', {}):
                            ps = data['data']['productSet']
                            ue = ps.get('userErrors', [])
                            if ue: errors.append({'errors': ue})
                            elif ps.get('product'): successes.append({'id': ps['product'].get('id'), 'title': ps['product'].get('title', '')[:50]})
                    except: pass
                results['errors'] = errors[:10]
                results['successes'] = successes[:10]
                results['error_count'] = len(errors)
                results['success_count'] = len(successes)
        except Exception as e:
            results['fetch_error'] = str(e)
    return results

# ========== ç™¼å¸ƒèˆ‡åˆªé™¤ ==========

def get_all_publications():
    query = '{ publications(first: 20) { edges { node { id name catalog { title } } } } }'
    result = graphql_request(query)
    pubs = []
    for edge in result.get('data', {}).get('publications', {}).get('edges', []):
        node = edge.get('node', {})
        pubs.append({'id': node.get('id'), 'name': node.get('name') or node.get('catalog', {}).get('title', 'Unknown')})
    return pubs

def publish_product_to_all_channels(product_id):
    publications = get_all_publications()
    if not publications: return {'success': False, 'error': 'No publications found'}
    publication_inputs = [{"publicationId": pub['id']} for pub in publications]
    mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { publishable { availablePublicationsCount { count } } userErrors { field message } } }"""
    result = graphql_request(mutation, {"id": product_id, "input": publication_inputs})
    user_errors = result.get('data', {}).get('publishablePublish', {}).get('userErrors', [])
    if user_errors: return {'success': False, 'errors': user_errors}
    return {'success': True, 'publications': len(publications)}

def batch_publish_workman_products():
    products = fetch_workman_product_ids()
    if not products: return {'success': False, 'error': 'No WORKMAN products found'}
    publications = get_all_publications()
    if not publications: return {'success': False, 'error': 'No publications found'}
    publication_inputs = [{"publicationId": pub['id']} for pub in publications]
    results = {'total': len(products), 'success': 0, 'failed': 0, 'errors': []}
    for product in products:
        mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
        result = graphql_request(mutation, {"id": product['id'], "input": publication_inputs})
        user_errors = result.get('data', {}).get('publishablePublish', {}).get('userErrors', [])
        if user_errors: results['failed'] += 1
        else: results['success'] += 1
        time.sleep(0.1)
    return results

def fetch_workman_product_ids():
    all_ids = []
    cursor = None
    while True:
        if cursor:
            query = 'query($cursor: String) { products(first: 250, after: $cursor, query: "vendor:WORKMAN") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }'
            result = graphql_request(query, {"cursor": cursor})
        else:
            query = '{ products(first: 250, query: "vendor:WORKMAN") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }'
            result = graphql_request(query)
        edges = result.get('data', {}).get('products', {}).get('edges', [])
        for edge in edges:
            node = edge['node']
            all_ids.append({'id': node['id'], 'title': node['title'], 'handle': node['handle'], 'status': node.get('status', '')})
            cursor = edge['cursor']
        if not result.get('data', {}).get('products', {}).get('pageInfo', {}).get('hasNextPage', False):
            break
        time.sleep(0.5)
    return all_ids

def create_delete_jsonl(product_ids):
    jsonl_path = os.path.join(JSONL_DIR, f"delete_workman_{int(time.time())}.jsonl")
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for product in product_ids:
            f.write(json.dumps({"input": {"id": product['id']}}, ensure_ascii=False) + '\n')
    return jsonl_path

def run_bulk_delete_mutation(staged_upload_path):
    query = """mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) { bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) { bulkOperation { id status } userErrors { field message } } }"""
    mutation = """mutation call($input: ProductDeleteInput!) { productDelete(input: $input) { deletedProductId userErrors { field message } } }"""
    return graphql_request(query, {"mutation": mutation, "stagedUploadPath": staged_upload_path})

def run_delete_workman_products():
    global scrape_status
    scrape_status = {"running": True, "phase": "deleting", "progress": 0, "total": 0, "current_product": "æ­£åœ¨æŸ¥è©¢ WORKMAN å•†å“...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}
    try:
        product_ids = fetch_workman_product_ids()
        if not product_ids:
            scrape_status['current_product'] = 'æ²’æœ‰æ‰¾åˆ° WORKMAN å•†å“'
            scrape_status['running'] = False
            return
        scrape_status['total'] = len(product_ids)
        jsonl_path = create_delete_jsonl(product_ids)
        scrape_status['jsonl_file'] = jsonl_path
        staged = create_staged_upload()
        if not staged:
            scrape_status['errors'].append({'error': 'å»ºç«‹ Staged Upload å¤±æ•—'})
            scrape_status['running'] = False
            return
        if not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³ JSONL å¤±æ•—'})
            scrape_status['running'] = False
            return
        staged_path = None
        for param in staged['parameters']:
            if param['name'] == 'key': staged_path = param['value']; break
        if not staged_path: staged_path = staged.get('resourceUrl', '')
        result = run_bulk_delete_mutation(staged_path)
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        if user_errors:
            scrape_status['errors'].append({'error': str(user_errors)})
            scrape_status['running'] = False
            return
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡åˆªé™¤å·²å•Ÿå‹•ï¼æ­£åœ¨åˆªé™¤ {len(product_ids)} å€‹å•†å“..."
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False

# ========== åº«å­˜åŒæ­¥ ==========

def fetch_workman_products_with_source():
    all_products = []
    cursor = None
    while True:
        after_clause = f', after: "{cursor}"' if cursor else ''
        query = f'{{ products(first: 50, query: "vendor:WORKMAN"{after_clause}) {{ edges {{ node {{ id title handle status metafield(namespace: "custom", key: "link") {{ value }} variants(first: 100) {{ edges {{ node {{ id sku inventoryItem {{ id inventoryLevels(first: 5) {{ edges {{ node {{ id quantities(names: ["available"]) {{ name quantity }} location {{ id }} }} }} }} }} }} }} }} }} cursor }} pageInfo {{ hasNextPage }} }} }}'
        result = graphql_request(query)
        edges = result.get('data', {}).get('products', {}).get('edges', [])
        for edge in edges:
            node = edge['node']
            source_url = node.get('metafield', {}).get('value', '') if node.get('metafield') else ''
            variants = []
            for v_edge in node.get('variants', {}).get('edges', []):
                v_node = v_edge['node']
                inv_item = v_node.get('inventoryItem', {})
                inv_levels = inv_item.get('inventoryLevels', {}).get('edges', [])
                vi = {'id': v_node['id'], 'sku': v_node.get('sku', ''), 'inventory_item_id': inv_item.get('id', ''), 'inventory_levels': []}
                for le in inv_levels:
                    ln = le['node']
                    available = 0
                    for q in ln.get('quantities', []):
                        if q['name'] == 'available': available = q['quantity']
                    vi['inventory_levels'].append({'id': ln['id'], 'location_id': ln.get('location', {}).get('id', ''), 'available': available})
                variants.append(vi)
            all_products.append({'id': node['id'], 'title': node['title'], 'handle': node['handle'], 'status': node['status'], 'source_url': source_url, 'variants': variants})
            cursor = edge['cursor']
        if not result.get('data', {}).get('products', {}).get('pageInfo', {}).get('hasNextPage', False): break
        time.sleep(0.5)
    return all_products

def check_workman_stock(product_url):
    result = {'available': True, 'page_exists': True, 'out_of_stock_reason': ''}
    if not product_url:
        return {'available': False, 'page_exists': False, 'out_of_stock_reason': 'ç„¡ä¾†æºé€£çµ'}
    try:
        response = requests.get(product_url, headers=HEADERS, timeout=30)
        if response.status_code == 404:
            return {'available': False, 'page_exists': False, 'out_of_stock_reason': 'é é¢å·²ä¸å­˜åœ¨ (404)'}
        if response.status_code != 200:
            return {'available': False, 'page_exists': False, 'out_of_stock_reason': f'HTTP {response.status_code}'}
        page_text = BeautifulSoup(response.text, 'html.parser').get_text()
        for keyword in OUT_OF_STOCK_KEYWORDS:
            if keyword in page_text:
                return {'available': False, 'page_exists': True, 'out_of_stock_reason': keyword}
        if 'å£²ã‚Šåˆ‡ã‚Œ' in page_text or 'å“åˆ‡ã‚Œ' in page_text:
            return {'available': False, 'page_exists': True, 'out_of_stock_reason': 'å£²ã‚Šåˆ‡ã‚Œ / å“åˆ‡ã‚Œ / äºˆç´„å—ä»˜ã¯çµ‚äº†'}
        return result
    except requests.exceptions.Timeout:
        return {'available': True, 'page_exists': True, 'out_of_stock_reason': 'é€£ç·šè¶…æ™‚ï¼ˆæš«ä¸è™•ç†ï¼‰'}
    except Exception as e:
        return {'available': True, 'page_exists': True, 'out_of_stock_reason': f'éŒ¯èª¤: {str(e)}ï¼ˆæš«ä¸è™•ç†ï¼‰'}

def set_product_to_draft(product_id):
    mutation = """mutation productUpdate($input: ProductInput!) { productUpdate(input: $input) { product { id status } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"id": product_id, "status": "DRAFT"}})
    errors = result.get('data', {}).get('productUpdate', {}).get('userErrors', [])
    if errors: return False
    return True

def zero_variant_inventory(inventory_item_id, location_id):
    mutation = """mutation inventorySetQuantities($input: InventorySetQuantitiesInput!) { inventorySetQuantities(input: $input) { inventoryAdjustmentGroup { reason } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"reason": "correction", "name": "available", "quantities": [{"inventoryItemId": inventory_item_id, "locationId": location_id, "quantity": 0}]}})
    errors = result.get('data', {}).get('inventorySetQuantities', {}).get('userErrors', [])
    return len(errors) == 0

def run_inventory_sync():
    global inventory_sync_status
    reset_inventory_sync_status()
    inventory_sync_status['running'] = True
    inventory_sync_status['phase'] = 'fetching'
    inventory_sync_status['current_product'] = 'æ­£åœ¨å–å¾— Shopify å•†å“æ¸…å–®...'
    print(f"[Sync] ========== é–‹å§‹åº«å­˜åŒæ­¥ ==========")
    try:
        products = fetch_workman_products_with_source()
        inventory_sync_status['total'] = len(products)
        if not products:
            inventory_sync_status['current_product'] = 'æ²’æœ‰æ‰¾åˆ° WORKMAN å•†å“'
            inventory_sync_status['running'] = False
            return
        inventory_sync_status['phase'] = 'checking'
        for idx, product in enumerate(products):
            inventory_sync_status['progress'] = idx + 1
            inventory_sync_status['current_product'] = f"[{idx+1}/{len(products)}] {product['title'][:30]}"
            if product['status'] == 'DRAFT':
                inventory_sync_status['results']['checked'] += 1
                continue
            source_url = product['source_url']
            if not source_url:
                match = re.search(r'workman-(\d+)', product.get('handle', ''))
                if match: source_url = f"{SOURCE_URL}/shop/g/g{match.group(1)}/"
                else:
                    inventory_sync_status['results']['checked'] += 1
                    inventory_sync_status['results']['errors'] += 1
                    continue
            stock = check_workman_stock(source_url)
            inventory_sync_status['results']['checked'] += 1
            if stock['available']:
                inventory_sync_status['results']['in_stock'] += 1
                inventory_sync_status['details'].append({'title': product['title'][:40], 'status': 'in_stock', 'source_url': source_url})
            else:
                inventory_sync_status['results']['out_of_stock'] += 1
                if not stock['page_exists']: inventory_sync_status['results']['page_gone'] += 1
                for variant in product['variants']:
                    for level in variant['inventory_levels']:
                        if level['available'] > 0:
                            zero_variant_inventory(variant['inventory_item_id'], level['location_id'])
                            inventory_sync_status['results']['inventory_zeroed'] += 1
                if set_product_to_draft(product['id']):
                    inventory_sync_status['results']['draft_set'] += 1
                inventory_sync_status['details'].append({'title': product['title'][:40], 'status': 'out_of_stock', 'reason': stock['out_of_stock_reason'], 'source_url': source_url})
            time.sleep(1)
        inventory_sync_status['phase'] = 'completed'
        r = inventory_sync_status['results']
        inventory_sync_status['current_product'] = f"âœ… å®Œæˆï¼æª¢æŸ¥:{r['checked']} æœ‰è²¨:{r['in_stock']} ç¼ºè²¨:{r['out_of_stock']} è‰ç¨¿:{r['draft_set']}"
        print(f"[Sync] ========== åº«å­˜åŒæ­¥å®Œæˆ ==========")
    except Exception as e:
        inventory_sync_status['errors'].append({'error': str(e)})
        inventory_sync_status['phase'] = 'error'
        print(f"[Sync] âŒ {e}")
    finally:
        inventory_sync_status['running'] = False

# ========== ä¸»æµç¨‹ ==========

def run_test_single():
    global scrape_status
    scrape_status = {"running": True, "phase": "testing", "progress": 0, "total": 1, "current_product": "æ¸¬è©¦å–®å“æ¨¡å¼...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}
    try:
        cat_key = 'kids'
        cat_info = CATEGORIES[cat_key]
        collection_id = get_or_create_collection(cat_info['collection'])
        if not collection_id:
            scrape_status['errors'].append({'error': 'ç„¡æ³•å»ºç«‹ Collection'})
            scrape_status['running'] = False
            return
        product_links = fetch_all_product_links(cat_key)
        if not product_links:
            scrape_status['errors'].append({'error': 'ç„¡æ³•å–å¾—å•†å“é€£çµ'})
            scrape_status['running'] = False
            return
        product_data = parse_product_page(product_links[0])
        if not product_data:
            scrape_status['errors'].append({'error': 'è§£æå•†å“å¤±æ•—'})
            scrape_status['running'] = False
            return
        entry = product_to_jsonl_entry(product_data, cat_info['tags'], cat_key, collection_id)
        product_input = entry['productSet']
        scrape_status['products'].append({'title': product_input['title'], 'handle': product_input['handle'], 'variants': len(product_input.get('variants', []))})
        mutation = """mutation productSet($input: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $input) { product { id title handle status productType seo { title description } variants(first: 10) { edges { node { id sku price taxable inventoryItem { unitCost { amount currencyCode } } } } } } userErrors { field code message } } }"""
        load_shopify_token()
        result = graphql_request(mutation, {"input": product_input, "synchronous": True})
        product_set = result.get('data', {}).get('productSet', {})
        user_errors = product_set.get('userErrors', [])
        if user_errors:
            scrape_status['errors'].append({'error': '; '.join([e.get('message', '') for e in user_errors])})
        else:
            product = product_set.get('product', {})
            publish_result = publish_product_to_all_channels(product.get('id', ''))
            scrape_status['current_product'] = f"âœ… æ¸¬è©¦æˆåŠŸï¼{product.get('title', '')}"
            scrape_status['test_result'] = {'id': product.get('id'), 'title': product.get('title'), 'handle': product.get('handle'), 'productType': product.get('productType', ''), 'seo': product.get('seo', {}), 'variants': product.get('variants', {}), 'published': publish_result.get('publications', 0)}
        scrape_status['progress'] = 1
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False

def run_scrape(category):
    global scrape_status
    scrape_status = {"running": True, "phase": "scraping", "progress": 0, "total": 0, "current_product": "", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}
    try:
        cats = ['work', 'mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not cats:
            scrape_status['errors'].append({'error': f'æœªçŸ¥åˆ†é¡: {category}'})
            scrape_status['running'] = False
            return
        all_entries = []
        for cat_key in cats:
            cat_info = CATEGORIES[cat_key]
            collection_id = get_or_create_collection(cat_info['collection'])
            if not collection_id: continue
            product_links = fetch_all_product_links(cat_key)
            if not product_links: continue
            scrape_status['total'] += len(product_links)
            for link in product_links:
                scrape_status['progress'] += 1
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {link.split('/')[-2]}"
                product_data = parse_product_page(link)
                if not product_data:
                    scrape_status['errors'].append({'url': link, 'error': 'è§£æå¤±æ•—'})
                    continue
                try:
                    entry = product_to_jsonl_entry(product_data, cat_info['tags'], cat_key, collection_id)
                    all_entries.append(entry)
                    scrape_status['products'].append({'title': entry['productSet']['title'], 'handle': entry['productSet']['handle'], 'variants': len(entry['productSet'].get('variants', []))})
                except Exception as e:
                    scrape_status['errors'].append({'url': link, 'error': str(e)})
                time.sleep(0.5)
        if all_entries:
            jsonl_path = os.path.join(JSONL_DIR, f"workman_{category}_{int(time.time())}.jsonl")
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for entry in all_entries:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            scrape_status['jsonl_file'] = jsonl_path
        scrape_status['current_product'] = f"å®Œæˆï¼å…± {len(all_entries)} å€‹å•†å“"
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False
        scrape_status['phase'] = "completed"

def run_bulk_upload(jsonl_path):
    global scrape_status
    scrape_status['phase'] = 'uploading'
    scrape_status['running'] = True
    try:
        staged = create_staged_upload()
        if not staged:
            scrape_status['errors'].append({'error': 'å»ºç«‹ Staged Upload å¤±æ•—'})
            return
        if not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³ JSONL å¤±æ•—'})
            return
        staged_path = None
        for param in staged['parameters']:
            if param['name'] == 'key': staged_path = param['value']; break
        if not staged_path: staged_path = staged.get('resourceUrl', '')
        result = run_bulk_mutation(staged_path)
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        if user_errors:
            scrape_status['errors'].append({'error': str(user_errors)})
            return
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
    finally:
        scrape_status['running'] = False

def update_existing_product_price(product_id, product_data):
    """å·²å­˜åœ¨çš„å•†å“ï¼šåªæ›´æ–°åƒ¹æ ¼ï¼Œä¸é‡æ–°ç¿»è­¯"""
    cost = product_data['price']
    selling_price = calculate_selling_price(cost, DEFAULT_WEIGHT)
    
    # å–å¾—å•†å“çš„æ‰€æœ‰ variants
    query = f"""
    {{
      product(id: "{product_id}") {{
        variants(first: 100) {{
          edges {{
            node {{
              id
              sku
              inventoryItem {{
                id
                inventoryLevels(first: 5) {{
                  edges {{
                    node {{
                      id
                      location {{ id }}
                      quantities(names: ["available"]) {{ name quantity }}
                    }}
                  }}
                }}
              }}
            }}
          }}
        }}
      }}
    }}
    """
    result = graphql_request(query)
    variants = result.get('data', {}).get('product', {}).get('variants', {}).get('edges', [])
    
    updated_variants = 0
    for v_edge in variants:
        v_node = v_edge['node']
        variant_id = v_node['id']
        
        # æ›´æ–°åƒ¹æ ¼
        mutation = """mutation productVariantUpdate($input: ProductVariantInput!) {
            productVariantUpdate(input: $input) {
                productVariant { id }
                userErrors { field message }
            }
        }"""
        graphql_request(mutation, {"input": {"id": variant_id, "price": str(selling_price)}})
        updated_variants += 1
        time.sleep(0.1)
    
    return updated_variants


def set_variant_inventory_available(inventory_item_id, location_id, quantity=10):
    """å°‡ variant åº«å­˜è¨­ç‚ºæœ‰è²¨ï¼ˆé è¨­ 10ï¼‰"""
    mutation = """mutation inventorySetQuantities($input: InventorySetQuantitiesInput!) {
        inventorySetQuantities(input: $input) {
            inventoryAdjustmentGroup { reason }
            userErrors { field message }
        }
    }"""
    result = graphql_request(mutation, {"input": {"reason": "correction", "name": "available", "quantities": [{"inventoryItemId": inventory_item_id, "locationId": location_id, "quantity": quantity}]}})
    errors = result.get('data', {}).get('inventorySetQuantities', {}).get('userErrors', [])
    return len(errors) == 0


def set_product_active(product_id):
    """å°‡å•†å“è¨­ç‚º ACTIVE"""
    mutation = """mutation productUpdate($input: ProductInput!) { productUpdate(input: $input) { product { id status } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"id": product_id, "status": "ACTIVE"}})
    errors = result.get('data', {}).get('productUpdate', {}).get('userErrors', [])
    return len(errors) == 0


def run_full_sync(category='all'):
    """
    æ™ºæ…§åŒæ­¥ï¼š
    1. çˆ¬ workman.jp å–å¾—æ‰€æœ‰å•†å“é€£çµ
    2. æ¯”å° Shopify ç¾æœ‰å•†å“
    3. æ–°å•†å“ â†’ ç¿»è­¯ + ä¸Šæ¶
    4. å·²å­˜åœ¨ + æœ‰è²¨ â†’ åªæ›´æ–°åƒ¹æ ¼ï¼Œåº«å­˜è¨­æœ‰è²¨
    5. å·²å­˜åœ¨ + ç¼ºè²¨ï¼ˆparse å›å‚³ Noneï¼‰â†’ åº«å­˜æ­¸é›¶ + è¨­è‰ç¨¿
    6. workman æ²’æœ‰ã€Shopify æœ‰ â†’ è¨­è‰ç¨¿
    """
    global scrape_status
    scrape_status = {"running": True, "phase": "cron_sync", "progress": 0, "total": 0, "current_product": "é–‹å§‹æ™ºæ…§åŒæ­¥...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "set_to_draft": 0}
    try:
        cats = ['work', 'mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not cats: raise Exception(f'æœªçŸ¥åˆ†é¡: {category}')
        
        # 1. å–å¾— Shopify ç¾æœ‰å•†å“ï¼ˆå« inventory è³‡æ–™ï¼‰
        scrape_status['current_product'] = 'å–å¾— Shopify ç¾æœ‰å•†å“...'
        existing_products = fetch_workman_products_with_source()
        existing_handles = {p['handle']: p for p in existing_products}
        print(f"[SYNC] Shopify ç¾æœ‰ {len(existing_handles)} å€‹ WORKMAN å•†å“")
        
        # 2. çˆ¬å– + æ¯”å°
        new_entries = []  # æ–°å•†å“ç”¨ Bulk Upload
        scraped_handles = set()
        updated_count = 0
        price_updated_count = 0
        
        for cat_key in cats:
            cat_info = CATEGORIES[cat_key]
            collection_id = get_or_create_collection(cat_info['collection'])
            if not collection_id: continue
            product_links = fetch_all_product_links(cat_key)
            if not product_links: continue
            scrape_status['total'] += len(product_links)
            
            for link in product_links:
                scrape_status['progress'] += 1
                code = link.split('/')[-2] if link.endswith('/') else link.split('/')[-1]
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {code}"
                
                # å¾ URL å–å¾— manage_code
                match = re.search(r'/g/g(\d+)/', link)
                manage_code = match.group(1) if match else ''
                my_handle = f"workman-{manage_code}" if manage_code else ''
                
                existing_info = existing_handles.get(my_handle) if my_handle else None
                
                if existing_info:
                    # ===== å·²å­˜åœ¨çš„å•†å“ï¼šåªæª¢æŸ¥åº«å­˜ + æ›´æ–°åƒ¹æ ¼ =====
                    scraped_handles.add(my_handle)
                    
                    # æª¢æŸ¥å®˜ç¶²åº«å­˜ï¼ˆç”¨ç°¡å–®çš„ HTTP GETï¼Œä¸éœ€è¦å®Œæ•´ parseï¼‰
                    stock = check_workman_stock(link)
                    
                    if stock['available']:
                        # æœ‰è²¨ â†’ åªæ›´æ–°åƒ¹æ ¼ + ç¢ºä¿ ACTIVE
                        try:
                            response = requests.get(link, headers=HEADERS, timeout=30)
                            if response.status_code == 200:
                                soup = BeautifulSoup(response.text, 'html.parser')
                                price_elem = soup.find('p', class_='block-goods-price')
                                if not price_elem:
                                    price_elem = soup.find(class_=re.compile(r'price'))
                                if price_elem:
                                    price_match = re.search(r'[\d,]+', price_elem.get_text(strip=True))
                                    if price_match:
                                        new_price = int(price_match.group().replace(',', ''))
                                        product_data_simple = {'price': new_price}
                                        update_existing_product_price(existing_info['id'], product_data_simple)
                                        price_updated_count += 1
                            
                            # ç¢ºä¿å•†å“æ˜¯ ACTIVEï¼ˆå¯èƒ½ä¹‹å‰è¢«è¨­ç‚ºè‰ç¨¿ï¼‰
                            if existing_info.get('status') == 'DRAFT':
                                set_product_active(existing_info['id'])
                                # é‡æ–°ç™¼å¸ƒ
                                publications = get_all_publication_ids()
                                if publications:
                                    pub_mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
                                    graphql_request(pub_mutation, {"id": existing_info['id'], "input": [{"publicationId": pid} for pid in publications]})
                            
                            updated_count += 1
                            print(f"[SYNC] âœ“ æ›´æ–°åƒ¹æ ¼: {existing_info['title'][:30]}")
                        except Exception as e:
                            scrape_status['errors'].append({'url': link, 'error': f'æ›´æ–°å¤±æ•—: {str(e)}'})
                    else:
                        # ç¼ºè²¨ â†’ åº«å­˜æ­¸é›¶ + è¨­è‰ç¨¿
                        print(f"[SYNC] âŒ ç¼ºè²¨ï¼Œä¸‹æ¶: {existing_info['title'][:30]} ({stock['out_of_stock_reason']})")
                        for variant in existing_info.get('variants', []):
                            for level in variant.get('inventory_levels', []):
                                if level['available'] > 0:
                                    zero_variant_inventory(variant['inventory_item_id'], level['location_id'])
                        if existing_info.get('status') != 'DRAFT':
                            set_product_to_draft(existing_info['id'])
                    
                    time.sleep(0.3)
                else:
                    # ===== æ–°å•†å“ï¼šå®Œæ•´çˆ¬å– + ç¿»è­¯ + åŠ å…¥ Bulk Upload =====
                    product_data = parse_product_page(link)
                    if not product_data: continue
                    
                    if manage_code:
                        scraped_handles.add(f"workman-{product_data['manage_code']}")
                    
                    try:
                        entry = product_to_jsonl_entry(product_data, cat_info['tags'], cat_key, collection_id)
                        new_entries.append(entry)
                        scrape_status['products'].append({'title': entry['productSet']['title'], 'handle': entry['productSet']['handle'], 'variants': len(entry['productSet'].get('variants', []))})
                        print(f"[SYNC] âœš æ–°å•†å“: {entry['productSet']['title'][:30]}")
                    except Exception as e:
                        scrape_status['errors'].append({'url': link, 'error': str(e)})
                    time.sleep(0.5)
        
        # 3. æ–°å•†å“æ‰¹é‡ä¸Šå‚³
        if new_entries:
            jsonl_path = os.path.join(JSONL_DIR, f"workman_{category}_{int(time.time())}.jsonl")
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for entry in new_entries:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            scrape_status['jsonl_file'] = jsonl_path
            
            scrape_status['phase'] = 'uploading'
            scrape_status['current_product'] = f'æ‰¹é‡ä¸Šå‚³ {len(new_entries)} å€‹æ–°å•†å“...'
            staged = create_staged_upload()
            if not staged: raise Exception('å»ºç«‹ Staged Upload å¤±æ•—')
            if not upload_jsonl_to_staged(staged, jsonl_path): raise Exception('ä¸Šå‚³ JSONL å¤±æ•—')
            staged_path = None
            for param in staged['parameters']:
                if param['name'] == 'key': staged_path = param['value']; break
            if not staged_path: staged_path = staged.get('resourceUrl', '')
            result = run_bulk_mutation(staged_path)
            if 'errors' in result: raise Exception(f'Bulk Mutation éŒ¯èª¤: {result["errors"]}')
            bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
            user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
            if user_errors: raise Exception(f'userErrors: {user_errors}')
            scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
            
            # ç­‰å¾…å®Œæˆ
            scrape_status['current_product'] = 'ç­‰å¾…ä¸Šå‚³å®Œæˆ...'
            max_wait, wait_time = 600, 0
            while wait_time < max_wait:
                status = check_bulk_operation_status()
                if status.get('status') == 'COMPLETED': break
                elif status.get('status') in ['FAILED', 'CANCELED']: raise Exception(f'å¤±æ•—: {status.get("status")}')
                time.sleep(5); wait_time += 5
            if wait_time >= max_wait: raise Exception('è¶…æ™‚')
            
            # ç™¼å¸ƒæ–°å•†å“
            scrape_status['phase'] = 'publishing'
            scrape_status['current_product'] = 'ç™¼å¸ƒæ–°å•†å“...'
            batch_publish_workman_products()
        
        # 4. ä¸‹æ¶ï¼šworkman æ²’æœ‰çš„å•†å“è¨­ç‚ºè‰ç¨¿
        scrape_status['phase'] = 'drafting'
        scrape_status['current_product'] = 'è™•ç†ä¸‹æ¶...'
        draft_count = 0
        for handle, product_info in existing_handles.items():
            if handle not in scraped_handles and product_info.get('status', '') == 'ACTIVE':
                print(f"[SYNC] ğŸ—‘ ä¸‹æ¶: {handle} - {product_info.get('title', '')[:30]}")
                # åº«å­˜æ­¸é›¶
                for variant in product_info.get('variants', []):
                    for level in variant.get('inventory_levels', []):
                        if level['available'] > 0:
                            zero_variant_inventory(variant['inventory_item_id'], level['location_id'])
                if set_product_to_draft(product_info['id']):
                    draft_count += 1
                time.sleep(0.2)
        
        scrape_status['set_to_draft'] = draft_count
        scrape_status['current_product'] = f"âœ… å®Œæˆï¼æ–°å•†å“ {len(new_entries)} å€‹ï¼Œæ›´æ–° {updated_count} å€‹ï¼Œä¸‹æ¶ {draft_count} å€‹"
        scrape_status['phase'] = 'completed'
        print(f"[SYNC] âœ… æ–°å•†å“: {len(new_entries)}, æ›´æ–°åƒ¹æ ¼: {price_updated_count}, ä¸‹æ¶: {draft_count}")
        return {'success': True, 'new_products': len(new_entries), 'updated': updated_count, 'set_to_draft': draft_count}
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        scrape_status['phase'] = 'error'
        return {'success': False, 'error': str(e)}
    finally:
        scrape_status['running'] = False

# ========== Flask è·¯ç”± ==========

@app.route('/')
def index():
    return open(os.path.join(os.path.dirname(__file__), 'templates', 'index.html'), 'r', encoding='utf-8').read()

@app.route('/api/status')
def api_status():
    return jsonify(scrape_status)

@app.route('/api/start')
def api_start():
    from flask import request
    category = request.args.get('category', 'mens')
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_scrape, args=(category,)).start()
    return jsonify({'started': True, 'category': category})

@app.route('/api/test_single')
def api_test_single():
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_test_single).start()
    return jsonify({'started': True, 'mode': 'test_single'})

@app.route('/api/test_result')
def api_test_result():
    return jsonify({'running': scrape_status.get('running'), 'phase': scrape_status.get('phase'), 'current_product': scrape_status.get('current_product'), 'errors': scrape_status.get('errors', []), 'test_result': scrape_status.get('test_result', {})})

@app.route('/api/cron')
def api_cron():
    from flask import request
    category = request.args.get('category', 'all')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    valid = ['work', 'mens', 'womens', 'kids', 'all']
    if category not in valid: return jsonify({'success': False, 'error': f'ç„¡æ•ˆåˆ†é¡: {category}'})
    threading.Thread(target=run_full_sync, args=(category,), daemon=False).start()
    return jsonify({'success': True, 'message': f'å·²é–‹å§‹åŒæ­¥: {category}', 'started_at': time.strftime('%Y-%m-%d %H:%M:%S')})

@app.route('/api/cron_sync')
def api_cron_sync():
    from flask import request
    category = request.args.get('category', 'all')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    return jsonify(run_full_sync(category))

@app.route('/api/upload')
def api_upload():
    from flask import request
    jsonl_file = request.args.get('file', '')
    if not jsonl_file or not os.path.exists(jsonl_file): return jsonify({'error': 'JSONL æª”æ¡ˆä¸å­˜åœ¨'})
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_bulk_upload, args=(jsonl_file,)).start()
    return jsonify({'started': True, 'file': jsonl_file})

@app.route('/api/bulk_status')
def api_bulk_status():
    op_id = scrape_status.get('bulk_operation_id', '')
    return jsonify(check_bulk_operation_status(op_id if op_id else None))

@app.route('/api/bulk_results')
def api_bulk_results():
    return jsonify(get_bulk_operation_results())

@app.route('/api/test')
def api_test():
    load_shopify_token()
    return jsonify(graphql_request("{ shop { name } }"))

@app.route('/api/delete')
def api_delete():
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_delete_workman_products).start()
    return jsonify({'started': True})

@app.route('/api/publish_all')
def api_publish_all():
    if scrape_status.get('running'): return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    def run_publish():
        global scrape_status
        scrape_status['running'] = True
        scrape_status['phase'] = 'publishing'
        try:
            results = batch_publish_workman_products()
            scrape_status['current_product'] = f"ç™¼å¸ƒå®Œæˆï¼æˆåŠŸ: {results.get('success', 0)}, å¤±æ•—: {results.get('failed', 0)}"
        except Exception as e:
            scrape_status['errors'].append({'error': str(e)})
        finally:
            scrape_status['running'] = False
    threading.Thread(target=run_publish, daemon=False).start()
    return jsonify({'success': True, 'message': 'å·²é–‹å§‹ç™¼å¸ƒ'})

@app.route('/api/publications')
def api_publications():
    try: return jsonify({'publications': get_all_publications()})
    except Exception as e: return jsonify({'error': str(e)})

@app.route('/api/count')
def api_count():
    try:
        load_shopify_token()
        result = graphql_request('{ productsCount(query: "vendor:WORKMAN") { count } }')
        return jsonify({'count': result.get('data', {}).get('productsCount', {}).get('count', 0)})
    except Exception as e: return jsonify({'error': str(e)})

@app.route('/api/test_workman')
def api_test_workman():
    results = {}
    try:
        r = requests.get(SOURCE_URL, headers=HEADERS, timeout=10)
        results['homepage'] = {'status': r.status_code, 'ok': r.status_code == 200}
    except Exception as e:
        results['homepage'] = {'error': str(e), 'ok': False}
    try:
        r = requests.get(SOURCE_URL + '/shop/c/c54/', headers=HEADERS, timeout=10)
        results['kids_page'] = {'status': r.status_code, 'ok': r.status_code == 200}
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            goods_links = [l for l in soup.find_all('a', href=True) if '/shop/g/' in l.get('href', '')]
            results['kids_page']['goods_links_found'] = len(goods_links)
            if goods_links: results['kids_page']['first_link'] = goods_links[0].get('href', '')
    except Exception as e:
        results['kids_page'] = {'error': str(e), 'ok': False}
    return jsonify(results)

@app.route('/api/test_product')
def api_test_product():
    from flask import request
    product_url = request.args.get('url', SOURCE_URL + '/shop/g/g2300022383210/')
    if not product_url.startswith('http'): product_url = SOURCE_URL + product_url
    results = {'url': product_url}
    try:
        r = requests.get(product_url, headers=HEADERS, timeout=15)
        results['status'] = r.status_code
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            te = soup.find('h1', class_='block-goods-name')
            results['title_found'] = te is not None
            if te: results['title'] = te.get_text(strip=True)[:50]
            pe = soup.find('p', class_='block-goods-price')
            results['price_elem_found'] = pe is not None
            if pe: results['price_text'] = pe.get_text(strip=True)
            cd = soup.find('dt', string='ç®¡ç†ç•ªå·')
            results['manage_code_dt_found'] = cd is not None
            if cd:
                dd = cd.find_next_sibling('dd')
                if dd: results['manage_code'] = dd.get_text(strip=True)
    except Exception as e:
        results['error'] = str(e)
    return jsonify(results)

# ========== åº«å­˜åŒæ­¥ API ==========

@app.route('/api/inventory_sync')
def api_inventory_sync():
    if inventory_sync_status.get('running'): return jsonify({'success': False, 'error': 'åº«å­˜åŒæ­¥æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_inventory_sync, daemon=False).start()
    return jsonify({'success': True, 'message': 'å·²é–‹å§‹åº«å­˜åŒæ­¥', 'started_at': time.strftime('%Y-%m-%d %H:%M:%S')})

@app.route('/api/inventory_sync_status')
def api_inventory_sync_status():
    return jsonify(inventory_sync_status)

@app.route('/api/check_stock')
def api_check_stock():
    from flask import request
    url = request.args.get('url', '')
    if not url: return jsonify({'error': 'è«‹æä¾› url åƒæ•¸'})
    return jsonify(check_workman_stock(url))


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
