"""
WORKMAN å•†å“çˆ¬èŸ² + Shopify Bulk Operations ä¸Šæ¶å·¥å…· v2.2
ä¾†æºï¼šworkman.jp
åŠŸèƒ½ï¼š
1. çˆ¬å– workman.jp å„åˆ†é¡å•†å“
2. ç¿»è­¯ä¸¦ç”¢ç”Ÿ JSONL æª”æ¡ˆ
3. ä½¿ç”¨ Shopify Bulk Operations API æ‰¹é‡ä¸Šå‚³
4. v2.2: ç¼ºè²¨/ä¸‹æ¶å•†å“ç›´æ¥åˆªé™¤ï¼ˆä¸è¨­è‰ç¨¿ï¼‰
"""

from flask import Flask, jsonify, send_file
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
import threading

app = Flask(__name__)

SHOPIFY_SHOP = ""
SHOPIFY_ACCESS_TOKEN = ""
SOURCE_URL = "https://workman.jp"
CATEGORIES = {
    'work': {'url': '/shop/c/c51/', 'collection': 'WORKMAN ä½œæ¥­æœ', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ä½œæ¥­æœ', 'å·¥ä½œæœ']},
    'mens': {'url': '/shop/c/c52/', 'collection': 'WORKMAN ç”·è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ç”·è£']},
    'womens': {'url': '/shop/c/c53/', 'collection': 'WORKMAN å¥³è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å¥³è£']},
    'kids': {'url': '/shop/c/c54/', 'collection': 'WORKMAN å…’ç«¥', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å…’ç«¥', 'ç«¥è£']}
}
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
DEFAULT_WEIGHT = 0.5
JSONL_DIR = "/tmp/workman_jsonl"
HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Accept-Language': 'ja,en;q=0.9'}

OUT_OF_STOCK_KEYWORDS = ['åº—èˆ—ã®ã¿ã®ãŠå–ã‚Šæ‰±ã„', 'ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚¹ãƒˆã‚¢è²©å£²çµ‚äº†', 'åº—èˆ—åœ¨åº«ã‚’ç¢ºèªã™ã‚‹', 'äºˆç´„å—ä»˜ã¯çµ‚äº†', 'å—ä»˜çµ‚äº†', 'å–ã‚Šæ‰±ã„ã‚’çµ‚äº†']

os.makedirs(JSONL_DIR, exist_ok=True)

scrape_status = {"running": False, "phase": "", "progress": 0, "total": 0, "current_product": "",
    "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}

inventory_sync_status = {"running": False, "phase": "", "progress": 0, "total": 0, "current_product": "",
    "results": {"checked": 0, "in_stock": 0, "out_of_stock": 0, "deleted": 0, "inventory_zeroed": 0, "errors": 0, "page_gone": 0},
    "details": [], "errors": []}

def reset_inventory_sync_status():
    global inventory_sync_status
    inventory_sync_status = {"running": False, "phase": "", "progress": 0, "total": 0, "current_product": "",
        "results": {"checked": 0, "in_stock": 0, "out_of_stock": 0, "deleted": 0, "inventory_zeroed": 0, "errors": 0, "page_gone": 0},
        "details": [], "errors": []}


def load_shopify_token():
    global SHOPIFY_SHOP, SHOPIFY_ACCESS_TOKEN
    if not SHOPIFY_SHOP: SHOPIFY_SHOP = os.environ.get("SHOPIFY_SHOP", "")
    if not SHOPIFY_ACCESS_TOKEN: SHOPIFY_ACCESS_TOKEN = os.environ.get("SHOPIFY_ACCESS_TOKEN", "")

def graphql_request(query, variables=None):
    load_shopify_token()
    url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/graphql.json"
    headers = {'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN, 'Content-Type': 'application/json'}
    payload = {'query': query}
    if variables: payload['variables'] = variables
    return requests.post(url, headers=headers, json=payload, timeout=60).json()

_collection_id_cache = {}

def get_or_create_collection(collection_name):
    global _collection_id_cache
    if collection_name in _collection_id_cache: return _collection_id_cache[collection_name]
    query = """query findCollection($title: String!) { collections(first: 1, query: $title) { edges { node { id title } } } }"""
    result = graphql_request(query, {"title": f"title:{collection_name}"})
    for edge in result.get('data', {}).get('collections', {}).get('edges', []):
        if edge['node']['title'] == collection_name:
            _collection_id_cache[collection_name] = edge['node']['id']; return edge['node']['id']
    mutation = """mutation createCollection($input: CollectionInput!) { collectionCreate(input: $input) { collection { id title } userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"title": collection_name, "descriptionHtml": f"<p>{collection_name} å•†å“ç³»åˆ—</p>"}})
    c = result.get('data', {}).get('collectionCreate', {}).get('collection')
    if c:
        _collection_id_cache[collection_name] = c['id']; publish_collection_to_all_channels(c['id']); return c['id']
    return None

def publish_collection_to_all_channels(collection_id):
    pids = get_all_publication_ids()
    if not pids: return
    mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { publishable { availablePublicationsCount { count } } userErrors { field message } } }"""
    graphql_request(mutation, {"id": collection_id, "input": [{"publicationId": p} for p in pids]})

def get_all_publication_ids():
    result = graphql_request('{ publications(first: 20) { edges { node { id name } } } }')
    return [e['node']['id'] for e in result.get('data', {}).get('publications', {}).get('edges', [])]

def calculate_selling_price(cost, weight):
    return int((cost + weight * 1250) / 0.7)

def contains_japanese(text):
    if not text: return False
    return bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF]', text))

def remove_japanese(text):
    if not text: return text
    return re.sub(r'\s+', ' ', re.sub(r'[\u3040-\u309F\u30A0-\u30FF]+', '', text)).strip()


# ========== ç¿»è­¯ ==========

def translate_with_chatgpt(title, description, size_spec=''):
    size_spec_section = f"\nå°ºå¯¸è¦æ ¼è¡¨ï¼š\n{size_spec}" if size_spec else ""
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬æœé£¾å“ç‰Œå•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼š{description[:1500] if description else ''}{size_spec_section}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{"title":"ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆå‰é¢åŠ ä¸Š WORKMANï¼‰","description":"ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆHTMLï¼Œç”¨<br>æ›è¡Œï¼‰","size_spec_translated":"ç¿»è­¯å¾Œçš„å°ºå¯¸è¦æ ¼ï¼ˆæ ¼å¼ï¼šåˆ—1|åˆ—2|åˆ—3ï¼Œæ¯è¡Œæ›è¡Œåˆ†éš”ï¼‰"}}

è¦å‰‡ï¼š1. ç¦æ—¥æ–‡ 2. é–‹é ­ã€ŒWORKMANã€3. å°ºå¯¸ï¼šã‚µã‚¤ã‚ºâ†’å°ºå¯¸ã€ç€ä¸ˆâ†’è¡£é•·ã€èº«å¹…â†’èº«å¯¬ã€è‚©å¹…â†’è‚©å¯¬ã€è¢–ä¸ˆâ†’è¢–é•· 4. å¿½ç•¥æ³¨æ„äº‹é …å’Œåƒ¹æ ¼ 5. åªå›å‚³JSON"""
    try:
        r = requests.post("https://api.openai.com/v1/chat/completions",
            headers={"Authorization": f"Bearer {OPENAI_API_KEY}", "Content-Type": "application/json"},
            json={"model": "gpt-4o-mini", "messages": [
                {"role": "system", "content": "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è¼¸å‡ºç¦æ­¢ä»»ä½•æ—¥æ–‡ã€‚"},
                {"role": "user", "content": prompt}], "temperature": 0, "max_tokens": 1500}, timeout=60)
        if r.status_code == 200:
            c = r.json()['choices'][0]['message']['content'].strip()
            if c.startswith('```'): c = c.split('\n', 1)[1]
            if c.endswith('```'): c = c.rsplit('```', 1)[0]
            t = json.loads(c.strip())
            tt = t.get('title', title); td = t.get('description', description); ts = t.get('size_spec_translated', '')
            if contains_japanese(tt): tt = remove_japanese(tt)
            if contains_japanese(td): td = remove_japanese(td)
            if not tt.startswith('WORKMAN'): tt = f"WORKMAN {tt}"
            sh = build_size_table_html(ts) if ts else ''
            if sh: td += '<br><br>' + sh
            return {'success': True, 'title': tt, 'description': td}
        return {'success': False, 'title': f"WORKMAN {title}", 'description': description}
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {'success': False, 'title': f"WORKMAN {title}", 'description': description}

def build_size_table_html(size_spec_text):
    if not size_spec_text: return ''
    lines = [l.strip() for l in size_spec_text.strip().split('\n') if l.strip()]
    if not lines: return ''
    html = '<div class="size-spec"><h3>ğŸ“ å°ºå¯¸è¦æ ¼</h3><table style="border-collapse:collapse;width:100%;margin:10px 0;">'
    for i, line in enumerate(lines):
        cells = [c.strip() for c in line.split('|')]
        if i == 0:
            html += '<tr style="background:#f5f5f5;">' + ''.join(f'<th style="border:1px solid #ddd;padding:8px;text-align:center;">{c}</th>' for c in cells) + '</tr>'
        else:
            html += '<tr>' + ''.join(f'<td style="border:1px solid #ddd;padding:8px;{"font-weight:bold;background:#fafafa;" if j==0 else "text-align:center;"}">{c}</td>' for j, c in enumerate(cells)) + '</tr>'
    html += '</table><p style="font-size:12px;color:#666;">â€» å°ºå¯¸å¯èƒ½æœ‰äº›è¨±èª¤å·®</p></div>'
    return html


# ========== çˆ¬å–å‡½æ•¸ ==========

def get_total_pages(category_url):
    url = SOURCE_URL + category_url
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            last_link = soup.find('a', string='æœ€å¾Œ')
            if last_link and last_link.get('href'):
                m = re.search(r'_p(\d+)', last_link['href'])
                if m: return int(m.group(1))
            pagination = soup.find_all('a', href=re.compile(r'_p\d+'))
            max_page = 1
            for link in pagination:
                m = re.search(r'_p(\d+)', link.get('href', ''))
                if m: max_page = max(max_page, int(m.group(1)))
            if max_page > 1: return max_page
            pager = soup.find('div', class_=re.compile(r'pager|pagination'))
            if pager:
                for link in pager.find_all('a'):
                    t = link.get_text(strip=True)
                    if t.isdigit(): max_page = max(max_page, int(t))
                return max_page
            return 1
    except Exception as e: print(f"[ERROR] å–å¾—ç¸½é æ•¸å¤±æ•—: {e}")
    return 1

def fetch_all_product_links(category_key):
    category = CATEGORIES[category_key]
    base_url = category['url']
    total_pages = get_total_pages(base_url)
    all_links = []
    for page in range(1, total_pages + 1):
        page_url = SOURCE_URL + base_url if page == 1 else SOURCE_URL + base_url.rstrip('/') + f'_p{page}/'
        try:
            r = requests.get(page_url, headers=HEADERS, timeout=30)
            if r.status_code == 200:
                soup = BeautifulSoup(r.text, 'html.parser')
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    if '/shop/g/' in href:
                        full_url = (SOURCE_URL + href if href.startswith('/') else href).split('?')[0]
                        if full_url not in all_links: all_links.append(full_url)
            elif r.status_code == 404: break
        except Exception as e: print(f"[ERROR] é é¢ {page}: {e}")
        time.sleep(0.5)
    print(f"[INFO] {category['collection']} å…± {len(all_links)} å€‹å•†å“")
    return all_links

def parse_product_page(url):
    try:
        r = requests.get(url, headers=HEADERS, timeout=30)
        if r.status_code != 200: return None
        soup = BeautifulSoup(r.text, 'html.parser')
        page_text = soup.get_text()
        for kw in OUT_OF_STOCK_KEYWORDS:
            if kw in page_text: return None
        if 'å£²ã‚Šåˆ‡ã‚Œ' in page_text or 'å“åˆ‡ã‚Œ' in page_text: return None
        if 'äºˆç´„å—ä»˜ã¯çµ‚äº†' in page_text or 'å—ä»˜çµ‚äº†' in page_text: return None

        title = ''
        te = soup.find('h1', class_='block-goods-name')
        if te: title = te.get_text(strip=True)
        elif soup.find('h1'): title = soup.find('h1').get_text(strip=True)

        price = 0
        pe = soup.find('p', class_='block-goods-price') or soup.find(class_=re.compile(r'price'))
        if pe:
            m = re.search(r'[\d,]+', pe.get_text(strip=True))
            if m: price = int(m.group().replace(',', ''))

        manage_code = ''
        cd = soup.find('dt', string='ç®¡ç†ç•ªå·')
        if cd:
            dd = cd.find_next_sibling('dd')
            if dd: manage_code = dd.get_text(strip=True)
        if not manage_code:
            m = re.search(r'/g/g(\d+)/', url)
            if m: manage_code = m.group(1)
        if not manage_code: return None
        if price == 0: price = 1500

        description = ''; size_spec = ''
        c1 = soup.find('dl', class_='block-goods-comment1')
        if c1:
            dd = c1.find('dd', class_='js-goods-tabContents')
            if dd:
                for tag in dd.find_all(['script', 'style']): tag.decompose()
                dc = [str(e) for e in dd.children if hasattr(e, 'name') and e.name in ['p', 'div'] and e.get_text(strip=True)]
                description = '\n'.join(dc)
        c2 = soup.find('dl', class_='block-goods-comment2')
        if c2:
            dd = c2.find('dd', class_='js-goods-tabContents')
            if dd:
                table = dd.find('table')
                if table:
                    for row in table.find_all('tr'):
                        size_spec += ' | '.join([c.get_text(strip=True) for c in row.find_all(['th', 'td'])]) + '\n'

        colors = []; images = []
        slider = soup.find('div', class_='js-goods-detail-goods-slider')
        if slider:
            for img in slider.find_all('img', class_='js-zoom'):
                src = img.get('src', '')
                if src:
                    fu = SOURCE_URL + src
                    if '_t1.' in src: images.insert(0, fu)
                    elif fu not in images: images.append(fu)
        gallery = soup.find('ul', class_='js-goods-detail-gallery-slider')
        if gallery:
            for item in gallery.find_all('li', class_='block-goods-gallery--color-variation-src'):
                ce = item.find('p', class_='block-goods-detail--color-variation-goods-color-name')
                if ce:
                    c = ce.get_text(strip=True)
                    if c and c not in colors: colors.append(c)
        if not colors: colors = ['æ¨™æº–']

        sizes = []
        sd = soup.find('dt', string='ã‚µã‚¤ã‚ºãƒ»ã‚¹ãƒšãƒƒã‚¯')
        if sd:
            sdd = sd.find_next_sibling('dd')
            if sdd:
                table = sdd.find('table')
                if table:
                    fr = table.find('tr')
                    if fr:
                        for th in fr.find_all('th')[1:]:
                            s = th.get_text(strip=True)
                            if s and s not in sizes: sizes.append(s)
        if not sizes: sizes = ['FREE']

        images = list(dict.fromkeys(images))[:10]
        if not images and manage_code: images.append(f"{SOURCE_URL}/img/goods/L/{manage_code}_t1.jpg")
        return {'url': url, 'title': title, 'price': price, 'manage_code': manage_code,
                'description': description, 'size_spec': size_spec, 'colors': colors, 'sizes': sizes, 'images': images}
    except Exception as e:
        print(f"[ERROR] è§£æå¤±æ•— {url}: {e}"); return None


# ========== JSONL ç”Ÿæˆ ==========

def product_to_jsonl_entry(product_data, tags, category_key, collection_id, existing_product_id=None):
    PRODUCT_TYPES = {'work': 'WORKMAN ä½œæ¥­æœ', 'mens': 'WORKMAN ç”·è£', 'womens': 'WORKMAN å¥³è£', 'kids': 'WORKMAN å…’ç«¥'}
    product_type = PRODUCT_TYPES.get(category_key, 'WORKMAN')
    translated = translate_with_chatgpt(product_data['title'], product_data['description'], product_data.get('size_spec', ''))
    title = translated['title']; description = translated['description']
    for pat in [r'<a[^>]*>.*?</a>', r'[^<>]*\d+[,ï¼Œ]?\d*\s*æ—¥åœ“[^<>]*', r'[^<>]*\d+[,ï¼Œ]?\d*\s*å††[^<>]*',
                r'[^<>]*\d+%\s*OFF[^<>]*', r'[^<>]*é™åƒ¹[^<>]*', r'[^<>]*å¤§å¹…[^<>]*',
                r'[^<>]*æ³¨æ„äº‹é …[^<>]*', r'[^<>]*è«‹æ³¨æ„[^<>]*', r'[^<>]*æ•¬è«‹è«’è§£[^<>]*',
                r'[^<>]*æ•¬è«‹è¦‹è«’[^<>]*', r'[^<>]*â€»[^<>]*']:
        description = re.sub(pat, '', description, flags=re.IGNORECASE)
    description = re.sub(r'<p>\s*</p>', '', description)
    description = re.sub(r'<br\s*/?>\s*<br\s*/?>', '<br>', description)
    description = re.sub(r'^\s*(<br\s*/?>)+', '', description)
    description = re.sub(r'(<br\s*/?>)+\s*$', '', description)
    description = re.sub(r'\n\s*\n', '\n', description).strip()
    description += "\n<br><br>\n<p><strong>ã€è«‹æ³¨æ„ä»¥ä¸‹äº‹é …ã€‘</strong></p>\n<p>â€»ä¸æ¥å—é€€æ›è²¨</p>\n<p>â€»é–‹ç®±è«‹å…¨ç¨‹éŒ„å½±</p>\n<p>â€»å› åº«å­˜æœ‰é™ï¼Œè¨‚è³¼æ™‚é–“ä¸åŒå¯èƒ½æœƒå‡ºç¾ç¼ºè²¨æƒ…æ³ã€‚</p>\n"

    mc = product_data['manage_code']; cost = product_data['price']
    colors = product_data['colors']; sizes = product_data['sizes']
    images = product_data['images']; source_url = product_data['url']
    selling_price = calculate_selling_price(cost, DEFAULT_WEIGHT)

    product_options = []
    has_color = len(colors) > 1 or (len(colors) == 1 and colors[0] != 'æ¨™æº–')
    has_size = len(sizes) > 1 or (len(sizes) == 1 and sizes[0] != 'FREE')
    if has_color: product_options.append({"name": "é¡è‰²", "values": [{"name": c} for c in colors]})
    if has_size: product_options.append({"name": "å°ºå¯¸", "values": [{"name": s} for s in sizes]})

    image_list = images[:10]; first_image = image_list[0] if image_list else None
    files = [{"originalSource": u, "contentType": "IMAGE"} for u in image_list]
    vf = {"originalSource": first_image, "contentType": "IMAGE"} if first_image else None

    variants = []
    if has_color and has_size:
        for c in colors:
            for s in sizes:
                v = {"price": selling_price, "sku": f"{mc}-{c}-{s}", "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}, "optionValues": [{"optionName": "é¡è‰²", "name": c}, {"optionName": "å°ºå¯¸", "name": s}]}
                if vf: v["file"] = vf
                variants.append(v)
    elif has_color:
        for c in colors:
            v = {"price": selling_price, "sku": f"{mc}-{c}", "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}, "optionValues": [{"optionName": "é¡è‰²", "name": c}]}
            if vf: v["file"] = vf
            variants.append(v)
    elif has_size:
        for s in sizes:
            v = {"price": selling_price, "sku": f"{mc}-{s}", "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}, "optionValues": [{"optionName": "å°ºå¯¸", "name": s}]}
            if vf: v["file"] = vf
            variants.append(v)
    else:
        v = {"price": selling_price, "sku": mc, "inventoryPolicy": "CONTINUE", "taxable": False, "inventoryItem": {"cost": cost}}
        if vf: v["file"] = vf
        variants.append(v)

    pi = {"title": title, "descriptionHtml": description, "vendor": "WORKMAN", "productType": product_type,
        "status": "ACTIVE", "handle": f"workman-{mc}", "tags": tags,
        "seo": {"title": f"{title} | WORKMAN æ—¥æœ¬ä»£è³¼", "description": f"æ—¥æœ¬ WORKMAN å®˜æ–¹æ­£å“ä»£è³¼ã€‚{title}ï¼Œå°ç£ç¾è²¨æˆ–æ—¥æœ¬ç›´é€ã€‚GOYOUTATI å¾¡ç”¨é”æ—¥æœ¬ä¼´æ‰‹ç¦®å°ˆé–€åº—ã€‚"},
        "metafields": [{"namespace": "custom", "key": "link", "value": source_url, "type": "url"}]}
    if existing_product_id: pi["id"] = existing_product_id
    if collection_id: pi["collections"] = [collection_id]
    if product_options: pi["productOptions"] = product_options
    if variants: pi["variants"] = variants
    if files: pi["files"] = files
    return {"productSet": pi, "synchronous": True}


# ========== Bulk Operations ==========

def create_staged_upload():
    query = """mutation stagedUploadsCreate($input: [StagedUploadInput!]!) { stagedUploadsCreate(input: $input) { stagedTargets { url resourceUrl parameters { name value } } userErrors { field message } } }"""
    result = graphql_request(query, {"input": [{"resource": "BULK_MUTATION_VARIABLES", "filename": "products.jsonl", "mimeType": "text/jsonl", "httpMethod": "POST"}]})
    if 'errors' in result: return None
    targets = result.get('data', {}).get('stagedUploadsCreate', {}).get('stagedTargets', [])
    return targets[0] if targets else None

def upload_jsonl_to_staged(staged_target, jsonl_path):
    params = {p['name']: p['value'] for p in staged_target['parameters']}
    with open(jsonl_path, 'rb') as f:
        r = requests.post(staged_target['url'], data=params, files={'file': ('products.jsonl', f, 'text/jsonl')}, timeout=300)
    return r.status_code in [200, 201, 204]

def run_bulk_mutation(staged_upload_path):
    query = """mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) { bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) { bulkOperation { id status } userErrors { field message } } }"""
    mutation = """mutation call($productSet: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $productSet) { product { id title } userErrors { field message } } }"""
    return graphql_request(query, {"mutation": mutation, "stagedUploadPath": staged_upload_path})

def check_bulk_operation_status(operation_id=None):
    if operation_id:
        query = """query($id: ID!) { node(id: $id) { ... on BulkOperation { id status errorCode createdAt completedAt objectCount fileSize url partialDataUrl } } }"""
        return graphql_request(query, {"id": operation_id}).get('data', {}).get('node', {})
    return graphql_request('{ currentBulkOperation(type: MUTATION) { id status errorCode createdAt completedAt objectCount fileSize url } }').get('data', {}).get('currentBulkOperation', {})

def get_bulk_operation_results():
    status = check_bulk_operation_status()
    results = {'status': status.get('status'), 'objectCount': status.get('objectCount'), 'errorCode': status.get('errorCode'), 'url': status.get('url')}
    if status.get('url'):
        try:
            r = requests.get(status['url'], timeout=30)
            if r.status_code == 200:
                lines = r.text.strip().split('\n')
                results['total_results'] = len(lines)
                errors, successes = [], []
                for line in lines[:50]:
                    try:
                        d = json.loads(line)
                        if 'data' in d and 'productSet' in d.get('data', {}):
                            ps = d['data']['productSet']
                            ue = ps.get('userErrors', [])
                            if ue: errors.append({'errors': ue})
                            elif ps.get('product'): successes.append({'id': ps['product'].get('id'), 'title': ps['product'].get('title', '')[:50]})
                    except: pass
                results.update({'errors': errors[:10], 'successes': successes[:10], 'error_count': len(errors), 'success_count': len(successes)})
        except Exception as e: results['fetch_error'] = str(e)
    return results


# ========== å•†å“ç®¡ç† ==========

def get_all_publications():
    result = graphql_request('{ publications(first: 20) { edges { node { id name catalog { title } } } } }')
    return [{'id': e['node'].get('id'), 'name': e['node'].get('name') or e['node'].get('catalog', {}).get('title', 'Unknown')} for e in result.get('data', {}).get('publications', {}).get('edges', [])]

def publish_product_to_all_channels(product_id):
    pubs = get_all_publications()
    if not pubs: return {'success': False}
    mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { publishable { availablePublicationsCount { count } } userErrors { field message } } }"""
    result = graphql_request(mutation, {"id": product_id, "input": [{"publicationId": p['id']} for p in pubs]})
    ue = result.get('data', {}).get('publishablePublish', {}).get('userErrors', [])
    return {'success': not ue, 'publications': len(pubs)}

def batch_publish_workman_products():
    products = fetch_workman_product_ids()
    if not products: return {'success': False}
    pubs = get_all_publications()
    if not pubs: return {'success': False}
    pi = [{"publicationId": p['id']} for p in pubs]
    results = {'total': len(products), 'success': 0, 'failed': 0}
    mutation = """mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }"""
    for p in products:
        r = graphql_request(mutation, {"id": p['id'], "input": pi})
        if r.get('data', {}).get('publishablePublish', {}).get('userErrors', []): results['failed'] += 1
        else: results['success'] += 1
        time.sleep(0.1)
    return results

def fetch_workman_product_ids():
    all_ids = []; cursor = None
    while True:
        if cursor:
            query = 'query($cursor: String) { products(first: 250, after: $cursor, query: "vendor:WORKMAN") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }'
            result = graphql_request(query, {"cursor": cursor})
        else:
            result = graphql_request('{ products(first: 250, query: "vendor:WORKMAN") { edges { node { id title handle status } cursor } pageInfo { hasNextPage } } }')
        for edge in result.get('data', {}).get('products', {}).get('edges', []):
            n = edge['node']
            all_ids.append({'id': n['id'], 'title': n['title'], 'handle': n['handle'], 'status': n.get('status', '')})
            cursor = edge['cursor']
        if not result.get('data', {}).get('products', {}).get('pageInfo', {}).get('hasNextPage', False): break
        time.sleep(0.5)
    return all_ids

def delete_product(product_id):
    """v2.2: åˆªé™¤å•†å“"""
    mutation = """mutation productDelete($input: ProductDeleteInput!) { productDelete(input: $input) { deletedProductId userErrors { field message } } }"""
    result = graphql_request(mutation, {"input": {"id": product_id}})
    errors = result.get('data', {}).get('productDelete', {}).get('userErrors', [])
    if errors:
        print(f"[åˆªé™¤å¤±æ•—] {product_id}: {errors}")
        return False
    print(f"[å·²åˆªé™¤] {product_id}")
    return True

def set_product_active(product_id):
    mutation = """mutation productUpdate($input: ProductInput!) { productUpdate(input: $input) { product { id status } userErrors { field message } } }"""
    return not graphql_request(mutation, {"input": {"id": product_id, "status": "ACTIVE"}}).get('data', {}).get('productUpdate', {}).get('userErrors', [])

def zero_variant_inventory(inventory_item_id, location_id):
    mutation = """mutation inventorySetQuantities($input: InventorySetQuantitiesInput!) { inventorySetQuantities(input: $input) { inventoryAdjustmentGroup { reason } userErrors { field message } } }"""
    return not graphql_request(mutation, {"input": {"reason": "correction", "name": "available", "quantities": [{"inventoryItemId": inventory_item_id, "locationId": location_id, "quantity": 0}]}}).get('data', {}).get('inventorySetQuantities', {}).get('userErrors', [])

def update_existing_product_price(product_id, product_data):
    cost = product_data['price']
    selling_price = calculate_selling_price(cost, DEFAULT_WEIGHT)
    result = graphql_request(f'{{ product(id: "{product_id}") {{ variants(first: 100) {{ edges {{ node {{ id sku }} }} }} }} }}')
    variants = result.get('data', {}).get('product', {}).get('variants', {}).get('edges', [])
    for v in variants:
        graphql_request("""mutation productVariantUpdate($input: ProductVariantInput!) { productVariantUpdate(input: $input) { productVariant { id } userErrors { field message } } }""",
            {"input": {"id": v['node']['id'], "price": str(selling_price)}})
        time.sleep(0.1)
    return len(variants)

def create_delete_jsonl(product_ids):
    jsonl_path = os.path.join(JSONL_DIR, f"delete_workman_{int(time.time())}.jsonl")
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for p in product_ids: f.write(json.dumps({"input": {"id": p['id']}}, ensure_ascii=False) + '\n')
    return jsonl_path

def run_bulk_delete_mutation(staged_upload_path):
    query = """mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) { bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) { bulkOperation { id status } userErrors { field message } } }"""
    mutation = """mutation call($input: ProductDeleteInput!) { productDelete(input: $input) { deletedProductId userErrors { field message } } }"""
    return graphql_request(query, {"mutation": mutation, "stagedUploadPath": staged_upload_path})

def run_delete_workman_products():
    global scrape_status
    scrape_status = {"running": True, "phase": "deleting", "progress": 0, "total": 0, "current_product": "æ­£åœ¨æŸ¥è©¢...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}
    try:
        pids = fetch_workman_product_ids()
        if not pids: scrape_status['current_product'] = 'æ²’æœ‰å•†å“'; scrape_status['running'] = False; return
        scrape_status['total'] = len(pids)
        jp = create_delete_jsonl(pids); scrape_status['jsonl_file'] = jp
        staged = create_staged_upload()
        if not staged: scrape_status['errors'].append({'error': 'Staged Upload å¤±æ•—'}); scrape_status['running'] = False; return
        if not upload_jsonl_to_staged(staged, jp): scrape_status['errors'].append({'error': 'JSONL ä¸Šå‚³å¤±æ•—'}); scrape_status['running'] = False; return
        sp = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), staged.get('resourceUrl', ''))
        result = run_bulk_delete_mutation(sp)
        ue = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        if ue: scrape_status['errors'].append({'error': str(ue)}); scrape_status['running'] = False; return
        bo = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        scrape_status['bulk_operation_id'] = bo.get('id', ''); scrape_status['bulk_status'] = bo.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡åˆªé™¤å·²å•Ÿå‹•ï¼æ­£åœ¨åˆªé™¤ {len(pids)} å€‹å•†å“..."
    except Exception as e: scrape_status['errors'].append({'error': str(e)})
    finally: scrape_status['running'] = False


# ========== åº«å­˜åŒæ­¥ ==========

def fetch_workman_products_with_source():
    all_products = []; cursor = None
    while True:
        ac = f', after: "{cursor}"' if cursor else ''
        query = f'{{ products(first: 50, query: "vendor:WORKMAN"{ac}) {{ edges {{ node {{ id title handle status metafield(namespace: "custom", key: "link") {{ value }} variants(first: 100) {{ edges {{ node {{ id sku inventoryItem {{ id inventoryLevels(first: 5) {{ edges {{ node {{ id quantities(names: ["available"]) {{ name quantity }} location {{ id }} }} }} }} }} }} }} }} }} cursor }} pageInfo {{ hasNextPage }} }} }}'
        result = graphql_request(query)
        for edge in result.get('data', {}).get('products', {}).get('edges', []):
            n = edge['node']
            su = n.get('metafield', {}).get('value', '') if n.get('metafield') else ''
            vs = []
            for ve in n.get('variants', {}).get('edges', []):
                vn = ve['node']; ii = vn.get('inventoryItem', {}); ils = ii.get('inventoryLevels', {}).get('edges', [])
                vi = {'id': vn['id'], 'sku': vn.get('sku', ''), 'inventory_item_id': ii.get('id', ''), 'inventory_levels': []}
                for le in ils:
                    ln = le['node']; av = 0
                    for q in ln.get('quantities', []):
                        if q['name'] == 'available': av = q['quantity']
                    vi['inventory_levels'].append({'id': ln['id'], 'location_id': ln.get('location', {}).get('id', ''), 'available': av})
                vs.append(vi)
            all_products.append({'id': n['id'], 'title': n['title'], 'handle': n['handle'], 'status': n['status'], 'source_url': su, 'variants': vs})
            cursor = edge['cursor']
        if not result.get('data', {}).get('products', {}).get('pageInfo', {}).get('hasNextPage', False): break
        time.sleep(0.5)
    return all_products

def check_workman_stock(product_url):
    result = {'available': True, 'page_exists': True, 'out_of_stock_reason': ''}
    if not product_url: return {'available': False, 'page_exists': False, 'out_of_stock_reason': 'ç„¡ä¾†æºé€£çµ'}
    try:
        r = requests.get(product_url, headers=HEADERS, timeout=30)
        if r.status_code == 404: return {'available': False, 'page_exists': False, 'out_of_stock_reason': 'é é¢å·²ä¸å­˜åœ¨ (404)'}
        if r.status_code != 200: return {'available': False, 'page_exists': False, 'out_of_stock_reason': f'HTTP {r.status_code}'}
        pt = BeautifulSoup(r.text, 'html.parser').get_text()
        for kw in OUT_OF_STOCK_KEYWORDS:
            if kw in pt: return {'available': False, 'page_exists': True, 'out_of_stock_reason': kw}
        if 'å£²ã‚Šåˆ‡ã‚Œ' in pt or 'å“åˆ‡ã‚Œ' in pt:
            return {'available': False, 'page_exists': True, 'out_of_stock_reason': 'å£²ã‚Šåˆ‡ã‚Œ / å“åˆ‡ã‚Œ'}
        if 'äºˆç´„å—ä»˜ã¯çµ‚äº†' in pt or 'å—ä»˜çµ‚äº†' in pt:
            return {'available': False, 'page_exists': True, 'out_of_stock_reason': 'äºˆç´„å—ä»˜çµ‚äº†'}
        return result
    except requests.exceptions.Timeout:
        return {'available': True, 'page_exists': True, 'out_of_stock_reason': 'é€£ç·šè¶…æ™‚ï¼ˆæš«ä¸è™•ç†ï¼‰'}
    except Exception as e:
        return {'available': True, 'page_exists': True, 'out_of_stock_reason': f'éŒ¯èª¤: {str(e)}ï¼ˆæš«ä¸è™•ç†ï¼‰'}

def run_inventory_sync():
    """v2.2: åº«å­˜åŒæ­¥ â€” ç¼ºè²¨å•†å“ç›´æ¥åˆªé™¤"""
    global inventory_sync_status
    reset_inventory_sync_status()
    inventory_sync_status['running'] = True; inventory_sync_status['phase'] = 'fetching'
    inventory_sync_status['current_product'] = 'æ­£åœ¨å–å¾— Shopify å•†å“æ¸…å–®...'
    try:
        products = fetch_workman_products_with_source()
        inventory_sync_status['total'] = len(products)
        if not products: inventory_sync_status['current_product'] = 'æ²’æœ‰æ‰¾åˆ°å•†å“'; inventory_sync_status['running'] = False; return
        inventory_sync_status['phase'] = 'checking'
        for idx, product in enumerate(products):
            inventory_sync_status['progress'] = idx + 1
            inventory_sync_status['current_product'] = f"[{idx+1}/{len(products)}] {product['title'][:30]}"
            if product['status'] == 'DRAFT':
                inventory_sync_status['results']['checked'] += 1; continue
            su = product['source_url']
            if not su:
                m = re.search(r'workman-(\d+)', product.get('handle', ''))
                if m: su = f"{SOURCE_URL}/shop/g/g{m.group(1)}/"
                else: inventory_sync_status['results']['checked'] += 1; inventory_sync_status['results']['errors'] += 1; continue
            stock = check_workman_stock(su)
            inventory_sync_status['results']['checked'] += 1
            if stock['available']:
                inventory_sync_status['results']['in_stock'] += 1
                inventory_sync_status['details'].append({'title': product['title'][:40], 'status': 'in_stock', 'source_url': su})
            else:
                inventory_sync_status['results']['out_of_stock'] += 1
                if not stock['page_exists']: inventory_sync_status['results']['page_gone'] += 1
                # v2.2: ç›´æ¥åˆªé™¤ï¼ˆä¸è¨­è‰ç¨¿ï¼‰
                if delete_product(product['id']):
                    inventory_sync_status['results']['deleted'] += 1
                inventory_sync_status['details'].append({'title': product['title'][:40], 'status': 'out_of_stock', 'reason': stock['out_of_stock_reason'], 'source_url': su})
            time.sleep(1)
        inventory_sync_status['phase'] = 'completed'
        r = inventory_sync_status['results']
        inventory_sync_status['current_product'] = f"âœ… å®Œæˆï¼æª¢æŸ¥:{r['checked']} æœ‰è²¨:{r['in_stock']} ç¼ºè²¨:{r['out_of_stock']} å·²åˆªé™¤:{r['deleted']}"
    except Exception as e:
        inventory_sync_status['errors'].append({'error': str(e)})
        inventory_sync_status['phase'] = 'error'
    finally:
        inventory_sync_status['running'] = False


# ========== ä¸»æµç¨‹ ==========

def run_test_single():
    global scrape_status
    scrape_status = {"running": True, "phase": "testing", "progress": 0, "total": 1, "current_product": "æ¸¬è©¦å–®å“...", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}
    try:
        cat_key = 'kids'; cat_info = CATEGORIES[cat_key]
        collection_id = get_or_create_collection(cat_info['collection'])
        if not collection_id: scrape_status['errors'].append({'error': 'ç„¡æ³•å»ºç«‹ Collection'}); return
        product_links = fetch_all_product_links(cat_key)
        if not product_links: scrape_status['errors'].append({'error': 'ç„¡æ³•å–å¾—å•†å“é€£çµ'}); return
        product_data = parse_product_page(product_links[0])
        if not product_data: scrape_status['errors'].append({'error': 'è§£æå•†å“å¤±æ•—'}); return
        entry = product_to_jsonl_entry(product_data, cat_info['tags'], cat_key, collection_id)
        pi = entry['productSet']
        scrape_status['products'].append({'title': pi['title'], 'handle': pi['handle'], 'variants': len(pi.get('variants', []))})
        mutation = """mutation productSet($input: ProductSetInput!, $synchronous: Boolean!) { productSet(synchronous: $synchronous, input: $input) { product { id title handle status productType seo { title description } variants(first: 10) { edges { node { id sku price taxable inventoryItem { unitCost { amount currencyCode } } } } } } userErrors { field code message } } }"""
        load_shopify_token()
        result = graphql_request(mutation, {"input": pi, "synchronous": True})
        ps = result.get('data', {}).get('productSet', {})
        ue = ps.get('userErrors', [])
        if ue: scrape_status['errors'].append({'error': '; '.join([e.get('message', '') for e in ue])})
        else:
            p = ps.get('product', {})
            pr = publish_product_to_all_channels(p.get('id', ''))
            scrape_status['current_product'] = f"âœ… æ¸¬è©¦æˆåŠŸï¼{p.get('title', '')}"
            scrape_status['test_result'] = {'id': p.get('id'), 'title': p.get('title'), 'handle': p.get('handle'), 'productType': p.get('productType', ''), 'seo': p.get('seo', {}), 'variants': p.get('variants', {}), 'published': pr.get('publications', 0)}
        scrape_status['progress'] = 1
    except Exception as e: scrape_status['errors'].append({'error': str(e)})
    finally: scrape_status['running'] = False

def run_scrape(category):
    global scrape_status
    scrape_status = {"running": True, "phase": "scraping", "progress": 0, "total": 0, "current_product": "", "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": ""}
    try:
        cats = ['work', 'mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not cats: scrape_status['errors'].append({'error': f'æœªçŸ¥åˆ†é¡: {category}'}); return
        all_entries = []
        for ck in cats:
            ci = CATEGORIES[ck]; cid = get_or_create_collection(ci['collection'])
            if not cid: continue
            links = fetch_all_product_links(ck)
            if not links: continue
            scrape_status['total'] += len(links)
            for link in links:
                scrape_status['progress'] += 1
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {link.split('/')[-2]}"
                pd = parse_product_page(link)
                if not pd: scrape_status['errors'].append({'url': link, 'error': 'è§£æå¤±æ•—'}); continue
                try:
                    entry = product_to_jsonl_entry(pd, ci['tags'], ck, cid)
                    all_entries.append(entry)
                    scrape_status['products'].append({'title': entry['productSet']['title'], 'handle': entry['productSet']['handle'], 'variants': len(entry['productSet'].get('variants', []))})
                except Exception as e: scrape_status['errors'].append({'url': link, 'error': str(e)})
                time.sleep(0.5)
        if all_entries:
            jp = os.path.join(JSONL_DIR, f"workman_{category}_{int(time.time())}.jsonl")
            with open(jp, 'w', encoding='utf-8') as f:
                for e in all_entries: f.write(json.dumps(e, ensure_ascii=False) + '\n')
            scrape_status['jsonl_file'] = jp
        scrape_status['current_product'] = f"å®Œæˆï¼å…± {len(all_entries)} å€‹å•†å“"
    except Exception as e: scrape_status['errors'].append({'error': str(e)})
    finally: scrape_status['running'] = False; scrape_status['phase'] = "completed"

def run_bulk_upload(jsonl_path):
    global scrape_status
    scrape_status['phase'] = 'uploading'; scrape_status['running'] = True
    try:
        staged = create_staged_upload()
        if not staged: scrape_status['errors'].append({'error': 'Staged Upload å¤±æ•—'}); return
        if not upload_jsonl_to_staged(staged, jsonl_path): scrape_status['errors'].append({'error': 'JSONL ä¸Šå‚³å¤±æ•—'}); return
        sp = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), staged.get('resourceUrl', ''))
        result = run_bulk_mutation(sp)
        ue = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        if ue: scrape_status['errors'].append({'error': str(ue)}); return
        bo = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        scrape_status['bulk_operation_id'] = bo.get('id', ''); scrape_status['bulk_status'] = bo.get('status', '')
    except Exception as e: scrape_status['errors'].append({'error': str(e)})
    finally: scrape_status['running'] = False


def run_full_sync(category='all'):
    """v2.2 æ™ºæ…§åŒæ­¥ï¼šæ–°å•†å“â†’ä¸Šæ¶ / å·²å­˜åœ¨+æœ‰è²¨â†’æ›´æ–°åƒ¹æ ¼ / ç¼ºè²¨/ä¸‹æ¶â†’åˆªé™¤"""
    global scrape_status
    scrape_status = {"running": True, "phase": "cron_sync", "progress": 0, "total": 0, "current_product": "é–‹å§‹æ™ºæ…§åŒæ­¥...",
        "products": [], "errors": [], "jsonl_file": "", "bulk_operation_id": "", "bulk_status": "", "deleted": 0}
    try:
        cats = ['work', 'mens', 'womens', 'kids'] if category == 'all' else [category] if category in CATEGORIES else []
        if not cats: raise Exception(f'æœªçŸ¥åˆ†é¡: {category}')

        scrape_status['current_product'] = 'å–å¾— Shopify ç¾æœ‰å•†å“...'
        existing_products = fetch_workman_products_with_source()
        existing_handles = {p['handle']: p for p in existing_products}

        new_entries = []; scraped_handles = set()
        updated_count = 0; price_updated_count = 0

        for ck in cats:
            ci = CATEGORIES[ck]; cid = get_or_create_collection(ci['collection'])
            if not cid: continue
            links = fetch_all_product_links(ck)
            if not links: continue
            scrape_status['total'] += len(links)

            for link in links:
                scrape_status['progress'] += 1
                code = link.split('/')[-2] if link.endswith('/') else link.split('/')[-1]
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {code}"
                m = re.search(r'/g/g(\d+)/', link)
                mc = m.group(1) if m else ''
                mh = f"workman-{mc}" if mc else ''
                ei = existing_handles.get(mh) if mh else None

                if ei:
                    scraped_handles.add(mh)
                    stock = check_workman_stock(link)
                    if stock['available']:
                        try:
                            r = requests.get(link, headers=HEADERS, timeout=30)
                            if r.status_code == 200:
                                soup = BeautifulSoup(r.text, 'html.parser')
                                pe = soup.find('p', class_='block-goods-price') or soup.find(class_=re.compile(r'price'))
                                if pe:
                                    pm = re.search(r'[\d,]+', pe.get_text(strip=True))
                                    if pm:
                                        update_existing_product_price(ei['id'], {'price': int(pm.group().replace(',', ''))})
                                        price_updated_count += 1
                            if ei.get('status') == 'DRAFT':
                                set_product_active(ei['id'])
                                pids = get_all_publication_ids()
                                if pids:
                                    graphql_request("""mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) { publishablePublish(id: $id, input: $input) { userErrors { field message } } }""",
                                        {"id": ei['id'], "input": [{"publicationId": p} for p in pids]})
                            updated_count += 1
                        except Exception as e: scrape_status['errors'].append({'url': link, 'error': f'æ›´æ–°å¤±æ•—: {str(e)}'})
                    else:
                        # v2.2: ç¼ºè²¨ â†’ ç›´æ¥åˆªé™¤
                        print(f"[SYNC] ğŸ—‘ ç¼ºè²¨åˆªé™¤: {ei['title'][:30]} ({stock['out_of_stock_reason']})")
                        if delete_product(ei['id']):
                            scrape_status['deleted'] = scrape_status.get('deleted', 0) + 1
                    time.sleep(0.3)
                else:
                    pd = parse_product_page(link)
                    if not pd: continue
                    if mc: scraped_handles.add(f"workman-{pd['manage_code']}")
                    try:
                        entry = product_to_jsonl_entry(pd, ci['tags'], ck, cid)
                        new_entries.append(entry)
                        scrape_status['products'].append({'title': entry['productSet']['title'], 'handle': entry['productSet']['handle'], 'variants': len(entry['productSet'].get('variants', []))})
                    except Exception as e: scrape_status['errors'].append({'url': link, 'error': str(e)})
                    time.sleep(0.5)

        # æ–°å•†å“æ‰¹é‡ä¸Šå‚³
        if new_entries:
            jp = os.path.join(JSONL_DIR, f"workman_{category}_{int(time.time())}.jsonl")
            with open(jp, 'w', encoding='utf-8') as f:
                for e in new_entries: f.write(json.dumps(e, ensure_ascii=False) + '\n')
            scrape_status['jsonl_file'] = jp; scrape_status['phase'] = 'uploading'
            scrape_status['current_product'] = f'æ‰¹é‡ä¸Šå‚³ {len(new_entries)} å€‹æ–°å•†å“...'
            staged = create_staged_upload()
            if not staged: raise Exception('Staged Upload å¤±æ•—')
            if not upload_jsonl_to_staged(staged, jp): raise Exception('JSONL ä¸Šå‚³å¤±æ•—')
            sp = next((p['value'] for p in staged['parameters'] if p['name'] == 'key'), staged.get('resourceUrl', ''))
            result = run_bulk_mutation(sp)
            if 'errors' in result: raise Exception(f'Bulk éŒ¯èª¤: {result["errors"]}')
            ue = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
            if ue: raise Exception(f'userErrors: {ue}')
            scrape_status['bulk_operation_id'] = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {}).get('id', '')
            scrape_status['current_product'] = 'ç­‰å¾…ä¸Šå‚³å®Œæˆ...'
            for _ in range(120):
                s = check_bulk_operation_status()
                if s.get('status') == 'COMPLETED': break
                elif s.get('status') in ['FAILED', 'CANCELED']: raise Exception(f'å¤±æ•—: {s.get("status")}')
                time.sleep(5)
            scrape_status['phase'] = 'publishing'; scrape_status['current_product'] = 'ç™¼å¸ƒæ–°å•†å“...'
            batch_publish_workman_products()

        # === v2.2: ä¸‹æ¶å•†å“ç›´æ¥åˆªé™¤ ===
        scrape_status['phase'] = 'deleting'
        scrape_status['current_product'] = 'æ¸…ç†ä¸‹æ¶å•†å“...'
        delete_count = scrape_status.get('deleted', 0)
        for handle, pi in existing_handles.items():
            if handle not in scraped_handles and pi.get('status', '') == 'ACTIVE':
                print(f"[SYNC] ğŸ—‘ åˆªé™¤: {handle} - {pi.get('title', '')[:30]}")
                scrape_status['current_product'] = f"åˆªé™¤: {pi.get('title', '')[:30]}"
                if delete_product(pi['id']):
                    delete_count += 1
                time.sleep(0.2)

        scrape_status['deleted'] = delete_count
        scrape_status['current_product'] = f"âœ… å®Œæˆï¼æ–°å•†å“ {len(new_entries)} å€‹ï¼Œæ›´æ–° {updated_count} å€‹ï¼Œåˆªé™¤ {delete_count} å€‹"
        scrape_status['phase'] = 'completed'
        return {'success': True, 'new_products': len(new_entries), 'updated': updated_count, 'deleted': delete_count}
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)}); scrape_status['phase'] = 'error'
        return {'success': False, 'error': str(e)}
    finally: scrape_status['running'] = False


# ========== Flask è·¯ç”± ==========

@app.route('/')
def index():
    return open(os.path.join(os.path.dirname(__file__), 'templates', 'index.html'), 'r', encoding='utf-8').read()

@app.route('/api/status')
def api_status():
    return jsonify(scrape_status)

@app.route('/api/start')
def api_start():
    from flask import request
    category = request.args.get('category', 'mens')
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_scrape, args=(category,)).start()
    return jsonify({'started': True, 'category': category})

@app.route('/api/test_single')
def api_test_single():
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_test_single).start()
    return jsonify({'started': True, 'mode': 'test_single'})

@app.route('/api/test_result')
def api_test_result():
    return jsonify({'running': scrape_status.get('running'), 'phase': scrape_status.get('phase'), 'current_product': scrape_status.get('current_product'), 'errors': scrape_status.get('errors', []), 'test_result': scrape_status.get('test_result', {})})

@app.route('/api/cron')
def api_cron():
    from flask import request
    category = request.args.get('category', 'all')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    valid = ['work', 'mens', 'womens', 'kids', 'all']
    if category not in valid: return jsonify({'success': False, 'error': f'ç„¡æ•ˆåˆ†é¡: {category}'})
    threading.Thread(target=run_full_sync, args=(category,), daemon=False).start()
    return jsonify({'success': True, 'message': f'å·²é–‹å§‹åŒæ­¥: {category}', 'started_at': time.strftime('%Y-%m-%d %H:%M:%S')})

@app.route('/api/cron_sync')
def api_cron_sync():
    from flask import request
    category = request.args.get('category', 'all')
    if scrape_status.get('running'): return jsonify({'success': False, 'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    return jsonify(run_full_sync(category))

@app.route('/api/upload')
def api_upload():
    from flask import request
    jf = request.args.get('file', '')
    if not jf or not os.path.exists(jf): return jsonify({'error': 'JSONL æª”æ¡ˆä¸å­˜åœ¨'})
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_bulk_upload, args=(jf,)).start()
    return jsonify({'started': True, 'file': jf})

@app.route('/api/bulk_status')
def api_bulk_status():
    return jsonify(check_bulk_operation_status(scrape_status.get('bulk_operation_id') or None))

@app.route('/api/bulk_results')
def api_bulk_results():
    return jsonify(get_bulk_operation_results())

@app.route('/api/test')
def api_test():
    load_shopify_token()
    return jsonify(graphql_request("{ shop { name } }"))

@app.route('/api/delete')
def api_delete():
    if scrape_status['running']: return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_delete_workman_products).start()
    return jsonify({'started': True})

@app.route('/api/publish_all')
def api_publish_all():
    if scrape_status.get('running'): return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    def run_publish():
        global scrape_status
        scrape_status['running'] = True; scrape_status['phase'] = 'publishing'
        try:
            r = batch_publish_workman_products()
            scrape_status['current_product'] = f"ç™¼å¸ƒå®Œæˆï¼æˆåŠŸ: {r.get('success', 0)}, å¤±æ•—: {r.get('failed', 0)}"
        except Exception as e: scrape_status['errors'].append({'error': str(e)})
        finally: scrape_status['running'] = False
    threading.Thread(target=run_publish, daemon=False).start()
    return jsonify({'success': True, 'message': 'å·²é–‹å§‹ç™¼å¸ƒ'})

@app.route('/api/publications')
def api_publications():
    try: return jsonify({'publications': get_all_publications()})
    except Exception as e: return jsonify({'error': str(e)})

@app.route('/api/count')
def api_count():
    try:
        load_shopify_token()
        return jsonify({'count': graphql_request('{ productsCount(query: "vendor:WORKMAN") { count } }').get('data', {}).get('productsCount', {}).get('count', 0)})
    except Exception as e: return jsonify({'error': str(e)})

@app.route('/api/test_workman')
def api_test_workman():
    results = {}
    try:
        r = requests.get(SOURCE_URL, headers=HEADERS, timeout=10)
        results['homepage'] = {'status': r.status_code, 'ok': r.status_code == 200}
    except Exception as e: results['homepage'] = {'error': str(e), 'ok': False}
    try:
        r = requests.get(SOURCE_URL + '/shop/c/c54/', headers=HEADERS, timeout=10)
        results['kids_page'] = {'status': r.status_code, 'ok': r.status_code == 200}
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            gl = [l for l in soup.find_all('a', href=True) if '/shop/g/' in l.get('href', '')]
            results['kids_page']['goods_links_found'] = len(gl)
            if gl: results['kids_page']['first_link'] = gl[0].get('href', '')
    except Exception as e: results['kids_page'] = {'error': str(e), 'ok': False}
    return jsonify(results)

@app.route('/api/test_product')
def api_test_product():
    from flask import request
    pu = request.args.get('url', SOURCE_URL + '/shop/g/g2300022383210/')
    if not pu.startswith('http'): pu = SOURCE_URL + pu
    results = {'url': pu}
    try:
        r = requests.get(pu, headers=HEADERS, timeout=15)
        results['status'] = r.status_code
        if r.status_code == 200:
            soup = BeautifulSoup(r.text, 'html.parser')
            te = soup.find('h1', class_='block-goods-name')
            results['title_found'] = te is not None
            if te: results['title'] = te.get_text(strip=True)[:50]
            pe = soup.find('p', class_='block-goods-price')
            results['price_elem_found'] = pe is not None
            if pe: results['price_text'] = pe.get_text(strip=True)
            cd = soup.find('dt', string='ç®¡ç†ç•ªå·')
            results['manage_code_dt_found'] = cd is not None
            if cd:
                dd = cd.find_next_sibling('dd')
                if dd: results['manage_code'] = dd.get_text(strip=True)
    except Exception as e: results['error'] = str(e)
    return jsonify(results)

@app.route('/api/inventory_sync')
def api_inventory_sync():
    if inventory_sync_status.get('running'): return jsonify({'success': False, 'error': 'åº«å­˜åŒæ­¥æ­£åœ¨åŸ·è¡Œä¸­'})
    threading.Thread(target=run_inventory_sync, daemon=False).start()
    return jsonify({'success': True, 'message': 'å·²é–‹å§‹åº«å­˜åŒæ­¥', 'started_at': time.strftime('%Y-%m-%d %H:%M:%S')})

@app.route('/api/inventory_sync_status')
def api_inventory_sync_status():
    return jsonify(inventory_sync_status)

@app.route('/api/check_stock')
def api_check_stock():
    from flask import request
    url = request.args.get('url', '')
    if not url: return jsonify({'error': 'è«‹æä¾› url åƒæ•¸'})
    return jsonify(check_workman_stock(url))


if __name__ == '__main__':
    print("WORKMAN çˆ¬èŸ²å·¥å…· v2.2")
    app.run(host='0.0.0.0', port=8080, debug=True)
