"""
WORKMAN å•†å“çˆ¬èŸ² + Shopify Bulk Operations ä¸Šæ¶å·¥å…·
ä¾†æºï¼šworkman.jp
åŠŸèƒ½ï¼š
1. çˆ¬å– workman.jp å„åˆ†é¡å•†å“
2. ç¿»è­¯ä¸¦ç”¢ç”Ÿ JSONL æª”æ¡ˆ
3. ä½¿ç”¨ Shopify Bulk Operations API æ‰¹é‡ä¸Šå‚³ï¼ˆæ•¸åƒå•†å“/åˆ†é˜ï¼‰
"""

from flask import Flask, jsonify, send_file
import requests
from bs4 import BeautifulSoup
import re
import json
import os
import time
import threading

app = Flask(__name__)

# ========== è¨­å®š ==========
SHOPIFY_SHOP = ""
SHOPIFY_ACCESS_TOKEN = ""

SOURCE_URL = "https://workman.jp"
CATEGORIES = {
    'work': {'url': '/shop/c/c51/', 'collection': 'WORKMAN ä½œæ¥­æœ', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ä½œæ¥­æœ', 'å·¥ä½œæœ']},
    'mens': {'url': '/shop/c/c52/', 'collection': 'WORKMAN ç”·è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'ç”·è£']},
    'womens': {'url': '/shop/c/c53/', 'collection': 'WORKMAN å¥³è£', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å¥³è£']},
    'kids': {'url': '/shop/c/c54/', 'collection': 'WORKMAN å…’ç«¥', 'tags': ['WORKMAN', 'æ—¥æœ¬', 'æœé£¾', 'å…’ç«¥', 'ç«¥è£']}
}

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "")
DEFAULT_WEIGHT = 0.5
JSONL_DIR = "/tmp/workman_jsonl"

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja,en;q=0.9',
}

# ç¢ºä¿ç›®éŒ„å­˜åœ¨
os.makedirs(JSONL_DIR, exist_ok=True)

scrape_status = {
    "running": False,
    "phase": "",  # "scraping" | "uploading"
    "progress": 0,
    "total": 0,
    "current_product": "",
    "products": [],
    "errors": [],
    "jsonl_file": "",
    "bulk_operation_id": "",
    "bulk_status": "",
}

# ========== å·¥å…·å‡½æ•¸ ==========

def load_shopify_token():
    global SHOPIFY_SHOP, SHOPIFY_ACCESS_TOKEN
    if not SHOPIFY_SHOP:
        SHOPIFY_SHOP = os.environ.get("SHOPIFY_SHOP", "")
    if not SHOPIFY_ACCESS_TOKEN:
        SHOPIFY_ACCESS_TOKEN = os.environ.get("SHOPIFY_ACCESS_TOKEN", "")


def graphql_request(query, variables=None):
    """åŸ·è¡Œ GraphQL è«‹æ±‚"""
    load_shopify_token()
    url = f"https://{SHOPIFY_SHOP}.myshopify.com/admin/api/2024-01/graphql.json"
    headers = {
        'X-Shopify-Access-Token': SHOPIFY_ACCESS_TOKEN,
        'Content-Type': 'application/json',
    }
    payload = {'query': query}
    if variables:
        payload['variables'] = variables
    
    response = requests.post(url, headers=headers, json=payload, timeout=60)
    return response.json()


# å¿«å– Collection ID
_collection_id_cache = {}


def get_or_create_collection(collection_name):
    """å–å¾—æˆ–å»ºç«‹ Collectionï¼Œå›å‚³ Collection ID"""
    global _collection_id_cache
    
    # æª¢æŸ¥å¿«å–
    if collection_name in _collection_id_cache:
        return _collection_id_cache[collection_name]
    
    # å…ˆæŸ¥è©¢æ˜¯å¦å­˜åœ¨
    query = """
    query findCollection($title: String!) {
      collections(first: 1, query: $title) {
        edges {
          node {
            id
            title
          }
        }
      }
    }
    """
    result = graphql_request(query, {"title": f"title:{collection_name}"})
    edges = result.get('data', {}).get('collections', {}).get('edges', [])
    
    for edge in edges:
        if edge['node']['title'] == collection_name:
            collection_id = edge['node']['id']
            _collection_id_cache[collection_name] = collection_id
            print(f"[Collection] æ‰¾åˆ°: {collection_name} -> {collection_id}")
            return collection_id
    
    # ä¸å­˜åœ¨ï¼Œå»ºç«‹æ–°çš„
    mutation = """
    mutation createCollection($input: CollectionInput!) {
      collectionCreate(input: $input) {
        collection {
          id
          title
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    result = graphql_request(mutation, {
        "input": {
            "title": collection_name,
            "descriptionHtml": f"<p>{collection_name} å•†å“ç³»åˆ—</p>"
        }
    })
    
    collection = result.get('data', {}).get('collectionCreate', {}).get('collection')
    if collection:
        collection_id = collection['id']
        _collection_id_cache[collection_name] = collection_id
        print(f"[Collection] å»ºç«‹: {collection_name} -> {collection_id}")
        
        # ç™¼å¸ƒ Collection åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“
        publish_collection_to_all_channels(collection_id)
        
        return collection_id
    
    errors = result.get('data', {}).get('collectionCreate', {}).get('userErrors', [])
    print(f"[Collection] å»ºç«‹å¤±æ•—: {collection_name}, éŒ¯èª¤: {errors}")
    return None


def publish_collection_to_all_channels(collection_id):
    """ç™¼å¸ƒ Collection åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“"""
    publication_ids = get_all_publication_ids()
    
    if not publication_ids:
        print(f"[Collection] æ²’æœ‰æ‰¾åˆ°ä»»ä½•éŠ·å”®ç®¡é“")
        return
    
    publication_inputs = [{"publicationId": pub_id} for pub_id in publication_ids]
    
    mutation = """
    mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
      publishablePublish(id: $id, input: $input) {
        publishable {
          availablePublicationsCount {
            count
          }
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    result = graphql_request(mutation, {"id": collection_id, "input": publication_inputs})
    
    user_errors = result.get('data', {}).get('publishablePublish', {}).get('userErrors', [])
    if user_errors:
        print(f"[Collection] ç™¼å¸ƒå¤±æ•—: {user_errors}")
    else:
        count = result.get('data', {}).get('publishablePublish', {}).get('publishable', {}).get('availablePublicationsCount', {}).get('count', 0)
        print(f"[Collection] å·²ç™¼å¸ƒåˆ° {count} å€‹éŠ·å”®ç®¡é“")


def get_all_publication_ids():
    """å–å¾—æ‰€æœ‰ Publication IDï¼ˆç”¨æ–¼ç™¼å¸ƒå•†å“ï¼‰"""
    query = """
    {
      publications(first: 20) {
        edges {
          node {
            id
            name
          }
        }
      }
    }
    """
    result = graphql_request(query)
    
    publication_ids = []
    edges = result.get('data', {}).get('publications', {}).get('edges', [])
    for edge in edges:
        publication_ids.append(edge['node']['id'])
    
    return publication_ids


def calculate_selling_price(cost, weight):
    """è¨ˆç®—å”®åƒ¹"""
    shipping_cost = 800 + (weight * 400)
    base_price = cost + shipping_cost
    selling_price = base_price * 1.15
    return round(selling_price / 10) * 10


def contains_japanese(text):
    if not text:
        return False
    hiragana = re.search(r'[\u3040-\u309F]', text)
    katakana = re.search(r'[\u30A0-\u30FF]', text)
    return bool(hiragana or katakana)


def remove_japanese(text):
    if not text:
        return text
    cleaned = re.sub(r'[\u3040-\u309F\u30A0-\u30FF]+', '', text)
    cleaned = re.sub(r'\s+', ' ', cleaned).strip()
    return cleaned


# ========== ç¿»è­¯ ==========

def translate_with_chatgpt(title, description, size_spec=''):
    """ç¿»è­¯å•†å“è³‡è¨Š"""
    size_spec_section = f"\nå°ºå¯¸è¦æ ¼è¡¨ï¼š\n{size_spec}" if size_spec else ""
    
    prompt = f"""ä½ æ˜¯å°ˆæ¥­çš„æ—¥æœ¬å•†å“ç¿»è­¯å’Œ SEO å°ˆå®¶ã€‚è«‹å°‡ä»¥ä¸‹æ—¥æœ¬æœé£¾å“ç‰Œå•†å“è³‡è¨Šç¿»è­¯æˆç¹é«”ä¸­æ–‡ã€‚

å•†å“åç¨±ï¼ˆæ—¥æ–‡ï¼‰ï¼š{title}
å•†å“èªªæ˜ï¼š{description[:1500] if description else ''}{size_spec_section}

è«‹å›å‚³ JSON æ ¼å¼ï¼ˆä¸è¦åŠ  markdown æ¨™è¨˜ï¼‰ï¼š
{{
    "title": "ç¿»è­¯å¾Œçš„å•†å“åç¨±ï¼ˆç¹é«”ä¸­æ–‡ï¼Œå‰é¢åŠ ä¸Š WORKMANï¼‰",
    "description": "ç¿»è­¯å¾Œçš„å•†å“èªªæ˜ï¼ˆHTMLæ ¼å¼ï¼Œç”¨<br>æ›è¡Œï¼‰",
    "size_spec_translated": "ç¿»è­¯å¾Œçš„å°ºå¯¸è¦æ ¼ï¼ˆæ ¼å¼ï¼šåˆ—1|åˆ—2|åˆ—3ï¼Œæ¯è¡Œæ›è¡Œåˆ†éš”ï¼‰"
}}

è¦å‰‡ï¼š
1. çµ•å°ç¦æ­¢æ—¥æ–‡ï¼ˆå¹³å‡åã€ç‰‡å‡åï¼‰
2. å•†å“åç¨±é–‹é ­å¿…é ˆæ˜¯ã€ŒWORKMANã€
3. å°ºå¯¸æ¬„ä½ç¿»è­¯ï¼šã‚µã‚¤ã‚ºâ†’å°ºå¯¸ã€ç€ä¸ˆâ†’è¡£é•·ã€èº«å¹…â†’èº«å¯¬ã€è‚©å¹…â†’è‚©å¯¬ã€è¢–ä¸ˆâ†’è¢–é•·
4. å®Œå…¨å¿½ç•¥æ³¨æ„äº‹é …ï¼ˆã”æ³¨æ„ã€æ³¨æ„äº‹é …ã€ã”äº†æ‰¿ã€â€»è¨˜è™Ÿé–‹é ­çš„è­¦å‘Šæ–‡å­—ç­‰ï¼‰ï¼Œä¸è¦ç¿»è­¯é€™äº›å…§å®¹
5. å®Œå…¨å¿½ç•¥åƒ¹æ ¼ç›¸é—œå…§å®¹ï¼ˆå††ã€æ—¥åœ“ã€OFFã€å‰²å¼•ã€å€¤ä¸‹ã’ç­‰ï¼‰
6. åªå›å‚³ JSON"""

    try:
        response = requests.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            },
            json={
                "model": "gpt-4o-mini",
                "messages": [
                    {"role": "system", "content": "ä½ æ˜¯ç¿»è­¯å°ˆå®¶ã€‚è¼¸å‡ºç¦æ­¢ä»»ä½•æ—¥æ–‡ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                "temperature": 0,
                "max_tokens": 1500
            },
            timeout=60
        )
        
        if response.status_code == 200:
            content = response.json()['choices'][0]['message']['content']
            content = content.strip()
            if content.startswith('```'):
                content = content.split('\n', 1)[1]
            if content.endswith('```'):
                content = content.rsplit('```', 1)[0]
            
            translated = json.loads(content.strip())
            
            trans_title = translated.get('title', title)
            trans_desc = translated.get('description', description)
            trans_size = translated.get('size_spec_translated', '')
            
            # ç§»é™¤æ—¥æ–‡
            if contains_japanese(trans_title):
                trans_title = remove_japanese(trans_title)
            if contains_japanese(trans_desc):
                trans_desc = remove_japanese(trans_desc)
            
            if not trans_title.startswith('WORKMAN'):
                trans_title = f"WORKMAN {trans_title}"
            
            # å»ºç«‹å°ºå¯¸è¡¨ HTML
            size_html = build_size_table_html(trans_size) if trans_size else ''
            if size_html:
                trans_desc += '<br><br>' + size_html
            
            return {'success': True, 'title': trans_title, 'description': trans_desc}
        else:
            return {'success': False, 'title': f"WORKMAN {title}", 'description': description}
            
    except Exception as e:
        print(f"[ç¿»è­¯éŒ¯èª¤] {e}")
        return {'success': False, 'title': f"WORKMAN {title}", 'description': description}


def build_size_table_html(size_spec_text):
    """å»ºç«‹å°ºå¯¸è¡¨ HTML"""
    if not size_spec_text:
        return ''
    
    lines = [line.strip() for line in size_spec_text.strip().split('\n') if line.strip()]
    if not lines:
        return ''
    
    html = '<div class="size-spec"><h3>ğŸ“ å°ºå¯¸è¦æ ¼</h3>'
    html += '<table style="border-collapse:collapse;width:100%;margin:10px 0;">'
    
    for i, line in enumerate(lines):
        cells = [c.strip() for c in line.split('|')]
        if i == 0:
            html += '<tr style="background:#f5f5f5;">'
            for cell in cells:
                html += f'<th style="border:1px solid #ddd;padding:8px;text-align:center;">{cell}</th>'
            html += '</tr>'
        else:
            html += '<tr>'
            for j, cell in enumerate(cells):
                style = 'border:1px solid #ddd;padding:8px;'
                if j == 0:
                    style += 'font-weight:bold;background:#fafafa;'
                else:
                    style += 'text-align:center;'
                html += f'<td style="{style}">{cell}</td>'
            html += '</tr>'
    
    html += '</table><p style="font-size:12px;color:#666;">â€» å°ºå¯¸å¯èƒ½æœ‰äº›è¨±èª¤å·®</p></div>'
    return html


# ========== çˆ¬å–å‡½æ•¸ ==========

def get_total_pages(category_url):
    """å–å¾—åˆ†é¡ç¸½é æ•¸"""
    url = SOURCE_URL + category_url
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            last_link = soup.find('a', string='æœ€å¾Œ')
            if last_link and last_link.get('href'):
                match = re.search(r'_p(\d+)', last_link['href'])
                if match:
                    return int(match.group(1))
            return 1
    except:
        pass
    return 1


def fetch_all_product_links(category_key):
    """å–å¾—åˆ†é¡å…§æ‰€æœ‰å•†å“é€£çµ"""
    category = CATEGORIES[category_key]
    base_url = category['url']
    total_pages = get_total_pages(base_url)
    
    print(f"[INFO] {category['collection']} å…± {total_pages} é ")
    
    all_links = []
    for page in range(1, total_pages + 1):
        if page == 1:
            page_url = SOURCE_URL + base_url
        else:
            page_url = SOURCE_URL + base_url.replace('/', f'_p{page}/', 1)
        
        try:
            response = requests.get(page_url, headers=HEADERS, timeout=30)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # æ‰¾æ‰€æœ‰é€£çµï¼Œç¯©é¸å‡ºå•†å“é€£çµ (/shop/g/)
                for link in soup.find_all('a', href=True):
                    href = link.get('href', '')
                    if '/shop/g/' in href:
                        full_url = SOURCE_URL + href if href.startswith('/') else href
                        if full_url not in all_links:
                            all_links.append(full_url)
                            
                print(f"[INFO] ç¬¬ {page} é æ‰¾åˆ° {len(all_links)} å€‹å•†å“é€£çµ")
        except Exception as e:
            print(f"[ERROR] é é¢ {page} è¼‰å…¥å¤±æ•—: {e}")
        
        time.sleep(0.3)
    
    print(f"[INFO] {category['collection']} å…± {len(all_links)} å€‹å•†å“")
    return all_links


def parse_product_page(url):
    """è§£æå•†å“é é¢"""
    try:
        response = requests.get(url, headers=HEADERS, timeout=30)
        if response.status_code != 200:
            print(f"[è§£æå¤±æ•—] {url} - HTTP {response.status_code}")
            return None
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # æ¨™é¡Œ - å˜—è©¦å¤šç¨®æ–¹å¼
        title = ''
        title_elem = soup.find('h1', class_='block-goods-name')
        if title_elem:
            title = title_elem.get_text(strip=True)
        else:
            # å‚™ç”¨ï¼šæ‰¾ä»»ä½• h1
            title_elem = soup.find('h1')
            if title_elem:
                title = title_elem.get_text(strip=True)
        
        print(f"[è§£æ] æ¨™é¡Œ: {title[:30] if title else '(ç„¡)'}")
        
        # åƒ¹æ ¼ - å˜—è©¦å¤šç¨®æ–¹å¼
        price = 0
        price_elem = soup.find('p', class_='block-goods-price')
        if not price_elem:
            # å‚™ç”¨ï¼šæ‰¾ class åŒ…å« price çš„å…ƒç´ 
            price_elem = soup.find(class_=re.compile(r'price'))
        if price_elem:
            price_text = price_elem.get_text(strip=True)
            match = re.search(r'[\d,]+', price_text)
            if match:
                price = int(match.group().replace(',', ''))
        
        print(f"[è§£æ] åƒ¹æ ¼: Â¥{price}")
        
        # ç®¡ç†ç•ªè™Ÿ - å˜—è©¦å¤šç¨®æ–¹å¼
        manage_code = ''
        code_dt = soup.find('dt', string='ç®¡ç†ç•ªå·')
        if code_dt:
            code_dd = code_dt.find_next_sibling('dd')
            if code_dd:
                manage_code = code_dd.get_text(strip=True)
        
        if not manage_code:
            # å‚™ç”¨ï¼šå¾ URL å–å¾—
            match = re.search(r'/g/g(\d+)/', url)
            if match:
                manage_code = match.group(1)
        
        print(f"[è§£æ] ç®¡ç†ç•ªè™Ÿ: {manage_code if manage_code else '(ç„¡)'}")
        
        # æ”¾å¯¬æ¢ä»¶ï¼šåªè¦æœ‰ manage_code å°±ç¹¼çºŒï¼ˆä¸å†è¦æ±‚ price >= 1000ï¼‰
        if not manage_code:
            print(f"[è§£æå¤±æ•—] {url} - ç„¡æ³•å–å¾—ç®¡ç†ç•ªè™Ÿ")
            return None
        
        # å¦‚æœåƒ¹æ ¼ç‚º 0ï¼Œè¨­å®šé è¨­å€¼
        if price == 0:
            price = 1500  # é è¨­åƒ¹æ ¼
            print(f"[è§£æ] åƒ¹æ ¼ç‚º 0ï¼Œä½¿ç”¨é è¨­å€¼ Â¥{price}")
        
        # å•†å“èªªæ˜
        description = ''
        size_spec = ''
        
        comment1 = soup.find('dl', class_='block-goods-comment1')
        if comment1:
            desc_dd = comment1.find('dd', class_='js-goods-tabContents')
            if desc_dd:
                for tag in desc_dd.find_all(['script', 'style']):
                    tag.decompose()
                desc_content = []
                for elem in desc_dd.children:
                    if hasattr(elem, 'name') and elem.name in ['p', 'div']:
                        text = elem.get_text(strip=True)
                        if text:
                            desc_content.append(str(elem))
                description = '\n'.join(desc_content)
        
        comment2 = soup.find('dl', class_='block-goods-comment2')
        if comment2:
            spec_dd = comment2.find('dd', class_='js-goods-tabContents')
            if spec_dd:
                table = spec_dd.find('table')
                if table:
                    rows = table.find_all('tr')
                    for row in rows:
                        cells = row.find_all(['th', 'td'])
                        row_text = ' | '.join([c.get_text(strip=True) for c in cells])
                        size_spec += row_text + '\n'
        
        # é¡è‰²å’Œåœ–ç‰‡
        colors = []
        images = []
        
        slider = soup.find('div', class_='js-goods-detail-goods-slider')
        if slider:
            for img in slider.find_all('img', class_='js-zoom'):
                img_src = img.get('src', '')
                if img_src:
                    full_url = SOURCE_URL + img_src
                    if '_t1.' in img_src:
                        images.insert(0, full_url)
                    elif full_url not in images:
                        images.append(full_url)
        
        gallery = soup.find('ul', class_='js-goods-detail-gallery-slider')
        if gallery:
            for item in gallery.find_all('li', class_='block-goods-gallery--color-variation-src'):
                color_elem = item.find('p', class_='block-goods-detail--color-variation-goods-color-name')
                if color_elem:
                    color = color_elem.get_text(strip=True)
                    if color and color not in colors:
                        colors.append(color)
        
        if not colors:
            colors = ['æ¨™æº–']
        
        # å°ºå¯¸
        sizes = []
        size_dt = soup.find('dt', string='ã‚µã‚¤ã‚ºãƒ»ã‚¹ãƒšãƒƒã‚¯')
        if size_dt:
            size_dd = size_dt.find_next_sibling('dd')
            if size_dd:
                table = size_dd.find('table')
                if table:
                    first_row = table.find('tr')
                    if first_row:
                        for th in first_row.find_all('th')[1:]:
                            size = th.get_text(strip=True)
                            if size and size not in sizes:
                                sizes.append(size)
        
        if not sizes:
            sizes = ['FREE']
        
        images = list(dict.fromkeys(images))[:10]
        
        if not images and manage_code:
            images.append(f"{SOURCE_URL}/img/goods/L/{manage_code}_t1.jpg")
        
        return {
            'url': url,
            'title': title,
            'price': price,
            'manage_code': manage_code,
            'description': description,
            'size_spec': size_spec,
            'colors': colors,
            'sizes': sizes,
            'images': images
        }
        
    except Exception as e:
        print(f"[ERROR] è§£æå¤±æ•— {url}: {e}")
        return None


def product_to_jsonl_entry(product_data, tags, category_key, collection_id):
    """å°‡å•†å“è³‡æ–™è½‰æ›ç‚º JSONL æ ¼å¼ï¼ˆShopify GraphQL ProductSetInputï¼‰"""
    
    # æ ¹æ“šåˆ†é¡è¨­å®šå•†å“é¡å‹
    PRODUCT_TYPES = {
        'work': 'WORKMAN ä½œæ¥­æœ',
        'mens': 'WORKMAN ç”·è£',
        'womens': 'WORKMAN å¥³è£',
        'kids': 'WORKMAN å…’ç«¥'
    }
    product_type = PRODUCT_TYPES.get(category_key, 'WORKMAN')
    
    # ç¿»è­¯
    translated = translate_with_chatgpt(
        product_data['title'],
        product_data['description'],
        product_data.get('size_spec', '')
    )
    
    title = translated['title']
    description = translated['description']
    
    import re
    import html
    
    # ç§»é™¤èªªæ˜æ–‡ä¸­çš„è¶…é€£çµï¼ˆåŒ…å« <a> æ¨™ç±¤å’Œå…¶ä¸­çš„æ–‡å­—ï¼‰
    description = re.sub(r'<a[^>]*>.*?</a>', '', description)
    
    # ç§»é™¤åƒ¹æ ¼ç›¸é—œçš„å¥å­ï¼ˆåŒ…å«ã€Œæ—¥åœ“ã€ã€Œå††ã€ã€ŒOFFã€ã€Œé™åƒ¹ã€ç­‰ï¼‰
    description = re.sub(r'[^<>]*\d+[,ï¼Œ]?\d*\s*æ—¥åœ“[^<>]*', '', description)
    description = re.sub(r'[^<>]*\d+[,ï¼Œ]?\d*\s*å††[^<>]*', '', description)
    description = re.sub(r'[^<>]*\d+%\s*OFF[^<>]*', '', description, flags=re.IGNORECASE)
    description = re.sub(r'[^<>]*é™åƒ¹[^<>]*', '', description)
    description = re.sub(r'[^<>]*å¤§å¹…[^<>]*', '', description)
    
    # ç§»é™¤æ³¨æ„äº‹é …ç›¸é—œå…§å®¹ï¼ˆç¿»è­¯å¾Œå¯èƒ½æ®˜ç•™çš„ï¼‰
    description = re.sub(r'[^<>]*æ³¨æ„äº‹é …[^<>]*', '', description)
    description = re.sub(r'[^<>]*è«‹æ³¨æ„[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ•¬è«‹è«’è§£[^<>]*', '', description)
    description = re.sub(r'[^<>]*æ•¬è«‹è¦‹è«’[^<>]*', '', description)
    description = re.sub(r'[^<>]*â€»[^<>]*', '', description)  # ç§»é™¤ â€» é–‹é ­çš„è­¦å‘Šæ–‡å­—
    
    # å¾¹åº•æ¸…ç†ç©ºç™½å’Œç©ºæ¨™ç±¤
    description = re.sub(r'<p>\s*</p>', '', description)  # ç§»é™¤ç©ºçš„ <p> æ¨™ç±¤
    description = re.sub(r'<br\s*/?>\s*<br\s*/?>', '<br>', description)  # é€£çºŒ br è®Šå–®ä¸€
    description = re.sub(r'^\s*(<br\s*/?>)+', '', description)  # ç§»é™¤é–‹é ­çš„ br
    description = re.sub(r'(<br\s*/?>)+\s*$', '', description)  # ç§»é™¤çµå°¾çš„ br
    description = re.sub(r'\n\s*\n', '\n', description)  # ç§»é™¤é€£çºŒç©ºè¡Œ
    description = description.strip()
    
    # åŠ å…¥çµ±ä¸€æ³¨æ„äº‹é …
    notice = """
<br><br>
<p><strong>ã€è«‹æ³¨æ„ä»¥ä¸‹äº‹é …ã€‘</strong></p>
<p>â€»ä¸æ¥å—é€€æ›è²¨</p>
<p>â€»é–‹ç®±è«‹å…¨ç¨‹éŒ„å½±</p>
<p>â€»å› åº«å­˜æœ‰é™ï¼Œè¨‚è³¼æ™‚é–“ä¸åŒå¯èƒ½æœƒå‡ºç¾ç¼ºè²¨æƒ…æ³ã€‚</p>
"""
    description = description + notice
    
    manage_code = product_data['manage_code']
    cost = product_data['price']  # æ—¥åœ“æˆæœ¬
    colors = product_data['colors']
    sizes = product_data['sizes']
    images = product_data['images']
    source_url = product_data['url']
    
    selling_price = calculate_selling_price(cost, DEFAULT_WEIGHT)
    
    # å»ºç«‹ productOptionsï¼ˆProductSetInput æ ¼å¼ï¼‰
    product_options = []
    has_color_option = len(colors) > 1 or (len(colors) == 1 and colors[0] != 'æ¨™æº–')
    has_size_option = len(sizes) > 1 or (len(sizes) == 1 and sizes[0] != 'FREE')
    
    if has_color_option:
        product_options.append({
            "name": "é¡è‰²",
            "values": [{"name": c} for c in colors]
        })
    
    if has_size_option:
        product_options.append({
            "name": "å°ºå¯¸",
            "values": [{"name": s} for s in sizes]
        })
    
    # æº–å‚™åœ–ç‰‡ï¼ˆåªå–å‰10å¼µï¼‰
    image_list = images[:10] if images else []
    first_image = image_list[0] if image_list else None
    
    # å»ºç«‹ files é™£åˆ—
    files = []
    if image_list:
        for img_url in image_list:
            files.append({
                "originalSource": img_url,
                "contentType": "IMAGE"
            })
    
    # å»ºç«‹ variant çš„ file ç‰©ä»¶ï¼ˆå¿…é ˆè·Ÿ files é™£åˆ—ä¸­çš„ç›¸åŒï¼‰
    variant_file = None
    if first_image:
        variant_file = {
            "originalSource": first_image,
            "contentType": "IMAGE"
        }
    
    # å»ºç«‹ variantsï¼ˆProductSetInput æ ¼å¼ï¼‰
    # åŠ å…¥ costï¼ˆæˆæœ¬ï¼‰å’Œ taxable: false
    variants = []
    
    if has_color_option and has_size_option:
        # é¡è‰² Ã— å°ºå¯¸
        for color in colors:
            for size in sizes:
                variant = {
                    "price": selling_price,
                    "sku": f"{manage_code}-{color}-{size}",
                    "inventoryPolicy": "CONTINUE",
                    "taxable": False,
                    "inventoryItem": {
                        "cost": cost  # æ—¥åœ“æˆæœ¬
                    },
                    "optionValues": [
                        {"optionName": "é¡è‰²", "name": color},
                        {"optionName": "å°ºå¯¸", "name": size}
                    ]
                }
                if variant_file:
                    variant["file"] = variant_file
                variants.append(variant)
    elif has_color_option:
        for color in colors:
            variant = {
                "price": selling_price,
                "sku": f"{manage_code}-{color}",
                "inventoryPolicy": "CONTINUE",
                "taxable": False,
                "inventoryItem": {
                    "cost": cost
                },
                "optionValues": [
                    {"optionName": "é¡è‰²", "name": color}
                ]
            }
            if variant_file:
                variant["file"] = variant_file
            variants.append(variant)
    elif has_size_option:
        for size in sizes:
            variant = {
                "price": selling_price,
                "sku": f"{manage_code}-{size}",
                "inventoryPolicy": "CONTINUE",
                "taxable": False,
                "inventoryItem": {
                    "cost": cost
                },
                "optionValues": [
                    {"optionName": "å°ºå¯¸", "name": size}
                ]
            }
            if variant_file:
                variant["file"] = variant_file
            variants.append(variant)
    else:
        # æ²’æœ‰é¸é …
        variant = {
            "price": selling_price,
            "sku": manage_code,
            "inventoryPolicy": "CONTINUE",
            "taxable": False,
            "inventoryItem": {
                "cost": cost
            }
        }
        if variant_file:
            variant["file"] = variant_file
        variants.append(variant)
    
    # å»ºç«‹ SEO è³‡è¨Šï¼ˆç¨ç«‹æ’°å¯«ï¼Œä¸ä½¿ç”¨èªªæ˜æ–‡ï¼‰
    seo_title = f"{title} | WORKMAN æ—¥æœ¬ä»£è³¼"
    seo_description = f"æ—¥æœ¬ WORKMAN å®˜æ–¹æ­£å“ä»£è³¼ã€‚{title}ï¼Œå°ç£ç¾è²¨æˆ–æ—¥æœ¬ç›´é€ï¼Œå“è³ªä¿è­‰ã€‚GOYOUTATI å¾¡ç”¨é”æ—¥æœ¬ä¼´æ‰‹ç¦®å°ˆé–€åº—ã€‚"
    
    # ProductSetInput çµæ§‹
    product_input = {
        "title": title,
        "descriptionHtml": description,
        "vendor": "WORKMAN",
        "productType": product_type,
        "status": "ACTIVE",
        "handle": f"workman-{manage_code}",
        "tags": tags,
        # SEO è³‡è¨Šï¼ˆç¨ç«‹æ’°å¯«ï¼‰
        "seo": {
            "title": seo_title,
            "description": seo_description
        },
        # ä¸­ç¹¼æ¬„ä½ - ä¾†æºé€£çµ
        "metafields": [
            {
                "namespace": "custom",
                "key": "link",
                "value": source_url,
                "type": "url"
            }
        ]
    }
    
    # åŠ å…¥ Collectionï¼ˆä½¿ç”¨ IDï¼‰
    if collection_id:
        product_input["collections"] = [collection_id]
    
    # åŠ å…¥é¸é …
    if product_options:
        product_input["productOptions"] = product_options
    
    # åŠ å…¥ variants
    if variants:
        product_input["variants"] = variants
    
    # åŠ å…¥åœ–ç‰‡ï¼ˆä½¿ç”¨ filesï¼‰
    if files:
        product_input["files"] = files
    
    # è®Šæ•¸åç¨±æ˜¯ productSetï¼ˆä¸æ˜¯ inputï¼‰
    return {
        "productSet": product_input,
        "synchronous": True
    }


# ========== Bulk Operations ==========

def create_staged_upload():
    """å»ºç«‹ Staged Upload URL"""
    query = """
    mutation stagedUploadsCreate($input: [StagedUploadInput!]!) {
      stagedUploadsCreate(input: $input) {
        stagedTargets {
          url
          resourceUrl
          parameters {
            name
            value
          }
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "input": [{
            "resource": "BULK_MUTATION_VARIABLES",
            "filename": "products.jsonl",
            "mimeType": "text/jsonl",
            "httpMethod": "POST"
        }]
    }
    
    result = graphql_request(query, variables)
    
    if 'errors' in result:
        print(f"[Staged Upload Error] {result['errors']}")
        return None
    
    targets = result.get('data', {}).get('stagedUploadsCreate', {}).get('stagedTargets', [])
    if targets:
        return targets[0]
    return None


def upload_jsonl_to_staged(staged_target, jsonl_path):
    """ä¸Šå‚³ JSONL åˆ° Staged URL"""
    url = staged_target['url']
    params = {p['name']: p['value'] for p in staged_target['parameters']}
    
    with open(jsonl_path, 'rb') as f:
        files = {'file': ('products.jsonl', f, 'text/jsonl')}
        response = requests.post(url, data=params, files=files, timeout=300)
    
    return response.status_code in [200, 201, 204]


def run_bulk_mutation(staged_upload_path):
    """åŸ·è¡Œ Bulk Mutation"""
    query = """
    mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) {
      bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) {
        bulkOperation {
          id
          status
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    # ä½¿ç”¨ productSet mutationï¼ˆ2024 å¹´å¾Œçš„æ–° API æ ¼å¼ï¼‰
    # è®Šæ•¸åç¨±å¿…é ˆæ˜¯ $productSetï¼ˆä¸æ˜¯ $inputï¼‰
    mutation = """
    mutation call($productSet: ProductSetInput!, $synchronous: Boolean!) {
      productSet(synchronous: $synchronous, input: $productSet) {
        product {
          id
          title
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "mutation": mutation,
        "stagedUploadPath": staged_upload_path
    }
    
    result = graphql_request(query, variables)
    return result


def check_bulk_operation_status(operation_id=None):
    """æª¢æŸ¥ Bulk Operation ç‹€æ…‹"""
    if operation_id:
        query = """
        query($id: ID!) {
          node(id: $id) {
            ... on BulkOperation {
              id
              status
              errorCode
              createdAt
              completedAt
              objectCount
              fileSize
              url
              partialDataUrl
            }
          }
        }
        """
        result = graphql_request(query, {"id": operation_id})
        return result.get('data', {}).get('node', {})
    else:
        # å–å¾—æœ€æ–°çš„ bulk operation
        query = """
        {
          currentBulkOperation(type: MUTATION) {
            id
            status
            errorCode
            createdAt
            completedAt
            objectCount
            fileSize
            url
          }
        }
        """
        result = graphql_request(query)
        return result.get('data', {}).get('currentBulkOperation', {})


def get_bulk_operation_results():
    """å–å¾— Bulk Operation çš„è©³ç´°çµæœ"""
    # å…ˆå–å¾—æœ€æ–°çš„ bulk operation
    status = check_bulk_operation_status()
    
    results = {
        'status': status.get('status'),
        'objectCount': status.get('objectCount'),
        'errorCode': status.get('errorCode'),
        'url': status.get('url'),
    }
    
    # å¦‚æœæœ‰çµæœ URLï¼Œä¸‹è¼‰çµæœ
    if status.get('url'):
        try:
            response = requests.get(status['url'], timeout=30)
            if response.status_code == 200:
                # çµæœæ˜¯ JSONL æ ¼å¼
                lines = response.text.strip().split('\n')
                results['total_results'] = len(lines)
                results['sample_results'] = []
                
                errors = []
                successes = []
                
                for line in lines[:50]:  # åªæª¢æŸ¥å‰ 50 è¡Œ
                    try:
                        data = json.loads(line)
                        
                        # æª¢æŸ¥ productSet çµæœ
                        if 'data' in data and 'productSet' in data.get('data', {}):
                            product_set = data['data']['productSet']
                            user_errors = product_set.get('userErrors', [])
                            
                            if user_errors:
                                errors.append({
                                    'errors': user_errors,
                                    'input': data.get('__parentId', '')
                                })
                            elif product_set.get('product'):
                                successes.append({
                                    'id': product_set['product'].get('id'),
                                    'title': product_set['product'].get('title', '')[:50]
                                })
                        # ç›¸å®¹èˆŠçš„ productCreate æ ¼å¼
                        elif 'data' in data and 'productCreate' in data.get('data', {}):
                            product_create = data['data']['productCreate']
                            user_errors = product_create.get('userErrors', [])
                            
                            if user_errors:
                                errors.append({
                                    'errors': user_errors,
                                    'input': data.get('__parentId', '')
                                })
                            elif product_create.get('product'):
                                successes.append({
                                    'id': product_create['product'].get('id'),
                                    'title': product_create['product'].get('title', '')[:50]
                                })
                        # æª¢æŸ¥æ˜¯å¦æœ‰éŒ¯èª¤
                        elif 'errors' in data:
                            errors.append({
                                'errors': data['errors'],
                                'input': ''
                            })
                        
                        results['sample_results'].append(data)
                    except:
                        pass
                
                results['errors'] = errors[:10]
                results['successes'] = successes[:10]
                results['error_count'] = len(errors)
                results['success_count'] = len(successes)
        except Exception as e:
            results['fetch_error'] = str(e)
    
    return results


# ========== æ‰¹é‡ç™¼å¸ƒåˆ°éŠ·å”®ç®¡é“ ==========

def get_all_publications():
    """å–å¾—æ‰€æœ‰éŠ·å”®ç®¡é“ï¼ˆPublicationsï¼‰"""
    query = """
    {
      publications(first: 20) {
        edges {
          node {
            id
            name
            catalog {
              title
            }
          }
        }
      }
    }
    """
    result = graphql_request(query)
    
    publications = []
    edges = result.get('data', {}).get('publications', {}).get('edges', [])
    for edge in edges:
        node = edge.get('node', {})
        publications.append({
            'id': node.get('id'),
            'name': node.get('name') or node.get('catalog', {}).get('title', 'Unknown')
        })
    
    return publications


def publish_product_to_all_channels(product_id):
    """ç™¼å¸ƒå•†å“åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“"""
    publications = get_all_publications()
    
    if not publications:
        return {'success': False, 'error': 'No publications found'}
    
    # å»ºç«‹ input é™£åˆ—
    publication_inputs = [{"publicationId": pub['id']} for pub in publications]
    
    mutation = """
    mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
      publishablePublish(id: $id, input: $input) {
        publishable {
          availablePublicationsCount {
            count
          }
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    result = graphql_request(mutation, {"id": product_id, "input": publication_inputs})
    
    user_errors = result.get('data', {}).get('publishablePublish', {}).get('userErrors', [])
    if user_errors:
        return {'success': False, 'errors': user_errors}
    
    return {'success': True, 'publications': len(publications)}


def batch_publish_workman_products():
    """æ‰¹é‡ç™¼å¸ƒæ‰€æœ‰ WORKMAN å•†å“åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“"""
    # å–å¾—æ‰€æœ‰ WORKMAN å•†å“
    product_ids = fetch_workman_product_ids()
    
    if not product_ids:
        return {'success': False, 'error': 'No WORKMAN products found'}
    
    # å–å¾—æ‰€æœ‰éŠ·å”®ç®¡é“
    publications = get_all_publications()
    
    if not publications:
        return {'success': False, 'error': 'No publications found'}
    
    publication_inputs = [{"publicationId": pub['id']} for pub in publications]
    
    results = {
        'total': len(product_ids),
        'success': 0,
        'failed': 0,
        'errors': []
    }
    
    for product_id in product_ids:
        mutation = """
        mutation publishablePublish($id: ID!, $input: [PublicationInput!]!) {
          publishablePublish(id: $id, input: $input) {
            userErrors {
              field
              message
            }
          }
        }
        """
        
        result = graphql_request(mutation, {"id": product_id, "input": publication_inputs})
        
        user_errors = result.get('data', {}).get('publishablePublish', {}).get('userErrors', [])
        if user_errors:
            results['failed'] += 1
            results['errors'].append({'id': product_id, 'errors': user_errors})
        else:
            results['success'] += 1
        
        time.sleep(0.1)  # é¿å… rate limit
    
    return results


# ========== æ‰¹é‡åˆªé™¤åŠŸèƒ½ ==========

def fetch_workman_product_ids():
    """å–å¾—æ‰€æœ‰ WORKMAN å•†å“çš„ IDï¼ˆä½¿ç”¨åˆ†é æŸ¥è©¢ï¼‰"""
    all_ids = []
    cursor = None
    
    while True:
        if cursor:
            query = """
            query($cursor: String) {
              products(first: 250, after: $cursor, query: "vendor:WORKMAN") {
                edges {
                  node {
                    id
                    title
                    handle
                  }
                  cursor
                }
                pageInfo {
                  hasNextPage
                }
              }
            }
            """
            result = graphql_request(query, {"cursor": cursor})
        else:
            query = """
            {
              products(first: 250, query: "vendor:WORKMAN") {
                edges {
                  node {
                    id
                    title
                    handle
                  }
                  cursor
                }
                pageInfo {
                  hasNextPage
                }
              }
            }
            """
            result = graphql_request(query)
        
        products = result.get('data', {}).get('products', {})
        edges = products.get('edges', [])
        
        for edge in edges:
            node = edge['node']
            all_ids.append({
                'id': node['id'],
                'title': node['title'],
                'handle': node['handle']
            })
            cursor = edge['cursor']
        
        if not products.get('pageInfo', {}).get('hasNextPage', False):
            break
        
        time.sleep(0.5)  # é¿å…é€Ÿç‡é™åˆ¶
    
    print(f"[INFO] æ‰¾åˆ° {len(all_ids)} å€‹ WORKMAN å•†å“")
    return all_ids


def create_delete_jsonl(product_ids):
    """ç”¢ç”Ÿåˆªé™¤ç”¨çš„ JSONL æª”æ¡ˆ"""
    jsonl_filename = f"delete_workman_{int(time.time())}.jsonl"
    jsonl_path = os.path.join(JSONL_DIR, jsonl_filename)
    
    with open(jsonl_path, 'w', encoding='utf-8') as f:
        for product in product_ids:
            # productDelete çš„ input æ ¼å¼
            entry = {"input": {"id": product['id']}}
            f.write(json.dumps(entry, ensure_ascii=False) + '\n')
    
    print(f"[INFO] åˆªé™¤ JSONL å·²ç”¢ç”Ÿ: {jsonl_path} ({len(product_ids)} å€‹å•†å“)")
    return jsonl_path


def run_bulk_delete_mutation(staged_upload_path):
    """åŸ·è¡Œ Bulk Delete Mutation"""
    query = """
    mutation bulkOperationRunMutation($mutation: String!, $stagedUploadPath: String!) {
      bulkOperationRunMutation(mutation: $mutation, stagedUploadPath: $stagedUploadPath) {
        bulkOperation {
          id
          status
        }
        userErrors {
          field
          message
        }
      }
    }
    """
    
    # productDelete mutation
    mutation = """
    mutation call($input: ProductDeleteInput!) {
      productDelete(input: $input) {
        deletedProductId
        userErrors {
          field
          message
        }
      }
    }
    """
    
    variables = {
        "mutation": mutation,
        "stagedUploadPath": staged_upload_path
    }
    
    result = graphql_request(query, variables)
    return result


def run_delete_workman_products():
    """åŸ·è¡Œæ‰¹é‡åˆªé™¤ WORKMAN å•†å“"""
    global scrape_status
    
    scrape_status = {
        "running": True,
        "phase": "deleting",
        "progress": 0,
        "total": 0,
        "current_product": "æ­£åœ¨æŸ¥è©¢ WORKMAN å•†å“...",
        "products": [],
        "errors": [],
        "jsonl_file": "",
        "bulk_operation_id": "",
        "bulk_status": "",
    }
    
    try:
        # 1. æŸ¥è©¢æ‰€æœ‰ WORKMAN å•†å“
        print("[Delete] æŸ¥è©¢ WORKMAN å•†å“...")
        product_ids = fetch_workman_product_ids()
        
        if not product_ids:
            scrape_status['current_product'] = 'æ²’æœ‰æ‰¾åˆ° WORKMAN å•†å“'
            scrape_status['running'] = False
            return
        
        scrape_status['total'] = len(product_ids)
        scrape_status['current_product'] = f'æ‰¾åˆ° {len(product_ids)} å€‹å•†å“ï¼Œæº–å‚™åˆªé™¤...'
        
        # è¨˜éŒ„è¦åˆªé™¤çš„å•†å“
        for p in product_ids[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
            scrape_status['products'].append({
                'title': p['title'],
                'handle': p['handle'],
                'variants': 0
            })
        
        # 2. ç”¢ç”Ÿåˆªé™¤ JSONL
        print("[Delete] ç”¢ç”Ÿåˆªé™¤ JSONL...")
        jsonl_path = create_delete_jsonl(product_ids)
        scrape_status['jsonl_file'] = jsonl_path
        
        # 3. å»ºç«‹ Staged Upload
        print("[Delete] å»ºç«‹ Staged Upload...")
        scrape_status['current_product'] = 'ä¸Šå‚³åˆªé™¤æ¸…å–®...'
        staged = create_staged_upload()
        
        if not staged:
            scrape_status['errors'].append({'error': 'å»ºç«‹ Staged Upload å¤±æ•—'})
            scrape_status['running'] = False
            return
        
        # 4. ä¸Šå‚³ JSONL
        print("[Delete] ä¸Šå‚³ JSONL...")
        if not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³ JSONL å¤±æ•—'})
            scrape_status['running'] = False
            return
        
        # 5. åŸ·è¡Œ Bulk Delete
        print("[Delete] åŸ·è¡Œæ‰¹é‡åˆªé™¤...")
        scrape_status['current_product'] = 'åŸ·è¡Œæ‰¹é‡åˆªé™¤...'
        
        staged_path = None
        for param in staged['parameters']:
            if param['name'] == 'key':
                staged_path = param['value']
                break
        
        if not staged_path:
            staged_path = staged.get('resourceUrl', '')
        
        result = run_bulk_delete_mutation(staged_path)
        
        if 'errors' in result:
            scrape_status['errors'].append({'error': str(result['errors'])})
            scrape_status['running'] = False
            return
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        
        if user_errors:
            scrape_status['errors'].append({'error': str(user_errors)})
            scrape_status['running'] = False
            return
        
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡åˆªé™¤å·²å•Ÿå‹•ï¼æ­£åœ¨åˆªé™¤ {len(product_ids)} å€‹å•†å“..."
        
        print(f"[Delete] æ“ä½œ ID: {bulk_op.get('id')}, ç‹€æ…‹: {bulk_op.get('status')}")
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        print(f"[ERROR] {e}")
    finally:
        scrape_status['running'] = False


# ========== ä¸»æµç¨‹ ==========

def run_test_single():
    """æ¸¬è©¦å–®å“ï¼šçˆ¬å–ä¸€å€‹å•†å“ä¸¦ç›´æ¥ä¸Šå‚³åˆ° Shopify"""
    global scrape_status
    
    scrape_status = {
        "running": True,
        "phase": "testing",
        "progress": 0,
        "total": 1,
        "current_product": "æ¸¬è©¦å–®å“æ¨¡å¼...",
        "products": [],
        "errors": [],
        "jsonl_file": "",
        "bulk_operation_id": "",
        "bulk_status": "",
    }
    
    try:
        # ä½¿ç”¨å…’ç«¥æœåˆ†é¡æ¸¬è©¦
        cat_key = 'kids'
        cat_info = CATEGORIES[cat_key]
        tags = cat_info['tags']
        collection_name = cat_info['collection']
        
        # å–å¾—æˆ–å»ºç«‹ Collection
        scrape_status['current_product'] = f"å–å¾—/å»ºç«‹ {collection_name}..."
        print(f"[Test] å–å¾—/å»ºç«‹ {collection_name}...")
        collection_id = get_or_create_collection(collection_name)
        
        if not collection_id:
            scrape_status['errors'].append({'error': 'ç„¡æ³•å»ºç«‹ Collection'})
            scrape_status['running'] = False
            return
        
        # å–å¾—ç¬¬ä¸€å€‹å•†å“é€£çµ
        scrape_status['current_product'] = "å–å¾—å•†å“é€£çµ..."
        print("[Test] å–å¾—ç¬¬ä¸€å€‹å•†å“é€£çµ...")
        product_links = fetch_all_product_links(cat_key)
        
        if not product_links:
            scrape_status['errors'].append({'error': 'ç„¡æ³•å–å¾—å•†å“é€£çµ'})
            scrape_status['running'] = False
            return
        
        # åªå–ç¬¬ä¸€å€‹
        link = product_links[0]
        scrape_status['current_product'] = f"çˆ¬å–: {link.split('/')[-2]}"
        print(f"[Test] çˆ¬å–: {link}")
        
        # è§£æå•†å“
        product_data = parse_product_page(link)
        
        if not product_data:
            scrape_status['errors'].append({'error': 'è§£æå•†å“å¤±æ•—'})
            scrape_status['running'] = False
            return
        
        # ç¿»è­¯ä¸¦å»ºç«‹è³‡æ–™
        scrape_status['current_product'] = f"ç¿»è­¯: {product_data['title'][:20]}..."
        print(f"[Test] ç¿»è­¯: {product_data['title'][:30]}...")
        entry = product_to_jsonl_entry(product_data, tags, cat_key, collection_id)
        
        product_input = entry['productSet']
        
        scrape_status['products'].append({
            'title': product_input['title'],
            'handle': product_input['handle'],
            'variants': len(product_input.get('variants', []))
        })
        
        # ç›´æ¥ç”¨ productSet mutation ä¸Šå‚³ï¼ˆä¸ç”¨ bulk operationï¼‰
        scrape_status['current_product'] = "ä¸Šå‚³åˆ° Shopify..."
        print("[Test] ç›´æ¥ä¸Šå‚³åˆ° Shopify...")
        
        mutation = """
        mutation productSet($input: ProductSetInput!, $synchronous: Boolean!) {
          productSet(synchronous: $synchronous, input: $input) {
            product {
              id
              title
              handle
              status
              productType
              onlineStoreUrl
              metafields(first: 5) {
                edges {
                  node {
                    namespace
                    key
                    value
                  }
                }
              }
              seo {
                title
                description
              }
              variants(first: 10) {
                edges {
                  node {
                    id
                    sku
                    price
                    taxable
                    inventoryItem {
                      unitCost {
                        amount
                        currencyCode
                      }
                    }
                  }
                }
              }
            }
            userErrors {
              field
              code
              message
            }
          }
        }
        """
        
        load_shopify_token()
        result = graphql_request(mutation, {
            "input": product_input,
            "synchronous": True
        })
        
        # æª¢æŸ¥çµæœ
        product_set = result.get('data', {}).get('productSet', {})
        user_errors = product_set.get('userErrors', [])
        
        if user_errors:
            error_msg = '; '.join([e.get('message', str(e)) for e in user_errors])
            scrape_status['errors'].append({'error': f'ä¸Šå‚³å¤±æ•—: {error_msg}'})
            scrape_status['current_product'] = f"âŒ ä¸Šå‚³å¤±æ•—: {error_msg}"
            print(f"[Test] âŒ ä¸Šå‚³å¤±æ•—: {user_errors}")
        else:
            product = product_set.get('product', {})
            product_id = product.get('id', '')
            product_title = product.get('title', '')
            product_handle = product.get('handle', '')
            
            # ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®ç®¡é“
            scrape_status['current_product'] = "ç™¼å¸ƒåˆ°éŠ·å”®ç®¡é“..."
            print("[Test] ç™¼å¸ƒåˆ°éŠ·å”®ç®¡é“...")
            publish_result = publish_product_to_all_channels(product_id)
            
            if publish_result.get('success'):
                scrape_status['current_product'] = f"âœ… æ¸¬è©¦æˆåŠŸï¼å•†å“: {product_title}"
                print(f"[Test] âœ… æˆåŠŸï¼ID: {product_id}")
                print(f"[Test] æ¨™é¡Œ: {product_title}")
                print(f"[Test] Handle: {product_handle}")
                print(f"[Test] é¡å‹: {product.get('productType', '')}")
                print(f"[Test] SEO: {product.get('seo', {})}")
                print(f"[Test] ç™¼å¸ƒåˆ° {publish_result.get('publications', 0)} å€‹éŠ·å”®ç®¡é“")
                
                # è¨˜éŒ„è©³ç´°çµæœ
                scrape_status['test_result'] = {
                    'id': product_id,
                    'title': product_title,
                    'handle': product_handle,
                    'productType': product.get('productType', ''),
                    'seo': product.get('seo', {}),
                    'metafields': product.get('metafields', {}),
                    'variants': product.get('variants', {}),
                    'published': publish_result.get('publications', 0)
                }
            else:
                scrape_status['current_product'] = f"âš ï¸ å•†å“å·²å»ºç«‹ä½†ç™¼å¸ƒå¤±æ•—"
                scrape_status['errors'].append({'error': f'ç™¼å¸ƒå¤±æ•—: {publish_result}'})
        
        scrape_status['progress'] = 1
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        scrape_status['current_product'] = f"âŒ éŒ¯èª¤: {str(e)}"
        print(f"[Test] âŒ éŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
    finally:
        scrape_status['running'] = False


def run_scrape(category):
    """åŸ·è¡Œçˆ¬å–ï¼Œç”¢ç”Ÿ JSONL æª”æ¡ˆ"""
    global scrape_status
    
    scrape_status = {
        "running": True,
        "phase": "scraping",
        "progress": 0,
        "total": 0,
        "current_product": "",
        "products": [],
        "errors": [],
        "jsonl_file": "",
        "bulk_operation_id": "",
        "bulk_status": "",
    }
    
    try:
        categories_to_scrape = []
        if category == 'all':
            categories_to_scrape = ['work', 'mens', 'womens', 'kids']
        elif category in CATEGORIES:
            categories_to_scrape = [category]
        else:
            scrape_status['errors'].append({'error': f'æœªçŸ¥åˆ†é¡: {category}'})
            scrape_status['running'] = False
            return
        
        all_jsonl_entries = []
        
        for cat_key in categories_to_scrape:
            cat_info = CATEGORIES[cat_key]
            tags = cat_info['tags']
            collection_name = cat_info['collection']
            
            # å–å¾—æˆ–å»ºç«‹ Collection
            scrape_status['current_product'] = f"æ­£åœ¨å–å¾—/å»ºç«‹ {collection_name} å•†å“ç³»åˆ—..."
            print(f"[Collection] å–å¾—/å»ºç«‹ {collection_name}...")
            collection_id = get_or_create_collection(collection_name)
            
            if not collection_id:
                error_msg = f"ç„¡æ³•å–å¾—/å»ºç«‹ {collection_name} å•†å“ç³»åˆ—"
                print(f"[ERROR] {error_msg}")
                scrape_status['errors'].append({'error': error_msg})
                continue
            
            scrape_status['current_product'] = f"æ­£åœ¨å–å¾— {collection_name} å•†å“é€£çµ..."
            print(f"[DEBUG] é–‹å§‹å–å¾— {collection_name} å•†å“é€£çµ...")
            
            product_links = fetch_all_product_links(cat_key)
            
            print(f"[DEBUG] å–å¾— {len(product_links)} å€‹å•†å“é€£çµ")
            
            if not product_links:
                error_msg = f"{collection_name} å–å¾— 0 å€‹å•†å“é€£çµï¼Œå¯èƒ½æ˜¯ç¶²è·¯å•é¡Œæˆ–ç¶²ç«™çµæ§‹è®Šæ›´"
                print(f"[ERROR] {error_msg}")
                scrape_status['errors'].append({'error': error_msg})
                scrape_status['current_product'] = error_msg
                continue
            
            scrape_status['total'] += len(product_links)
            scrape_status['current_product'] = f"æ‰¾åˆ° {len(product_links)} å€‹å•†å“ï¼Œé–‹å§‹è™•ç†..."
            
            for idx, link in enumerate(product_links):
                scrape_status['progress'] += 1
                scrape_status['current_product'] = f"[{scrape_status['progress']}/{scrape_status['total']}] {link.split('/')[-2]}"
                
                product_data = parse_product_page(link)
                
                if not product_data:
                    scrape_status['errors'].append({'url': link, 'error': 'è§£æå¤±æ•—'})
                    continue
                
                try:
                    print(f"[ç¿»è­¯] {product_data['title'][:30]}...")
                    entry = product_to_jsonl_entry(product_data, tags, cat_key, collection_id)
                    all_jsonl_entries.append(entry)
                    
                    scrape_status['products'].append({
                        'title': entry['productSet']['title'],
                        'handle': entry['productSet']['handle'],
                        'variants': len(entry['productSet'].get('variants', []))
                    })
                    print(f"[OK] {entry['productSet']['title'][:30]}")
                except Exception as e:
                    print(f"[ERROR] {product_data['title'][:20]}: {e}")
                    scrape_status['errors'].append({'url': link, 'error': str(e)})
                
                time.sleep(0.5)  # é¿å…ç¿»è­¯ API éè¼‰
        
        # å¯«å…¥ JSONL æª”æ¡ˆ
        if all_jsonl_entries:
            jsonl_filename = f"workman_{category}_{int(time.time())}.jsonl"
            jsonl_path = os.path.join(JSONL_DIR, jsonl_filename)
            
            with open(jsonl_path, 'w', encoding='utf-8') as f:
                for entry in all_jsonl_entries:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            scrape_status['jsonl_file'] = jsonl_path
            print(f"[å®Œæˆ] JSONL æª”æ¡ˆå·²ç”¢ç”Ÿ: {jsonl_path} ({len(all_jsonl_entries)} å€‹å•†å“)")
        
        scrape_status['current_product'] = f"å®Œæˆï¼å…± {len(all_jsonl_entries)} å€‹å•†å“"
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        print(f"[ERROR] {e}")
    finally:
        scrape_status['running'] = False
        scrape_status['phase'] = "completed"


def run_bulk_upload(jsonl_path):
    """åŸ·è¡Œ Bulk Upload"""
    global scrape_status
    
    scrape_status['phase'] = 'uploading'
    scrape_status['running'] = True
    scrape_status['current_product'] = 'æ­£åœ¨æº–å‚™ä¸Šå‚³...'
    
    try:
        # 1. å»ºç«‹ Staged Upload
        print("[Bulk] å»ºç«‹ Staged Upload...")
        scrape_status['current_product'] = 'å»ºç«‹ä¸Šå‚³é€£çµ...'
        staged = create_staged_upload()
        
        if not staged:
            scrape_status['errors'].append({'error': 'å»ºç«‹ Staged Upload å¤±æ•—'})
            return
        
        # 2. ä¸Šå‚³ JSONL
        print("[Bulk] ä¸Šå‚³ JSONL æª”æ¡ˆ...")
        scrape_status['current_product'] = 'ä¸Šå‚³ JSONL æª”æ¡ˆ...'
        
        if not upload_jsonl_to_staged(staged, jsonl_path):
            scrape_status['errors'].append({'error': 'ä¸Šå‚³ JSONL å¤±æ•—'})
            return
        
        # 3. åŸ·è¡Œ Bulk Mutation
        print("[Bulk] åŸ·è¡Œæ‰¹é‡å»ºç«‹...")
        scrape_status['current_product'] = 'åŸ·è¡Œæ‰¹é‡å»ºç«‹...'
        
        # æ‰¾åˆ° key åƒæ•¸ä½œç‚º stagedUploadPath
        staged_path = None
        for param in staged['parameters']:
            if param['name'] == 'key':
                staged_path = param['value']
                break
        
        if not staged_path:
            staged_path = staged.get('resourceUrl', '')
        
        print(f"[Bulk] Staged path: {staged_path}")
        result = run_bulk_mutation(staged_path)
        
        if 'errors' in result:
            scrape_status['errors'].append({'error': str(result['errors'])})
            return
        
        bulk_op = result.get('data', {}).get('bulkOperationRunMutation', {}).get('bulkOperation', {})
        user_errors = result.get('data', {}).get('bulkOperationRunMutation', {}).get('userErrors', [])
        
        if user_errors:
            scrape_status['errors'].append({'error': str(user_errors)})
            return
        
        scrape_status['bulk_operation_id'] = bulk_op.get('id', '')
        scrape_status['bulk_status'] = bulk_op.get('status', '')
        scrape_status['current_product'] = f"æ‰¹é‡æ“ä½œå·²å•Ÿå‹•: {bulk_op.get('status', '')}"
        
        print(f"[Bulk] æ“ä½œ ID: {bulk_op.get('id')}, ç‹€æ…‹: {bulk_op.get('status')}")
        
    except Exception as e:
        scrape_status['errors'].append({'error': str(e)})
        print(f"[ERROR] {e}")
    finally:
        scrape_status['running'] = False


# ========== Flask è·¯ç”± ==========

@app.route('/')
def index():
    return '''<!DOCTYPE html>
<html>
<head>
    <meta charset="UTF-8">
    <title>WORKMAN çˆ¬èŸ² (Bulk Operations)</title>
    <style>
        * { box-sizing: border-box; }
        body { font-family: -apple-system, BlinkMacSystemFont, sans-serif; max-width: 1000px; margin: 0 auto; padding: 20px; background: #f5f5f5; }
        h1 { color: #d32f2f; }
        .card { background: white; border-radius: 12px; padding: 20px; margin: 20px 0; box-shadow: 0 2px 8px rgba(0,0,0,0.1); }
        .btn { padding: 12px 24px; border: none; border-radius: 8px; cursor: pointer; font-size: 16px; margin: 5px; transition: all 0.2s; }
        .btn:hover { transform: translateY(-2px); }
        .btn-work { background: #1976d2; color: white; }
        .btn-mens { background: #388e3c; color: white; }
        .btn-womens { background: #d81b60; color: white; }
        .btn-kids { background: #f57c00; color: white; }
        .btn-all { background: #7b1fa2; color: white; }
        .btn-upload { background: #d32f2f; color: white; font-size: 18px; padding: 15px 30px; }
        .btn-check { background: #455a64; color: white; }
        .btn-delete { background: #b71c1c; color: white; }
        .btn:disabled { background: #ccc; cursor: not-allowed; transform: none; }
        #status { padding: 15px; background: #e3f2fd; border-radius: 8px; margin: 15px 0; }
        #log { height: 300px; overflow-y: auto; background: #263238; color: #aed581; padding: 15px; border-radius: 8px; font-family: monospace; font-size: 13px; }
        .progress { height: 8px; background: #e0e0e0; border-radius: 4px; margin: 10px 0; }
        .progress-bar { height: 100%; background: linear-gradient(90deg, #4caf50, #8bc34a); border-radius: 4px; transition: width 0.3s; }
        .progress-bar-delete { background: linear-gradient(90deg, #f44336, #ff5722); }
        table { width: 100%; border-collapse: collapse; margin-top: 15px; }
        th, td { padding: 10px; text-align: left; border-bottom: 1px solid #eee; }
        th { background: #f5f5f5; }
        .phase { display: inline-block; padding: 4px 12px; border-radius: 20px; font-size: 12px; font-weight: bold; }
        .phase-scraping { background: #fff3e0; color: #e65100; }
        .phase-uploading { background: #e3f2fd; color: #1565c0; }
        .phase-deleting { background: #ffebee; color: #c62828; }
        .phase-completed { background: #e8f5e9; color: #2e7d32; }
        .warning-box { background: #fff3e0; border: 2px solid #ff9800; border-radius: 8px; padding: 15px; margin: 10px 0; }
    </style>
</head>
<body>
    <h1>ğŸ­ WORKMAN çˆ¬èŸ² (Bulk Operations ç‰ˆ)</h1>
    
    <div class="card">
        <h3>ğŸ”— é€£ç·šæ¸¬è©¦</h3>
        <p>çˆ¬å–å‰å…ˆæ¸¬è©¦æ˜¯å¦èƒ½é€£æ¥åˆ° workman.jp</p>
        <button class="btn btn-check" onclick="testConnection()">ğŸ”— æ¸¬è©¦é€£ç·š workman.jp</button>
        <button class="btn btn-check" onclick="testProductParse()">ğŸ” æ¸¬è©¦å•†å“é é¢è§£æ</button>
        <button class="btn btn-check" onclick="testShopify()">ğŸ”— æ¸¬è©¦é€£ç·š Shopify</button>
    </div>
    
    <div class="card" style="border: 2px solid #28a745; background: #f0fff4;">
        <h3>ğŸ§ª æ¸¬è©¦å–®å“ï¼ˆå¿«é€Ÿé©—è­‰ï¼‰</h3>
        <p>åªçˆ¬å–<strong>ä¸€å€‹å•†å“</strong>ä¸¦ç›´æ¥ä¸Šå‚³åˆ° Shopifyï¼Œç”¨æ–¼å¿«é€Ÿæ¸¬è©¦æ ¼å¼æ˜¯å¦æ­£ç¢ºã€‚</p>
        <button class="btn" style="background:#28a745;color:white;" onclick="testSingle()">ğŸ§ª æ¸¬è©¦å–®å“ä¸Šå‚³</button>
        <button class="btn btn-check" onclick="checkTestResult()">ğŸ“‹ æŸ¥çœ‹æ¸¬è©¦çµæœ</button>
        <div id="testResult" style="margin-top:10px;padding:10px;background:#fff;border-radius:5px;display:none;"></div>
    </div>
    
    <div class="card">
        <h3>ğŸ“¥ ç¬¬ä¸€æ­¥ï¼šçˆ¬å–å•†å“ â†’ ç”¢ç”Ÿ JSONL</h3>
        <p>é¸æ“‡åˆ†é¡é–‹å§‹çˆ¬å–ï¼Œå®Œæˆå¾Œæœƒç”¢ç”Ÿ JSONL æª”æ¡ˆ</p>
        <button class="btn btn-work" onclick="startScrape('work')">ğŸ”§ ä½œæ¥­æœ</button>
        <button class="btn btn-mens" onclick="startScrape('mens')">ğŸ‘” ç”·è£</button>
        <button class="btn btn-womens" onclick="startScrape('womens')">ğŸ‘— å¥³è£</button>
        <button class="btn btn-kids" onclick="startScrape('kids')">ğŸ‘¶ å…’ç«¥æœ</button>
        <button class="btn btn-all" onclick="startScrape('all')">ğŸš€ å…¨éƒ¨</button>
    </div>
    
    <div class="card">
        <h3>ğŸ“¤ ç¬¬äºŒæ­¥ï¼šæ‰¹é‡ä¸Šå‚³åˆ° Shopify</h3>
        <p>çˆ¬å–å®Œæˆå¾Œï¼Œé»æ“Šä¸‹æ–¹æŒ‰éˆ•æ‰¹é‡ä¸Šå‚³ï¼ˆæ•¸åƒå•†å“åªéœ€å¹¾åˆ†é˜ï¼‰</p>
        <div style="margin: 10px 0; padding: 10px; background: #e8f5e9; border-radius: 8px;">
            <label style="cursor: pointer;">
                <input type="checkbox" id="autoPublish" checked style="margin-right: 8px;">
                <strong>ä¸Šå‚³å®Œæˆå¾Œè‡ªå‹•ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®ç®¡é“</strong>
            </label>
        </div>
        <button class="btn btn-upload" id="uploadBtn" onclick="startUpload()" disabled>ğŸ“¤ æ‰¹é‡ä¸Šå‚³åˆ° Shopify</button>
        <button class="btn btn-check" onclick="checkStatus()">ğŸ” æª¢æŸ¥ä¸Šå‚³ç‹€æ…‹</button>
        <button class="btn btn-check" onclick="checkResults()">ğŸ“‹ æŸ¥çœ‹è©³ç´°çµæœ</button>
    </div>
    
    <div class="card">
        <h3>ğŸ—‘ï¸ æ‰¹é‡åˆªé™¤ WORKMAN å•†å“</h3>
        <div class="warning-box">
            âš ï¸ <strong>è­¦å‘Šï¼šæ­¤æ“ä½œæœƒåˆªé™¤ Shopify ä¸­æ‰€æœ‰ vendor ç‚º "WORKMAN" çš„å•†å“ï¼</strong>
        </div>
        <button class="btn btn-delete" onclick="startDelete()">ğŸ—‘ï¸ åˆªé™¤æ‰€æœ‰ WORKMAN å•†å“</button>
        <button class="btn btn-check" onclick="countProducts()">ğŸ“Š æŸ¥è©¢å•†å“æ•¸é‡</button>
    </div>
    
    <div class="card">
        <h3>ğŸ“¢ ç™¼å¸ƒåˆ°éŠ·å”®ç®¡é“</h3>
        <p>å•†å“å»ºç«‹å¾Œï¼Œéœ€è¦ç™¼å¸ƒåˆ°éŠ·å”®ç®¡é“æ‰æœƒåœ¨å•†åº—é¡¯ç¤ºã€‚</p>
        <button class="btn btn-upload" onclick="publishAll()">ğŸ“¢ ç™¼å¸ƒæ‰€æœ‰ WORKMAN å•†å“</button>
        <button class="btn btn-check" onclick="getPublications()">ğŸ“‹ æŸ¥çœ‹éŠ·å”®ç®¡é“</button>
    </div>
    
    <div class="card">
        <h3>ğŸ“Š åŸ·è¡Œç‹€æ…‹</h3>
        <div id="status">ç­‰å¾…é–‹å§‹...</div>
        <div class="progress"><div class="progress-bar" id="progressBar" style="width:0%"></div></div>
    </div>
    
    <div class="card">
        <h3>ğŸ“‹ åŸ·è¡Œè¨˜éŒ„</h3>
        <div id="log"></div>
    </div>
    
    <script>
        let currentJsonlFile = '';
        
        function startScrape(category) {
            if (!confirm(`ç¢ºå®šè¦çˆ¬å– ${category} åˆ†é¡ï¼Ÿ`)) return;
            
            // é‡ç½®ç‹€æ…‹
            resetTracking();
            document.getElementById('uploadBtn').disabled = true;
            document.getElementById('log').innerHTML = '';
            
            fetch('/api/start?category=' + category)
                .then(r => r.json())
                .then(data => {
                    log('ğŸš€ é–‹å§‹çˆ¬å–: ' + category);
                    pollStatus();
                });
        }
        
        function startUpload() {
            if (!currentJsonlFile) {
                alert('è«‹å…ˆå®Œæˆçˆ¬å–ï¼');
                return;
            }
            if (!confirm('ç¢ºå®šè¦æ‰¹é‡ä¸Šå‚³åˆ° Shopifyï¼Ÿ')) return;
            
            resetTracking();
            
            fetch('/api/upload?file=' + encodeURIComponent(currentJsonlFile))
                .then(r => r.json())
                .then(data => {
                    log('ğŸš€ é–‹å§‹æ‰¹é‡ä¸Šå‚³...');
                    pollStatus();
                    // é–‹å§‹è¼ªè©¢ bulk operation ç‹€æ…‹
                    setTimeout(pollBulkStatus, 5000);
                });
        }
        
        function pollBulkStatus() {
            fetch('/api/bulk_status')
                .then(r => r.json())
                .then(data => {
                    let status = data.status || 'UNKNOWN';
                    let count = data.objectCount || 0;
                    
                    document.getElementById('status').textContent = `Bulk Operation: ${status}, è™•ç†æ•¸: ${count}`;
                    
                    if (status === 'COMPLETED') {
                        log(`âœ… æ‰¹é‡ä¸Šå‚³å®Œæˆï¼å…±è™•ç† ${count} å€‹å•†å“`);
                        
                        // è‡ªå‹•ç™¼å¸ƒ
                        if (document.getElementById('autoPublish') && document.getElementById('autoPublish').checked) {
                            log('ğŸ“¢ è‡ªå‹•ç™¼å¸ƒåˆ°æ‰€æœ‰éŠ·å”®ç®¡é“...');
                            publishAll();
                        } else {
                            log('âš ï¸ è«‹é»æ“Šã€ŒğŸ“¢ ç™¼å¸ƒæ‰€æœ‰ WORKMAN å•†å“ã€æŒ‰éˆ•ä¾†é–‹å•ŸéŠ·å”®ç®¡é“');
                        }
                    } else if (status === 'FAILED' || status === 'CANCELED') {
                        log(`âŒ æ‰¹é‡ä¸Šå‚³å¤±æ•—: ${status}`);
                        if (data.errorCode) {
                            log(`éŒ¯èª¤ç¢¼: ${data.errorCode}`);
                        }
                    } else if (status === 'RUNNING' || status === 'CREATED') {
                        // ç¹¼çºŒè¼ªè©¢
                        setTimeout(pollBulkStatus, 3000);
                    }
                })
                .catch(err => {
                    log('âŒ æª¢æŸ¥ç‹€æ…‹å¤±æ•—: ' + err);
                });
        }
        
        function checkStatus() {
            fetch('/api/bulk_status')
                .then(r => r.json())
                .then(data => {
                    let status = data.status || 'UNKNOWN';
                    let count = data.objectCount || 0;
                    log(`ğŸ“Š Bulk ç‹€æ…‹: ${status}, è™•ç†æ•¸: ${count}`);
                    if (data.errorCode) {
                        log(`âŒ éŒ¯èª¤ç¢¼: ${data.errorCode}`);
                    }
                    if (data.url) {
                        log(`ğŸ“„ çµæœ URL: æœ‰`);
                    }
                    
                    // å¦‚æœå®Œæˆäº†ï¼Œæç¤ºç™¼å¸ƒ
                    if (status === 'COMPLETED') {
                        log('âœ… æ‰¹é‡ä¸Šå‚³å·²å®Œæˆï¼');
                        if (document.getElementById('autoPublish') && document.getElementById('autoPublish').checked) {
                            log('ğŸ“¢ è‡ªå‹•ç™¼å¸ƒä¸­...');
                            publishAll();
                        } else {
                            log('âš ï¸ è«‹é»æ“Šã€ŒğŸ“¢ ç™¼å¸ƒæ‰€æœ‰ WORKMAN å•†å“ã€æŒ‰éˆ•ä¾†é–‹å•ŸéŠ·å”®ç®¡é“');
                        }
                    }
                });
        }
        
        function checkResults() {
            log('ğŸ“‹ æ­£åœ¨å–å¾—è©³ç´°çµæœ...');
            fetch('/api/bulk_results')
                .then(r => r.json())
                .then(data => {
                    log(`ğŸ“Š ç‹€æ…‹: ${data.status}`);
                    log(`ğŸ“Š ç¸½æ•¸: ${data.objectCount}`);
                    
                    if (data.error_count !== undefined) {
                        log(`âœ… æˆåŠŸ: ${data.success_count} å€‹`);
                        log(`âŒ å¤±æ•—: ${data.error_count} å€‹`);
                    }
                    
                    if (data.successes && data.successes.length > 0) {
                        log('--- æˆåŠŸçš„å•†å“ ---');
                        for (let s of data.successes.slice(0, 5)) {
                            log(`   âœ“ ${s.title}`);
                        }
                    }
                    
                    if (data.errors && data.errors.length > 0) {
                        log('--- éŒ¯èª¤è¨Šæ¯ ---');
                        for (let e of data.errors.slice(0, 5)) {
                            log(`   âŒ ${JSON.stringify(e.errors)}`);
                        }
                    }
                    
                    if (data.fetch_error) {
                        log(`âŒ å–å¾—çµæœå¤±æ•—: ${data.fetch_error}`);
                    }
                    
                    if (!data.url) {
                        log('âš ï¸ æ²’æœ‰çµæœ URLï¼Œå¯èƒ½æ“ä½œå°šæœªå®Œæˆ');
                    }
                })
                .catch(err => {
                    log('âŒ å–å¾—çµæœå¤±æ•—: ' + err);
                });
        }
        
        function startDelete() {
            if (!confirm('âš ï¸ è­¦å‘Šï¼\\n\\næ­¤æ“ä½œæœƒåˆªé™¤ Shopify ä¸­æ‰€æœ‰ WORKMAN å•†å“ï¼\\n\\nç¢ºå®šè¦ç¹¼çºŒå—ï¼Ÿ')) return;
            if (!confirm('å†æ¬¡ç¢ºèªï¼šçœŸçš„è¦åˆªé™¤æ‰€æœ‰ WORKMAN å•†å“å—ï¼Ÿ')) return;
            
            resetTracking();
            document.getElementById('log').innerHTML = '';
            
            fetch('/api/delete')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ éŒ¯èª¤: ' + data.error);
                    } else {
                        log('ğŸ—‘ï¸ é–‹å§‹æ‰¹é‡åˆªé™¤...');
                        pollStatus();
                    }
                });
        }
        
        function countProducts() {
            log('ğŸ“Š æ­£åœ¨æŸ¥è©¢ WORKMAN å•†å“æ•¸é‡...');
            fetch('/api/count')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ éŒ¯èª¤: ' + data.error);
                    } else {
                        log(`ğŸ“Š ç›®å‰æœ‰ ${data.count} å€‹ WORKMAN å•†å“`);
                    }
                });
        }
        
        function publishAll() {
            if (!confirm('ç¢ºå®šè¦ç™¼å¸ƒæ‰€æœ‰ WORKMAN å•†å“åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“ï¼Ÿ')) return;
            
            log('ğŸ“¢ æ­£åœ¨ç™¼å¸ƒå•†å“åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“...');
            document.getElementById('status').textContent = 'æ­£åœ¨ç™¼å¸ƒå•†å“...';
            
            fetch('/api/publish_all')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ éŒ¯èª¤: ' + data.error);
                    } else {
                        log(`ğŸ“¢ ç™¼å¸ƒå®Œæˆï¼æˆåŠŸ: ${data.success}, å¤±æ•—: ${data.failed}`);
                        if (data.errors && data.errors.length > 0) {
                            log('éŒ¯èª¤è©³æƒ…: ' + JSON.stringify(data.errors.slice(0, 3)));
                        }
                    }
                    document.getElementById('status').textContent = 'ç™¼å¸ƒå®Œæˆ';
                })
                .catch(err => {
                    log('âŒ ç™¼å¸ƒå¤±æ•—: ' + err);
                });
        }
        
        function getPublications() {
            log('ğŸ“‹ æ­£åœ¨æŸ¥è©¢éŠ·å”®ç®¡é“...');
            fetch('/api/publications')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ éŒ¯èª¤: ' + data.error);
                    } else if (data.publications) {
                        log(`ğŸ“‹ æ‰¾åˆ° ${data.publications.length} å€‹éŠ·å”®ç®¡é“:`);
                        data.publications.forEach(pub => {
                            log(`   - ${pub.name} (${pub.id})`);
                        });
                    }
                });
        }
        
        function testConnection() {
            log('ğŸ”— æ¸¬è©¦é€£ç·š workman.jp...');
            fetch('/api/test_workman')
                .then(r => r.json())
                .then(data => {
                    if (data.homepage && data.homepage.ok) {
                        log('âœ… workman.jp ä¸»é é€£ç·šæˆåŠŸ');
                    } else {
                        log('âŒ workman.jp ä¸»é é€£ç·šå¤±æ•—: ' + JSON.stringify(data.homepage));
                    }
                    
                    if (data.kids_page && data.kids_page.ok) {
                        log(`âœ… å…’ç«¥æœåˆ†é¡é é€£ç·šæˆåŠŸï¼Œæ‰¾åˆ° ${data.kids_page.goods_links_found || 0} å€‹å•†å“é€£çµ`);
                        if (data.kids_page.first_link) {
                            log(`   ç¬¬ä¸€å€‹é€£çµ: ${data.kids_page.first_link}`);
                        }
                    } else {
                        log('âŒ å…’ç«¥æœåˆ†é¡é é€£ç·šå¤±æ•—: ' + JSON.stringify(data.kids_page));
                    }
                })
                .catch(err => {
                    log('âŒ æ¸¬è©¦å¤±æ•—: ' + err);
                });
        }
        
        function testProductParse() {
            log('ğŸ” æ¸¬è©¦å•†å“é é¢è§£æ...');
            fetch('/api/test_product')
                .then(r => r.json())
                .then(data => {
                    log('ğŸ“„ æ¸¬è©¦ URL: ' + data.url);
                    log('   HTTP ç‹€æ…‹: ' + data.status);
                    
                    if (data.title_found) {
                        log('   âœ… æ¨™é¡Œ: ' + data.title);
                    } else {
                        log('   âŒ æ‰¾ä¸åˆ°æ¨™é¡Œ (block-goods-name)');
                        if (data.h1_found) {
                            log('   ğŸ“ å‚™ç”¨ h1: ' + data.h1_text);
                        }
                    }
                    
                    if (data.price_elem_found) {
                        log('   âœ… åƒ¹æ ¼: ' + data.price_text);
                    } else {
                        log('   âŒ æ‰¾ä¸åˆ°åƒ¹æ ¼ (block-goods-price)');
                        if (data.price_any_found) {
                            log('   ğŸ“ å‚™ç”¨åƒ¹æ ¼: ' + data.price_any_text);
                        }
                    }
                    
                    if (data.manage_code_dt_found) {
                        log('   âœ… ç®¡ç†ç•ªè™Ÿ: ' + data.manage_code);
                    } else {
                        log('   âŒ æ‰¾ä¸åˆ°ç®¡ç†ç•ªè™Ÿ (dt ç®¡ç†ç•ªå·)');
                        if (data.manage_code_from_url) {
                            log('   ğŸ“ å¾ URL å–å¾—: ' + data.manage_code_from_url);
                        }
                    }
                    
                    if (data.relevant_classes && data.relevant_classes.length > 0) {
                        log('   ğŸ“‹ ç›¸é—œ class: ' + data.relevant_classes.slice(0, 10).join(', '));
                    }
                })
                .catch(err => {
                    log('âŒ æ¸¬è©¦å¤±æ•—: ' + err);
                });
        }
        
        function testShopify() {
            log('ğŸ”— æ¸¬è©¦é€£ç·š Shopify...');
            fetch('/api/test')
                .then(r => r.json())
                .then(data => {
                    if (data.data && data.data.shop) {
                        log('âœ… Shopify é€£ç·šæˆåŠŸ: ' + data.data.shop.name);
                    } else if (data.errors) {
                        log('âŒ Shopify é€£ç·šå¤±æ•—: ' + JSON.stringify(data.errors));
                    } else {
                        log('âš ï¸ Shopify å›æ‡‰: ' + JSON.stringify(data));
                    }
                })
                .catch(err => {
                    log('âŒ æ¸¬è©¦å¤±æ•—: ' + err);
                });
        }
        
        function testSingle() {
            if (!confirm('å°‡çˆ¬å–ä¸€å€‹å…’ç«¥æœå•†å“ä¸¦ç›´æ¥ä¸Šå‚³åˆ° Shopifyï¼Œç¢ºå®šè¦æ¸¬è©¦ï¼Ÿ')) return;
            
            log('ğŸ§ª é–‹å§‹æ¸¬è©¦å–®å“ä¸Šå‚³...');
            document.getElementById('status').textContent = 'æ¸¬è©¦å–®å“æ¨¡å¼...';
            document.getElementById('testResult').style.display = 'none';
            
            fetch('/api/test_single')
                .then(r => r.json())
                .then(data => {
                    if (data.error) {
                        log('âŒ ' + data.error);
                    } else {
                        log('ğŸ§ª æ¸¬è©¦å·²é–‹å§‹ï¼Œè«‹ç­‰å¾…...');
                        pollTestStatus();
                    }
                })
                .catch(err => {
                    log('âŒ æ¸¬è©¦å•Ÿå‹•å¤±æ•—: ' + err);
                });
        }
        
        function pollTestStatus() {
            fetch('/api/status')
                .then(r => r.json())
                .then(data => {
                    document.getElementById('status').textContent = data.current_product || 'è™•ç†ä¸­...';
                    
                    if (data.running) {
                        setTimeout(pollTestStatus, 1000);
                    } else {
                        // æ¸¬è©¦å®Œæˆï¼Œé¡¯ç¤ºçµæœ
                        checkTestResult();
                    }
                });
        }
        
        function checkTestResult() {
            log('ğŸ“‹ æŸ¥è©¢æ¸¬è©¦çµæœ...');
            fetch('/api/test_result')
                .then(r => r.json())
                .then(data => {
                    const resultDiv = document.getElementById('testResult');
                    resultDiv.style.display = 'block';
                    
                    if (data.errors && data.errors.length > 0) {
                        resultDiv.innerHTML = '<strong style="color:red;">âŒ æ¸¬è©¦å¤±æ•—:</strong><br>' + 
                            data.errors.map(e => e.error || JSON.stringify(e)).join('<br>');
                        log('âŒ æ¸¬è©¦å¤±æ•—: ' + JSON.stringify(data.errors));
                    } else if (data.test_result && data.test_result.id) {
                        const r = data.test_result;
                        
                        // å–å¾—ç¬¬ä¸€å€‹ variant çš„è³‡è¨Š
                        let variantInfo = '(ç„¡)';
                        if (r.variants && r.variants.edges && r.variants.edges.length > 0) {
                            const v = r.variants.edges[0].node;
                            const cost = v.inventoryItem?.unitCost?.amount || '(ç©º)';
                            const currency = v.inventoryItem?.unitCost?.currencyCode || '';
                            const taxable = v.taxable === false ? 'âŒ ä¸èª²ç¨…' : 'âœ… èª²ç¨…';
                            variantInfo = `SKU: ${v.sku}, åƒ¹æ ¼: ${v.price}, æˆæœ¬: ${cost} ${currency}, ${taxable}`;
                        }
                        
                        resultDiv.innerHTML = `
                            <strong style="color:green;">âœ… æ¸¬è©¦æˆåŠŸï¼</strong><br>
                            <table style="width:100%;border-collapse:collapse;margin-top:10px;">
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>ID</strong></td><td style="padding:5px;border:1px solid #ddd;">${r.id}</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>æ¨™é¡Œ</strong></td><td style="padding:5px;border:1px solid #ddd;">${r.title}</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>Handle</strong></td><td style="padding:5px;border:1px solid #ddd;">${r.handle}</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>å•†å“é¡å‹</strong></td><td style="padding:5px;border:1px solid #ddd;">${r.productType || '(ç©º)'}</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>SEO æ¨™é¡Œ</strong></td><td style="padding:5px;border:1px solid #ddd;">${r.seo?.title || '(ç©º)'}</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>SEO æè¿°</strong></td><td style="padding:5px;border:1px solid #ddd;">${(r.seo?.description || '(ç©º)').substring(0, 80)}...</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>Variant (ç¬¬1å€‹)</strong></td><td style="padding:5px;border:1px solid #ddd;">${variantInfo}</td></tr>
                                <tr><td style="padding:5px;border:1px solid #ddd;"><strong>éŠ·å”®ç®¡é“</strong></td><td style="padding:5px;border:1px solid #ddd;">${r.published} å€‹</td></tr>
                            </table>
                            <p style="margin-top:10px;">ğŸ‘‰ <a href="https://admin.shopify.com/store/goyoulink/products" target="_blank">å‰å¾€ Shopify å¾Œå°æŸ¥çœ‹</a></p>
                        `;
                        log('âœ… æ¸¬è©¦æˆåŠŸï¼å•†å“: ' + r.title);
                    } else {
                        resultDiv.innerHTML = '<strong>â³ å°šç„¡æ¸¬è©¦çµæœ</strong><br>ç‹€æ…‹: ' + (data.current_product || 'ç­‰å¾…ä¸­');
                    }
                })
                .catch(err => {
                    log('âŒ æŸ¥è©¢å¤±æ•—: ' + err);
                });
        }
        
        function resetTracking() {
            lastProductCount = 0;
            lastProgress = 0;
            lastPhase = '';
            lastErrorCount = 0;
        }
        
        function pollStatus() {
            fetch('/api/status')
                .then(r => r.json())
                .then(data => {
                    updateUI(data);
                    if (data.running) {
                        setTimeout(pollStatus, 1000);  // 1 ç§’æ›´æ–°ä¸€æ¬¡
                    }
                });
        }
        
        let lastProductCount = 0;
        let lastProgress = 0;
        let lastPhase = '';
        
        function updateUI(data) {
            let phaseClass = 'phase-' + data.phase;
            let phaseText = {scraping: 'çˆ¬å–ä¸­', uploading: 'ä¸Šå‚³ä¸­', deleting: 'åˆªé™¤ä¸­', completed: 'å®Œæˆ'}[data.phase] || data.phase;
            
            // éšæ®µè®ŠåŒ–æ™‚è¨˜éŒ„
            if (data.phase !== lastPhase) {
                if (data.phase === 'scraping') log('ğŸ“¥ é–‹å§‹çˆ¬å–å•†å“...');
                else if (data.phase === 'uploading') log('ğŸ“¤ é–‹å§‹ä¸Šå‚³åˆ° Shopify...');
                else if (data.phase === 'deleting') log('ğŸ—‘ï¸ é–‹å§‹åˆªé™¤å•†å“...');
                else if (data.phase === 'completed') log('âœ… ä½œæ¥­å®Œæˆï¼');
                lastPhase = data.phase;
            }
            
            let statusHtml = `<span class="phase ${phaseClass}">${phaseText}</span> `;
            statusHtml += data.current_product || '';
            
            if (data.total > 0) {
                statusHtml += `<br>é€²åº¦: ${data.progress} / ${data.total}`;
                let pct = (data.progress / data.total * 100).toFixed(1);
                statusHtml += ` (${pct}%)`;
            }
            if (data.jsonl_file) {
                statusHtml += `<br>ğŸ“„ JSONL: ${data.jsonl_file.split('/').pop()}`;
                currentJsonlFile = data.jsonl_file;
                document.getElementById('uploadBtn').disabled = false;
            }
            if (data.bulk_operation_id) {
                statusHtml += `<br>ğŸ”„ Bulk ID: ${data.bulk_operation_id.split('/').pop()}`;
                statusHtml += `<br>ğŸ“Š ç‹€æ…‹: ${data.bulk_status}`;
            }
            if (data.errors.length > 0) {
                statusHtml += `<br>âš ï¸ éŒ¯èª¤: ${data.errors.length} å€‹`;
            }
            
            document.getElementById('status').innerHTML = statusHtml;
            
            let pct = data.total > 0 ? (data.progress / data.total * 100) : 0;
            document.getElementById('progressBar').style.width = pct + '%';
            
            // é€²åº¦è®ŠåŒ–æ™‚è¨˜éŒ„
            if (data.progress > lastProgress && data.progress % 10 === 0) {
                log(`ğŸ“Š é€²åº¦: ${data.progress} / ${data.total}`);
            }
            lastProgress = data.progress;
            
            // æ–°å•†å“æ™‚è¨˜éŒ„
            if (data.products.length > lastProductCount) {
                let newProducts = data.products.slice(lastProductCount);
                for (let p of newProducts) {
                    log(`âœ“ ${p.title}`);
                }
                lastProductCount = data.products.length;
            }
            
            // éŒ¯èª¤è¨˜éŒ„
            if (data.errors.length > lastErrorCount) {
                let newErrors = data.errors.slice(lastErrorCount);
                for (let err of newErrors) {
                    if (err.error) {
                        log(`âŒ ${err.error}`);
                    } else if (err.url) {
                        log(`âŒ å¤±æ•—: ${err.url.split('/').pop()}`);
                    }
                }
                lastErrorCount = data.errors.length;
            }
        }
        
        let lastErrorCount = 0;
        
        function log(msg) {
            let logDiv = document.getElementById('log');
            let time = new Date().toLocaleTimeString();
            // é¿å…é‡è¤‡è¨Šæ¯ï¼ˆåªæª¢æŸ¥æœ€è¿‘ 50 è¡Œï¼‰
            let recentLog = logDiv.innerHTML.substring(0, 5000);
            if (!recentLog.includes(msg.substring(0, 50))) {
                logDiv.innerHTML = `[${time}] ${msg}\n` + logDiv.innerHTML;
            }
        }
        
        // åˆå§‹è¼‰å…¥ç‹€æ…‹
        pollStatus();
    </script>
</body>
</html>'''


@app.route('/api/status')
def api_status():
    return jsonify(scrape_status)


@app.route('/api/start')
def api_start():
    from flask import request
    category = request.args.get('category', 'mens')
    
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_scrape, args=(category,))
    thread.start()
    
    return jsonify({'started': True, 'category': category})


@app.route('/api/test_single')
def api_test_single():
    """æ¸¬è©¦å–®å“ï¼šçˆ¬å–ä¸€å€‹å•†å“ä¸¦ç›´æ¥ä¸Šå‚³"""
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_test_single)
    thread.start()
    
    return jsonify({'started': True, 'mode': 'test_single'})


@app.route('/api/test_result')
def api_test_result():
    """å–å¾—æ¸¬è©¦å–®å“çš„è©³ç´°çµæœ"""
    return jsonify({
        'running': scrape_status.get('running', False),
        'phase': scrape_status.get('phase', ''),
        'current_product': scrape_status.get('current_product', ''),
        'errors': scrape_status.get('errors', []),
        'test_result': scrape_status.get('test_result', {})
    })


@app.route('/api/upload')
def api_upload():
    from flask import request
    jsonl_file = request.args.get('file', '')
    
    if not jsonl_file or not os.path.exists(jsonl_file):
        return jsonify({'error': 'JSONL æª”æ¡ˆä¸å­˜åœ¨'})
    
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_bulk_upload, args=(jsonl_file,))
    thread.start()
    
    return jsonify({'started': True, 'file': jsonl_file})


@app.route('/api/bulk_status')
def api_bulk_status():
    op_id = scrape_status.get('bulk_operation_id', '')
    status = check_bulk_operation_status(op_id if op_id else None)
    return jsonify(status)


@app.route('/api/bulk_results')
def api_bulk_results():
    """å–å¾— Bulk Operation çš„è©³ç´°çµæœ"""
    results = get_bulk_operation_results()
    return jsonify(results)


@app.route('/api/test')
def api_test():
    """æ¸¬è©¦ Shopify é€£ç·š"""
    load_shopify_token()
    result = graphql_request("{ shop { name } }")
    return jsonify(result)


@app.route('/api/delete')
def api_delete():
    """æ‰¹é‡åˆªé™¤æ‰€æœ‰ WORKMAN å•†å“"""
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    thread = threading.Thread(target=run_delete_workman_products)
    thread.start()
    
    return jsonify({'started': True})


@app.route('/api/publish_all')
def api_publish_all():
    """æ‰¹é‡ç™¼å¸ƒæ‰€æœ‰ WORKMAN å•†å“åˆ°æ‰€æœ‰éŠ·å”®ç®¡é“"""
    if scrape_status['running']:
        return jsonify({'error': 'æ­£åœ¨åŸ·è¡Œä¸­'})
    
    try:
        results = batch_publish_workman_products()
        return jsonify(results)
    except Exception as e:
        return jsonify({'error': str(e)})


@app.route('/api/publications')
def api_publications():
    """å–å¾—æ‰€æœ‰éŠ·å”®ç®¡é“"""
    try:
        publications = get_all_publications()
        return jsonify({'publications': publications})
    except Exception as e:
        return jsonify({'error': str(e)})


@app.route('/api/count')
def api_count():
    """æŸ¥è©¢ WORKMAN å•†å“æ•¸é‡"""
    try:
        load_shopify_token()
        query = """
        {
          productsCount(query: "vendor:WORKMAN") {
            count
          }
        }
        """
        result = graphql_request(query)
        count = result.get('data', {}).get('productsCount', {}).get('count', 0)
        return jsonify({'count': count})
    except Exception as e:
        return jsonify({'error': str(e)})


@app.route('/api/test_workman')
def api_test_workman():
    """æ¸¬è©¦é€£ç·šåˆ° workman.jp"""
    results = {}
    
    # æ¸¬è©¦ä¸»é 
    try:
        response = requests.get(SOURCE_URL, headers=HEADERS, timeout=10)
        results['homepage'] = {
            'status': response.status_code,
            'ok': response.status_code == 200
        }
    except Exception as e:
        results['homepage'] = {'error': str(e), 'ok': False}
    
    # æ¸¬è©¦å…’ç«¥æœåˆ†é¡é 
    try:
        response = requests.get(SOURCE_URL + '/shop/c/c54/', headers=HEADERS, timeout=10)
        results['kids_page'] = {
            'status': response.status_code,
            'ok': response.status_code == 200
        }
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ‰¾æ‰€æœ‰é€£çµï¼Œç¯©é¸å‡ºå•†å“é€£çµ (/shop/g/)
            goods_links = []
            for link in soup.find_all('a', href=True):
                href = link.get('href', '')
                if '/shop/g/' in href and href not in [l.get('href') for l in goods_links]:
                    goods_links.append(link)
            
            results['kids_page']['goods_links_found'] = len(goods_links)
            
            if goods_links:
                results['kids_page']['first_link'] = goods_links[0].get('href', '')
                results['kids_page']['sample_links'] = [l.get('href', '') for l in goods_links[:5]]
    except Exception as e:
        results['kids_page'] = {'error': str(e), 'ok': False}
    
    return jsonify(results)


@app.route('/api/test_product')
def api_test_product():
    """æ¸¬è©¦è§£æå–®ä¸€å•†å“é é¢"""
    from flask import request
    product_url = request.args.get('url', '')
    
    if not product_url:
        # é è¨­æ¸¬è©¦ç¬¬ä¸€å€‹å…’ç«¥å•†å“
        product_url = SOURCE_URL + '/shop/g/g2300022383210/'
    elif not product_url.startswith('http'):
        product_url = SOURCE_URL + product_url
    
    results = {'url': product_url}
    
    try:
        response = requests.get(product_url, headers=HEADERS, timeout=15)
        results['status'] = response.status_code
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ¨™é¡Œ
            title_elem = soup.find('h1', class_='block-goods-name')
            results['title_found'] = title_elem is not None
            if title_elem:
                results['title'] = title_elem.get_text(strip=True)[:50]
            else:
                # å˜—è©¦å…¶ä»–æ–¹å¼
                h1 = soup.find('h1')
                results['h1_found'] = h1 is not None
                if h1:
                    results['h1_text'] = h1.get_text(strip=True)[:50]
            
            # åƒ¹æ ¼
            price_elem = soup.find('p', class_='block-goods-price')
            results['price_elem_found'] = price_elem is not None
            if price_elem:
                results['price_text'] = price_elem.get_text(strip=True)
            else:
                # å˜—è©¦å…¶ä»–æ–¹å¼
                price_any = soup.find(class_=re.compile(r'price'))
                results['price_any_found'] = price_any is not None
                if price_any:
                    results['price_any_text'] = price_any.get_text(strip=True)[:50]
            
            # ç®¡ç†ç•ªè™Ÿ
            code_dt = soup.find('dt', string='ç®¡ç†ç•ªå·')
            results['manage_code_dt_found'] = code_dt is not None
            if code_dt:
                code_dd = code_dt.find_next_sibling('dd')
                if code_dd:
                    results['manage_code'] = code_dd.get_text(strip=True)
            
            # å¾ URL å–å¾—å‚™ç”¨
            match = re.search(r'/g/g(\d+)/', product_url)
            if match:
                results['manage_code_from_url'] = match.group(1)
            
            # åˆ—å‡ºé é¢ä¸Šçš„ä¸€äº› class
            all_classes = set()
            for tag in soup.find_all(class_=True):
                for c in tag.get('class', []):
                    if 'goods' in c.lower() or 'price' in c.lower() or 'product' in c.lower():
                        all_classes.add(c)
            results['relevant_classes'] = list(all_classes)[:20]
            
    except Exception as e:
        results['error'] = str(e)
    
    return jsonify(results)


if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=True)
